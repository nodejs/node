diff --git a/BUILD.gn b/BUILD.gn
index c211d67bb40..6f9b163b7da 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -3527,6 +3527,7 @@ v8_header_set("v8_internal_headers") {
     "src/deoptimizer/deoptimize-reason.h",
     "src/deoptimizer/deoptimized-frame-info.h",
     "src/deoptimizer/deoptimizer.h",
+    "src/deoptimizer/deopt-frame-dumper.h",
     "src/deoptimizer/frame-description.h",
     "src/deoptimizer/frame-translation-builder.h",
     "src/deoptimizer/materialized-object-store.h",
@@ -3965,6 +3966,7 @@ v8_header_set("v8_internal_headers") {
     "src/objects/swiss-hash-table-helpers.h",
     "src/objects/swiss-name-dictionary-inl.h",
     "src/objects/swiss-name-dictionary.h",
+    "src/objects/dumping.h",
     "src/objects/synthetic-module-inl.h",
     "src/objects/synthetic-module.h",
     "src/objects/tagged-field-inl.h",
@@ -5307,6 +5309,7 @@ v8_source_set("v8_base_without_compiler") {
     "src/deoptimizer/deoptimize-reason.cc",
     "src/deoptimizer/deoptimized-frame-info.cc",
     "src/deoptimizer/deoptimizer.cc",
+    "src/deoptimizer/deopt-frame-dumper.cc",
     "src/deoptimizer/frame-translation-builder.cc",
     "src/deoptimizer/materialized-object-store.cc",
     "src/deoptimizer/translated-state.cc",
@@ -5527,6 +5530,7 @@ v8_source_set("v8_base_without_compiler") {
     "src/objects/string-table.cc",
     "src/objects/string.cc",
     "src/objects/swiss-name-dictionary.cc",
+    "src/objects/dumping.cc",
     "src/objects/symbol-table.cc",
     "src/objects/synthetic-module.cc",
     "src/objects/tagged-impl.cc",
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index 497604815c0..5b3e9080d65 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -517,6 +517,10 @@ void BaselineCompiler::VisitSingleBytecode() {
 
   VerifyFrame();
 
+  if (v8_flags.sparkplug_dumping) {
+    TraceBytecode(Runtime::kTraceSmileyface);
+  }
+
 #ifdef V8_TRACE_UNOPTIMIZED
   TraceBytecode(Runtime::kTraceUnoptimizedBytecodeEntry);
 #endif
@@ -576,20 +580,21 @@ void BaselineCompiler::VerifyFrame() {
   }
 }
 
-#ifdef V8_TRACE_UNOPTIMIZED
 void BaselineCompiler::TraceBytecode(Runtime::FunctionId function_id) {
-  if (!v8_flags.trace_baseline_exec) return;
+  if (!v8_flags.sparkplug_dumping) return;
+
+  #ifdef V8_TRACE_UNOPTIMIZED
   ASM_CODE_COMMENT_STRING(&masm_,
                           function_id == Runtime::kTraceUnoptimizedBytecodeEntry
                               ? "Trace bytecode entry"
                               : "Trace bytecode exit");
+  #endif
   SaveAccumulatorScope accumulator_scope(this, &basm_);
   CallRuntime(function_id, bytecode_,
               Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +
                            iterator().current_offset()),
               kInterpreterAccumulatorRegister);
 }
-#endif
 
 #define DECLARE_VISITOR(name, ...) void Visit##name();
 BYTECODE_LIST(DECLARE_VISITOR)
@@ -685,7 +690,9 @@ template <typename... Args>
 void BaselineCompiler::CallRuntime(Runtime::FunctionId function, Args... args) {
 #ifdef DEBUG
   effect_state_.CheckEffect();
-  effect_state_.MayDeopt();
+  if (function != Runtime::kTraceSmileyface) {
+    effect_state_.MayDeopt();
+  }
 #endif
   __ LoadContext(kContextRegister);
   int nargs = __ Push(args...);
diff --git a/src/baseline/baseline-compiler.h b/src/baseline/baseline-compiler.h
index ecbb5e68ddc..6b90667de08 100644
--- a/src/baseline/baseline-compiler.h
+++ b/src/baseline/baseline-compiler.h
@@ -144,9 +144,7 @@ class BaselineCompiler {
   template <ConvertReceiverMode kMode, typename... Args>
   void BuildCall(uint32_t slot, uint32_t arg_count, Args... args);
 
-#ifdef V8_TRACE_UNOPTIMIZED
   void TraceBytecode(Runtime::FunctionId function_id);
-#endif
 
   // Single bytecode visitors.
 #define DECLARE_VISITOR(name, ...) void Visit##name();
diff --git a/src/builtins/builtins-definitions.h b/src/builtins/builtins-definitions.h
index 6569ad187e7..c3dc247ad5e 100644
--- a/src/builtins/builtins-definitions.h
+++ b/src/builtins/builtins-definitions.h
@@ -85,6 +85,9 @@ namespace internal {
   /* Calls */                                                                  \
   /* ES6 section 9.2.1 [[Call]] ( thisArgument, argumentsList) */              \
   ASM(CallFunction_ReceiverIsNullOrUndefined, CallTrampoline)                  \
+  /* Frame dumping */                                                          \
+  ASM(DumpTurboFrame, Void)                                                    \
+  ASM(FunctionEntryPrint, Void)                                                \
   ASM(CallFunction_ReceiverIsNotNullOrUndefined, CallTrampoline)               \
   ASM(CallFunction_ReceiverIsAny, CallTrampoline)                              \
   /* ES6 section 9.4.1.1 [[Call]] ( thisArgument, argumentsList) */            \
@@ -221,6 +224,7 @@ namespace internal {
   TFC(CompileLazyDeoptimizedCode, JSTrampoline)                                \
   TFC(InstantiateAsmJs, JSTrampoline)                                          \
   ASM(NotifyDeoptimized, Void)                                                 \
+  ASM(NotifyDumpFrame, Void)                                                   \
                                                                                \
   /* Trampolines called when returning from a deoptimization that expects   */ \
   /* to continue in a JavaScript builtin to finish the functionality of a   */ \
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 22497e4cd84..02d635811b9 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -2063,6 +2063,119 @@ void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {
   __ ret(1 * kSystemPointerSize);  // Remove rax.
 }
 
+void Builtins::Generate_NotifyDumpFrame(MacroAssembler* masm) {
+  // Enter an internal frame.
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ CallRuntime(Runtime::kNotifyDumpFrame);
+    // Tear down internal frame.
+  }
+
+  // State restore
+
+  static constexpr int kDoubleRegsSize =
+      kDoubleSize * XMMRegister::kNumRegisters;
+
+  static constexpr int kRegSaveOffset =
+      kDoubleRegsSize - 8;
+
+  static constexpr int kNumberOfRegisters = Register::kNumRegisters;
+  static constexpr int kGPRegsSize = kSystemPointerSize * kNumberOfRegisters;
+
+  static constexpr int kRFLAGSOffset = kNumberOfRegisters * kSystemPointerSize + kRegSaveOffset;
+  // +8 for rflags
+  static constexpr int kStackEntriesSaveOffset = kRegSaveOffset + kGPRegsSize + 8;
+
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kRegSaveOffset == 0x78);
+  static_assert(kStackEntriesSaveOffset == 0x100);
+
+
+  const RegisterConfiguration* config = RegisterConfiguration::Default();
+
+  __ movq(rdi, Immediate(kFrameDumpRegRestoreBase));
+
+  // Restore stack
+  static_assert(Register::from_code(rsp.code()) == rsp);
+
+  /// restore rsp
+  int rsp_offset = rsp.code() * kSystemPointerSize + kRegSaveOffset;
+  __ movq(rsp, MemOperand(rdi, rsp_offset));
+
+  // Saved off is not entirely correct since we save after ret address is deleted. Fix that now
+  __ addq(rsp, Immediate(8));
+
+
+  /// load fp-sp-delta
+  __ movq(rax, MemOperand(rdi, kStackEntriesSaveOffset));
+
+  __ addq(rdi, Immediate(kStackEntriesSaveOffset));
+  __ addq(rdi, Immediate(8));
+
+  __ addq(rdi, rax);
+
+  __ movq(rsi, rsp);
+  __ addq(rsi, rax);
+
+  __ addq(rax, Immediate(8));
+
+
+  Label restore_stack_frame_loop;
+  __ bind(&restore_stack_frame_loop);
+
+  ///// Loop Body
+  __ movq(rcx, MemOperand(rdi, 0));
+  __ movq(MemOperand(rsi, 0), rcx);
+
+  // decrease i, src, dst
+  __ subq(rax, Immediate(8));
+  __ subq(rdi, Immediate(8));
+  __ subq(rsi, Immediate(8));
+
+  __ cmpq(rax, Immediate(0));
+  __ j(not_equal, &restore_stack_frame_loop);
+
+
+  __ movq(rdi, Immediate(kFrameDumpRegRestoreBase));
+
+  // Restore double registers
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    XMMRegister xmm_reg = XMMRegister::from_code(code);
+    int offset = code * kDoubleSize;
+    __ Movsd(xmm_reg, Operand(rdi, offset));
+  }
+
+
+  // Restore general purpose registers
+  for (int i = 0; i < kNumberOfRegisters; i++) {
+    int offset = i * kSystemPointerSize + kRegSaveOffset;
+    if (!(Register::from_code(i) == rsp || Register::from_code(i) == rdi)) {
+      __ movq(Register::from_code(i), MemOperand(rdi, offset));
+    }
+  }
+
+  // restore flags
+  __ pushq(rax);
+
+  __ pushq(Immediate(0));
+  __ movq(rax, MemOperand(rdi, kRFLAGSOffset));
+  __ movq(MemOperand(rsp, 0), rax);
+  __ popfq();
+
+  __ popq(rax);
+
+  static_assert(Register::from_code(rdi.code()) == rdi);
+
+  // save and restore corrupted rdi reg
+  __ pushq(rbx);
+  __ movq(rbx, Immediate(kFrameDumpRegRestoreBase));
+  __ movq(rdi, MemOperand(rbx, rdi.code() * kSystemPointerSize + kRegSaveOffset));
+  __ popq(rbx);
+
+  __ ret(0);
+}
+
 // static
 void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
@@ -4889,6 +5002,358 @@ void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
   Generate_DeoptimizationEntry(masm, DeoptimizeKind::kLazy);
 }
 
+
+void Builtins::Generate_DumpTurboFrame(MacroAssembler* masm) {
+  static constexpr int kDoubleRegsSize =
+      kDoubleSize * XMMRegister::kNumRegisters;
+
+  static constexpr int kRegSaveOffset =
+      kDoubleRegsSize - 8;
+
+  static constexpr int kNumberOfRegisters = Register::kNumRegisters;
+  static constexpr int kGPRegsSize = kSystemPointerSize * kNumberOfRegisters;
+
+  static constexpr int kRFLAGSOffset = kNumberOfRegisters * kSystemPointerSize + kRegSaveOffset;
+  // +8 for rflags
+  static constexpr int kStackEntriesSaveOffset = kRegSaveOffset + kGPRegsSize + 8;
+
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kRegSaveOffset == 0x78);
+  static_assert(kStackEntriesSaveOffset == 0x100);
+
+  static constexpr int kSaveMemEndAddress = 0x13370000 + 0x1000 * 10;
+
+  const RegisterConfiguration* config = RegisterConfiguration::Default();
+
+
+  /*
+  Save register state since we want to return
+  */
+
+  __ pushq(rdi);
+  __ movq(rdi, Immediate(kFrameDumpRegRestoreBase));
+
+  // Save double registers
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    XMMRegister xmm_reg = XMMRegister::from_code(code);
+    int offset = code * kDoubleSize;
+    __ Movsd(Operand(rdi, offset), xmm_reg);
+  }
+
+  static_assert(Register::from_code(rdi.code()) == rdi);
+
+  // Save general purpose registers
+  for (int i = 0; i < kNumberOfRegisters; i++) {
+    int offset = i * kSystemPointerSize + kRegSaveOffset;
+    __ movq(MemOperand(rdi, offset), Register::from_code(i));
+  }
+
+  __ pushq(rax);
+
+  // save flags
+  __ pushfq();
+  __ movq(rax, MemOperand(rsp, 0));
+  __ movq(MemOperand(rdi, kRFLAGSOffset), rax);
+  __ popfq();
+
+  __ popq(rax);
+
+  // save (hardware) stack frame
+  __ pushq(rax);
+  __ pushq(rbx);
+  __ pushq(rcx);
+
+  static constexpr int kCurrentHWStackFrameOff = 0x20;
+  // since saved rax + rbx + rcx + rdi
+
+  __ leaq(rax, Operand(rsp, kCurrentHWStackFrameOff));
+  __ movq(rbx, Operand(rbp, 0));
+  __ subq(rax, rbx);
+  __ negq(rax);
+
+  // rax contains current (hardware) stack frame size that needs to be saved
+
+  __ addq(rdi, Immediate(kStackEntriesSaveOffset));
+  // save fp-sp-delta
+  __ movq(MemOperand(rdi, 0), rax);
+  __ addq(rdi, Immediate(8));
+
+
+  __ addq(rdi, rax);
+
+
+  __ leaq(rbx, Operand(rsp, kCurrentHWStackFrameOff));
+  __ addq(rbx, rax);
+
+  // artificially add 8 to correctly loop below
+  __ addq(rax, Immediate(8));
+
+  Label save_stack_frame_loop;
+
+  __ cmpq(rdi, Immediate(kSaveMemEndAddress));
+  __ j(less, &save_stack_frame_loop);
+
+  // abort
+  __ Abort(AbortReason::kDumpingStackFrameToLargeToSave);
+
+
+  __ bind(&save_stack_frame_loop);
+
+  ///// Loop Body
+  __ movq(rcx, MemOperand(rbx, 0));
+  __ movq(MemOperand(rdi, 0), rcx);
+
+  // decrease i, src, dst
+  __ subq(rax, Immediate(8));
+  __ subq(rdi, Immediate(8));
+  __ subq(rbx, Immediate(8));
+
+
+  __ cmpq(rax, Immediate(0));
+  __ j(not_equal, &save_stack_frame_loop);
+
+
+  __ popq(rcx);
+  __ popq(rbx);
+  __ popq(rax);
+
+  // save and restore corrupted rdi reg
+  __ popq(rdi);
+  __ pushq(rbx);
+  __ movq(rbx, Immediate(kFrameDumpRegRestoreBase));
+  __ movq(MemOperand(rbx, rdi.code() * kSystemPointerSize + kRegSaveOffset), rdi);
+  __ popq(rbx);
+
+  /**/
+  Isolate* isolate = masm->isolate();
+
+  // Save all double registers, they will later be copied to the deoptimizer's
+  // FrameDescription.
+  __ AllocateStackSpace(kDoubleRegsSize);
+
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    XMMRegister xmm_reg = XMMRegister::from_code(code);
+    int offset = code * kDoubleSize;
+    __ Movsd(Operand(rsp, offset), xmm_reg);
+  }
+
+  // Save all general purpose registers, they will later be copied to the
+  // deoptimizer's FrameDescription.
+  for (int i = 0; i < kNumberOfRegisters; i++) {
+    __ pushq(Register::from_code(i));
+  }
+
+  static constexpr int kSavedRegistersAreaSize =
+      kNumberOfRegisters * kSystemPointerSize + kDoubleRegsSize;
+  static constexpr int kCurrentOffsetToReturnAddress = kSavedRegistersAreaSize;
+  static constexpr int kCurrentOffsetToParentSP =
+      kCurrentOffsetToReturnAddress + kPCOnStackSize;
+
+  __ Store(
+      ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate),
+      rbp);
+
+  // Get the address of the location in the code object
+  // and compute the fp-to-sp delta in register arg5.
+  __ movq(kCArgRegs[2], Operand(rsp, kCurrentOffsetToReturnAddress));
+  // Load the fp-to-sp-delta.
+  __ leaq(kCArgRegs[3], Operand(rsp, kCurrentOffsetToParentSP));
+  __ subq(kCArgRegs[3], rbp);
+  __ negq(kCArgRegs[3]);
+
+  // Allocate a new deoptimizer object.
+  __ PrepareCallCFunction(6);
+  __ Move(rax, 0);
+  Label context_check;
+  __ movq(rdi, Operand(rbp, CommonFrameConstants::kContextOrFrameTypeOffset));
+  __ JumpIfSmi(rdi, &context_check);
+  __ movq(rax, Operand(rbp, StandardFrameConstants::kFunctionOffset));
+  __ bind(&context_check);
+  __ movq(kCArgRegs[0], rax);
+  __ Move(kCArgRegs[1], static_cast<int>(DeoptimizeKind::kEager));
+  // Args 3 and 4 are already in the right registers.
+
+  // On linux pass the arguments in r8.
+
+  // r8 is kCArgRegs[4] on Linux
+  __ LoadAddress(r8, ExternalReference::isolate_address(isolate));
+
+  // new arg: r9 == deopt id
+  __ movq(r9, MemOperand(rsp, kStackEntriesSaveOffset + 8));
+
+  {
+    AllowExternalCallThatCantCauseGC scope(masm);
+    __ CallCFunction(ExternalReference::new_dumper_function(), 6);
+  }
+  // Preserve deoptimizer object in register rax and get the input
+  // frame descriptor pointer.
+  __ movq(rbx, Operand(rax, Deoptimizer::input_offset()));
+
+  // Fill in the input registers.
+  for (int i = kNumberOfRegisters - 1; i >= 0; i--) {
+    int offset =
+        (i * kSystemPointerSize) + FrameDescription::registers_offset();
+    __ PopQuad(Operand(rbx, offset));
+  }
+
+  // Fill in the double input registers.
+  int double_regs_offset = FrameDescription::double_registers_offset();
+  for (int i = 0; i < XMMRegister::kNumRegisters; i++) {
+    int dst_offset = i * kDoubleSize + double_regs_offset;
+    __ popq(Operand(rbx, dst_offset));
+  }
+
+  // Mark the stack as not iterable for the CPU profiler which won't be able to
+  // walk the stack without the return address.
+  __ movb(__ ExternalReferenceAsOperand(
+              ExternalReference::stack_is_iterable_address(isolate)),
+          Immediate(0));
+
+
+  // Remove the return address from the stack.
+  // !!!We need this return address since we don't deopt at the end!!!
+  // However the logic relies on the fact that we increase RSP here
+  __ addq(rsp, Immediate(kPCOnStackSize));
+
+
+  // Compute a pointer to the unwinding limit in register rcx; that is
+  // the first stack slot not part of the input frame.
+  __ movq(rcx, Operand(rbx, FrameDescription::frame_size_offset()));
+  __ addq(rcx, rsp);
+
+  // Unwind the stack down to - but not including - the unwinding
+  // limit and copy the contents of the activation frame to the input
+  // frame description.
+  __ leaq(rdx, Operand(rbx, FrameDescription::frame_content_offset()));
+  Label pop_loop_header;
+  __ jmp(&pop_loop_header);
+  Label pop_loop;
+  __ bind(&pop_loop);
+  __ Pop(Operand(rdx, 0));
+  __ addq(rdx, Immediate(sizeof(intptr_t)));
+  __ bind(&pop_loop_header);
+  __ cmpq(rcx, rsp);
+  __ j(not_equal, &pop_loop);
+
+  // Compute the output frame in the deoptimizer.
+  __ pushq(rax);
+  __ PrepareCallCFunction(2);
+  __ movq(kCArgRegs[0], rax);
+  __ LoadAddress(kCArgRegs[1], ExternalReference::isolate_address(isolate));
+  {
+    AllowExternalCallThatCantCauseGC scope(masm);
+    __ CallCFunction(ExternalReference::dump_output_frames_function(), 2);
+  }
+  __ popq(rax);
+
+  __ movq(rsp, Operand(rax, Deoptimizer::caller_frame_top_offset()));
+
+  // Replace the current (input) frame with the output frames.
+  Label outer_push_loop, inner_push_loop, outer_loop_header, inner_loop_header;
+  // Outer loop state: rax = current FrameDescription**, rdx = one past the
+  // last FrameDescription**.
+  __ movl(rdx, Operand(rax, Deoptimizer::output_count_offset()));
+  __ movq(rax, Operand(rax, Deoptimizer::output_offset()));
+  __ leaq(rdx, Operand(rax, rdx, times_system_pointer_size, 0));
+  __ jmp(&outer_loop_header);
+  __ bind(&outer_push_loop);
+  // Inner loop state: rbx = current FrameDescription*, rcx = loop index.
+  __ movq(rbx, Operand(rax, 0));
+  __ movq(rcx, Operand(rbx, FrameDescription::frame_size_offset()));
+  __ jmp(&inner_loop_header);
+  __ bind(&inner_push_loop);
+  __ subq(rcx, Immediate(sizeof(intptr_t)));
+  __ Push(Operand(rbx, rcx, times_1, FrameDescription::frame_content_offset()));
+  __ bind(&inner_loop_header);
+  __ testq(rcx, rcx);
+  __ j(not_zero, &inner_push_loop);
+  __ addq(rax, Immediate(kSystemPointerSize));
+  __ bind(&outer_loop_header);
+  __ cmpq(rax, rdx);
+  __ j(below, &outer_push_loop);
+
+
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    XMMRegister xmm_reg = XMMRegister::from_code(code);
+    int src_offset = code * kDoubleSize + double_regs_offset;
+    __ Movsd(xmm_reg, Operand(rbx, src_offset));
+  }
+
+  // Push pc and continuation from the last output frame.
+  __ PushQuad(Operand(rbx, FrameDescription::pc_offset()));
+  __ PushQuad(Operand(rbx, FrameDescription::continuation_offset()));
+
+  // Push the registers from the last output frame.
+  for (int i = 0; i < kNumberOfRegisters; i++) {
+    int offset =
+        (i * kSystemPointerSize) + FrameDescription::registers_offset();
+    __ PushQuad(Operand(rbx, offset));
+  }
+
+  // Restore the registers from the stack.
+  for (int i = kNumberOfRegisters - 1; i >= 0; i--) {
+    Register r = Register::from_code(i);
+    // Do not restore rsp, simply pop the value into the next register
+    // and overwrite this afterwards.
+    if (r == rsp) {
+      DCHECK_GT(i, 0);
+      r = Register::from_code(i - 1);
+    }
+    __ popq(r);
+  }
+
+  /*
+  __ movb(__ ExternalReferenceAsOperand(
+              ExternalReference::stack_is_iterable_address(isolate)),
+          Immediate(1));
+  */
+  // Return to the continuation point.
+  // __ ret(0);
+  /**/
+  __ ret(0);
+}
+
+void Builtins::Generate_FunctionEntryPrint(MacroAssembler* masm) {
+  // Print "~~----~~~---~~\n"
+  static constexpr int kSYSWRITE = 1;
+  static constexpr int kSTDOUTFD = 1;
+
+
+  // save registers
+  __ pushq(rdi);
+  __ pushq(rsi);
+  __ pushq(rdx);
+  __ pushq(r11);
+  __ pushq(rcx);
+  __ pushq(rax);
+
+  __ movq(rdi, Immediate64(0xa7e7e2d2d2d7e));
+  __ pushq(rdi);
+  __ movq(rdi, Immediate64(0x7e7e2d2d2d2d7e7e));
+  __ pushq(rdi);
+
+  __ movq(rdi, Immediate(kSTDOUTFD));
+  __ movq(rsi, rsp);
+  __ movq(rdx, Immediate(15));
+
+  __ Syscall(kSYSWRITE);
+
+  // restore registers
+  __ addq(rsp, Immediate(0x10));
+  __ popq(rax);
+  __ popq(rcx);
+  __ popq(r11);
+  __ popq(rdx);
+  __ popq(rsi);
+  __ popq(rdi);
+
+  __ ret(0);
+}
+
+
 namespace {
 
 // Restarts execution either at the current or next (in execution order)
diff --git a/src/codegen/bailout-reason. b/src/codegen/bailout-reason.
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/src/codegen/bailout-reason.h b/src/codegen/bailout-reason.h
index 449170cbe32..e2a43ee298e 100644
--- a/src/codegen/bailout-reason.h
+++ b/src/codegen/bailout-reason.h
@@ -104,7 +104,9 @@ namespace internal {
   V(kInvalidReceiver, "Expected JS object or primitive object")                \
   V(kUnexpectedInstanceType, "Unexpected instance type encountered")           \
   V(kTurboshaftTypeAssertionFailed,                                            \
-    "A type assertion failed in Turboshaft-generated code")
+    "A type assertion failed in Turboshaft-generated code")                    \
+  V(kDumpingStackFrameToLargeToSave,                                           \
+    "# Aborting on encounter of stack frame during dumping that is to large to be saved")
 
 #define BAILOUT_MESSAGES_LIST(V)                                             \
   V(kNoReason, "no reason")                                                  \
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index afb74deec37..cc73340708a 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -188,6 +188,7 @@ class CompilerTracer : public AllStatic {
   static void TraceFinishBaselineCompile(Isolate* isolate,
                                          Handle<SharedFunctionInfo> shared,
                                          double ms_timetaken) {
+    isolate->RecordJITUsage(Isolate::JITType::kSparkplug);
     if (!v8_flags.trace_baseline) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "completed compiling", shared, CodeKind::BASELINE);
@@ -199,6 +200,7 @@ class CompilerTracer : public AllStatic {
                                        Handle<JSFunction> function, bool osr,
                                        double ms_prepare, double ms_execute,
                                        double ms_finalize) {
+    isolate->RecordJITUsage(Isolate::JITType::kMaglev);
     if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "completed compiling", function, CodeKind::MAGLEV);
@@ -538,6 +540,7 @@ void TurbofanCompilationJob::RecordCompilationStats(ConcurrencyMode mode,
                                                     Isolate* isolate) const {
   DCHECK(compilation_info()->IsOptimizing());
   Handle<SharedFunctionInfo> shared = compilation_info()->shared_info();
+  isolate->RecordJITUsage(Isolate::JITType::kTurbofan);
   if (v8_flags.trace_opt || v8_flags.trace_opt_stats) {
     double ms_creategraph = time_taken_to_prepare_.InMillisecondsF();
     double ms_optimize = time_taken_to_execute_.InMillisecondsF();
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index cefe52d7cbe..8c517a7d6f3 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -13,6 +13,7 @@
 #include "src/date/date.h"
 #include "src/debug/debug.h"
 #include "src/deoptimizer/deoptimizer.h"
+#include "src/deoptimizer/deopt-frame-dumper.h"
 #include "src/execution/encoded-c-signature.h"
 #include "src/execution/isolate-utils.h"
 #include "src/execution/isolate.h"
@@ -483,6 +484,11 @@ FUNCTION_REFERENCE(new_deoptimizer_function, Deoptimizer::New)
 FUNCTION_REFERENCE(compute_output_frames_function,
                    Deoptimizer::ComputeOutputFrames)
 
+FUNCTION_REFERENCE(new_dumper_function, DeoptFrameDumper::New)
+FUNCTION_REFERENCE(dump_output_frames_function, DeoptFrameDumper::ComputeOutputFrames)
+
+
+
 #ifdef V8_ENABLE_WEBASSEMBLY
 FUNCTION_REFERENCE(wasm_sync_stack_limit, wasm::sync_stack_limit)
 FUNCTION_REFERENCE(wasm_switch_to_the_central_stack,
diff --git a/src/codegen/external-reference.h b/src/codegen/external-reference.h
index e3536d9f8ff..20c05713333 100644
--- a/src/codegen/external-reference.h
+++ b/src/codegen/external-reference.h
@@ -132,6 +132,7 @@ class StatsCounter;
   V(check_object_type, "check_object_type")                                    \
   V(compute_integer_hash, "ComputeSeededHash")                                 \
   V(compute_output_frames_function, "Deoptimizer::ComputeOutputFrames()")      \
+  V(dump_output_frames_function, "DeoptFrameDumper::ComputeOutputFrames()")    \
   V(copy_fast_number_jsarray_elements_to_typed_array,                          \
     "copy_fast_number_jsarray_elements_to_typed_array")                        \
   V(copy_typed_array_elements_slice, "copy_typed_array_elements_slice")        \
@@ -222,6 +223,7 @@ class StatsCounter;
   V(mutable_big_int_right_shift_and_canonicalize_function,                     \
     "MutableBigInt_RightShiftAndCanonicalize")                                 \
   V(new_deoptimizer_function, "Deoptimizer::New()")                            \
+  V(new_dumper_function, "DeoptFrameDumper::New()")                            \
   V(orderedhashmap_gethash_raw, "orderedhashmap_gethash_raw")                  \
   V(printf_function, "printf")                                                 \
   V(refill_math_random, "MathRandom::RefillCache")                             \
diff --git a/src/codegen/x64/assembler-x64.cc b/src/codegen/x64/assembler-x64.cc
index f7ff30fc7f1..3776f3a852d 100644
--- a/src/codegen/x64/assembler-x64.cc
+++ b/src/codegen/x64/assembler-x64.cc
@@ -1110,6 +1110,12 @@ void Assembler::call(Label* L) {
   }
 }
 
+void Assembler::syscall() {
+  EnsureSpace ensure_space(this);
+  emit(0xf);
+  emit(0x5);
+}
+
 void Assembler::call(Handle<Code> target, RelocInfo::Mode rmode) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   EnsureSpace ensure_space(this);
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index 36420e4d28a..698e0916d96 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -1079,6 +1079,8 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // Call near relative 32-bit displacement, relative to next instruction.
   void call(Label* L);
 
+  void syscall();
+
   // Explicitly emit a near call / near jump. The displacement is relative to
   // the next instructions (which starts at {pc_offset() + kNearJmpInstrSize}).
   static constexpr int kNearJmpInstrSize = 5;
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index ede8a310642..33eebe3d85a 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -2266,6 +2266,11 @@ void MacroAssembler::JumpIfNotSmi(Register src, Label* on_not_smi,
   j(NegateCondition(smi), on_not_smi, near_jump);
 }
 
+void MacroAssembler::Syscall(int num) {
+  movq(rax, Immediate(num));
+  syscall();
+}
+
 void MacroAssembler::JumpIfNotSmi(Operand src, Label* on_not_smi,
                                   Label::Distance near_jump) {
   Condition smi = CheckSmi(src);
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 101b666ffb9..3aa980ccf25 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -327,6 +327,8 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void JumpIfNotSmi(Operand src, Label* on_not_smi,
                     Label::Distance near_jump = Label::kFar);
 
+  void Syscall(int num);
+
   // Operations on tagged smi values.
 
   // Smis represent a subset of integers. The subset is always equivalent to
diff --git a/src/common/globals.h b/src/common/globals.h
index 6db6c7bff8a..72e389010eb 100644
--- a/src/common/globals.h
+++ b/src/common/globals.h
@@ -749,6 +749,8 @@ enum class CallApiCallbackMode {
   kOptimized,
 };
 
+constexpr int kFrameDumpRegRestoreBase = 0x13370000;
+
 // This constant is used as an undefined value when passing source positions.
 constexpr int kNoSourcePosition = -1;
 
diff --git a/src/compiler/backend/code-generator.cc b/src/compiler/backend/code-generator.cc
index 79367c0769b..7e3efe8a830 100644
--- a/src/compiler/backend/code-generator.cc
+++ b/src/compiler/backend/code-generator.cc
@@ -802,6 +802,17 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleInstruction(
       branch.fallthru = true;
       AssembleArchDeoptBranch(instr, &branch);
       masm()->bind(exit->continue_label());
+      if (v8_flags.turbofan_dumping) {
+        FrameStateDescriptor* descriptor =
+        GetDeoptimizationEntry(instr, frame_state_offset).descriptor();
+        // here we insert even if currently dumping is temporarily disabled
+        // should not matter for our usecase
+        if (descriptor->type() == FrameStateType::kUnoptimizedFunction && !isolate()->IsIsolateDumpDislabed()) {
+          // Only insert dumping when we know there is an equivalent frame on interpreter side and
+          // we are not in a worker
+          AssembleDumpFrame();
+        }
+      }
       break;
     }
     case kFlags_set: {
diff --git a/src/compiler/backend/code-generator.h b/src/compiler/backend/code-generator.h
index 86e75328dc2..46daaa09e82 100644
--- a/src/compiler/backend/code-generator.h
+++ b/src/compiler/backend/code-generator.h
@@ -315,6 +315,8 @@ class V8_EXPORT_PRIVATE CodeGenerator final : public GapResolver::Assembler {
   void AssembleReturn(InstructionOperand* pop);
 
   void AssembleDeconstructFrame();
+  void AssembleDumpFrame();
+  void AssembleFunctionEntryPrint();
 
   // Generates code to manipulate the stack in preparation for a tail call.
   void AssemblePrepareTailCall();
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index c26a815fcb9..27b598c7b29 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -1239,6 +1239,14 @@ void CodeGenerator::AssembleDeconstructFrame() {
   __ popq(rbp);
 }
 
+void CodeGenerator::AssembleDumpFrame() {
+  __ CallBuiltin(Builtin::kDumpTurboFrame);
+}
+
+void CodeGenerator::AssembleFunctionEntryPrint() {
+  __ CallBuiltin(Builtin::kFunctionEntryPrint);
+}
+
 void CodeGenerator::AssemblePrepareTailCall() {
   if (frame_access_state()->has_frame()) {
     __ movq(rbp, MemOperand(rbp, 0));
diff --git a/src/d8/d8.cc b/src/d8/d8.cc
index 79e3ea60c2d..7f3698b94d1 100644
--- a/src/d8/d8.cc
+++ b/src/d8/d8.cc
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include "src/common/globals.h"
+
+
 #include <errno.h>
 #include <stdlib.h>
 #include <string.h>
@@ -1781,7 +1784,9 @@ int PerIsolateData::RealmIndexOrThrow(
 // When v8_flags.verify_predictable mode is enabled it returns result of
 // v8::Platform::MonotonicallyIncreasingTime().
 double Shell::GetTimestamp() {
-  if (i::v8_flags.verify_predictable) {
+  if (i::v8_flags.predictable) {
+    return 1.1;
+  } else if (i::v8_flags.verify_predictable) {
     return g_platform->MonotonicallyIncreasingTime();
   } else {
     base::TimeDelta delta = base::TimeTicks::Now() - kInitialTicks;
@@ -4346,7 +4351,9 @@ bool ends_with(const char* input, const char* suffix) {
 
 bool SourceGroup::Execute(Isolate* isolate) {
   bool success = true;
+  // convert to internal isolate
 #ifdef V8_FUZZILLI
+  v8::internal::Isolate *internal_isolate = reinterpret_cast<v8::internal::Isolate*>(isolate);
   if (fuzzilli_reprl) {
     HandleScope handle_scope(isolate);
     Local<String> file_name =
@@ -4354,7 +4361,18 @@ bool SourceGroup::Execute(Isolate* isolate) {
             .ToLocalChecked();
 
     size_t script_size;
+    unsigned int source_pos_dump_seed;
+    CHECK_EQ(read(REPRL_CRFD, &source_pos_dump_seed, sizeof(source_pos_dump_seed)), sizeof(source_pos_dump_seed));
+
+    internal_isolate->SetDumpingSeed(source_pos_dump_seed);
+
+    if (i::v8_flags.load_dump_positions) {
+      internal_isolate->ClearDumpPositionCache();
+      internal_isolate->ReadDumpPositionFromFile();
+    }
+
     CHECK_EQ(read(REPRL_CRFD, &script_size, 8), 8);
+
     char* buffer = new char[script_size + 1];
     char* ptr = buffer;
     size_t remaining = script_size;
@@ -4720,6 +4738,9 @@ void Worker::ExecuteInThread() {
   create_params.array_buffer_allocator = Shell::array_buffer_allocator;
   isolate_ = Isolate::New(create_params);
 
+  v8::internal::Isolate *internal_isolate = reinterpret_cast<v8::internal::Isolate*>(isolate_);
+  internal_isolate->SetIsolateDumpDislabed();
+
   // Make the Worker instance available to the whole thread.
   SetCurrentWorker(this);
 
@@ -5750,7 +5771,19 @@ void d8_install_sigterm_handler() {
 
 }  // namespace
 
+constexpr size_t kFrameDumpRegRestoreBase = 0x13370000;
+constexpr size_t kPageSize = 0x1000;
+
+
 int Shell::Main(int argc, char* argv[]) {
+  void* res = mmap((void *)kFrameDumpRegRestoreBase, 10 * kPageSize, PROT_READ | PROT_WRITE,
+                   MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
+  if (res != (void*)kFrameDumpRegRestoreBase) {
+    return 1;
+  }
+
+  setvbuf(stdout, NULL, _IONBF, 0);
+
   v8::base::EnsureConsoleOutput();
   if (!SetOptions(argc, argv)) return 1;
   if (!i::v8_flags.fuzzing) d8_install_sigterm_handler();
@@ -5958,12 +5991,12 @@ int Shell::Main(int argc, char* argv[]) {
             ->GetCoverageBitmap(reinterpret_cast<i::Isolate*>(isolate))
             .size()));
   }
-  char helo[] = "HELO";
+  char helo[] = "XDXD";
   if (write(REPRL_CWFD, helo, 4) != 4 || read(REPRL_CRFD, helo, 4) != 4) {
     fuzzilli_reprl = false;
   }
 
-  if (memcmp(helo, "HELO", 4) != 0) {
+  if (memcmp(helo, "XDXD", 4) != 0) {
     FATAL("REPRL: Invalid response from parent");
   }
 #endif  // V8_FUZZILLI
@@ -6098,7 +6131,28 @@ int Shell::Main(int argc, char* argv[]) {
 #ifdef V8_FUZZILLI
       // Send result to parent (fuzzilli) and reset edge guards.
       if (fuzzilli_reprl) {
+        // Write source_pos_dump file if possible
+        v8::internal::Isolate *internal_isolate = reinterpret_cast<v8::internal::Isolate*>(isolate);
+
+        if (internal_isolate->IsDumpingEnabled()) {
+          internal_isolate->WriteAndResetDumpOut();
+
+          if (i::v8_flags.generate_dump_positions) internal_isolate->WriteDumpPositionToFile(); 
+        }
+
         int status = result << 8;
+        uint8_t jit_state = internal_isolate->GetJITState();
+
+        #pragma pack(1)
+        struct {
+            int status;
+            uint8_t jit_state;
+        } s;
+        s.status = status;
+        s.jit_state = jit_state;
+
+        const size_t ssize = 5;
+
         std::vector<bool> bitmap;
         if (options.fuzzilli_enable_builtins_coverage) {
           bitmap = i::BasicBlockProfiler::Get()->GetCoverageBitmap(
@@ -6121,8 +6175,12 @@ int Shell::Main(int argc, char* argv[]) {
         // to be flushed after every execution
         fflush(stdout);
         fflush(stderr);
-        CHECK_EQ(write(REPRL_CWFD, &status, 4), 4);
+        CHECK_EQ(write(REPRL_CWFD, &s, ssize), ssize);
         sanitizer_cov_reset_edgeguards();
+
+        internal_isolate->DiffPrintClear();
+        internal_isolate->SetInScriptDumpingEnabled(false);
+
         if (options.fuzzilli_enable_builtins_coverage) {
           i::BasicBlockProfiler::Get()->ResetCounts(
               reinterpret_cast<i::Isolate*>(isolate));
diff --git a/src/debug/debug-evaluate.cc b/src/debug/debug-evaluate.cc
index e4419e9dbfe..ce18fcf2135 100644
--- a/src/debug/debug-evaluate.cc
+++ b/src/debug/debug-evaluate.cc
@@ -398,6 +398,8 @@ bool DebugEvaluate::IsSideEffectFreeIntrinsic(Runtime::FunctionId id) {
   /* Test */                             \
   V(GetOptimizationStatus)               \
   V(OptimizeFunctionOnNextCall)          \
+  V(EnableFrameDumping)                  \
+  V(DisableFrameDumping)                 \
   V(OptimizeOsr)
 
 // Intrinsics with inline versions have to be allowlisted here a second time.
diff --git a/src/deoptimizer/deopt-frame-dumper.cc b/src/deoptimizer/deopt-frame-dumper.cc
new file mode 100644
index 00000000000..cb665ef42e0
--- /dev/null
+++ b/src/deoptimizer/deopt-frame-dumper.cc
@@ -0,0 +1,1813 @@
+// Copyright 2013 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/deoptimizer/deopt-frame-dumper.h"
+
+#include "src/base/memory.h"
+#include "src/codegen/interface-descriptors.h"
+#include "src/codegen/register-configuration.h"
+#include "src/codegen/reloc-info.h"
+#include "src/debug/debug.h"
+#include "src/deoptimizer/deoptimized-frame-info.h"
+#include "src/deoptimizer/materialized-object-store.h"
+#include "src/deoptimizer/translated-state.h"
+#include "src/execution/frames-inl.h"
+#include "src/execution/isolate.h"
+#include "src/execution/pointer-authentication.h"
+#include "src/execution/v8threads.h"
+#include "src/handles/handles-inl.h"
+#include "src/heap/heap-inl.h"
+#include "src/logging/counters.h"
+#include "src/logging/log.h"
+#include "src/logging/runtime-call-stats-scope.h"
+#include "src/objects/js-function-inl.h"
+#include "src/objects/oddball.h"
+#include "src/snapshot/embedded/embedded-data.h"
+#include "src/utils/utils.h"
+
+#include "src/execution/frames.h"
+#include "src/objects/objects.h"
+
+
+
+namespace v8 {
+
+using base::Memory;
+
+namespace internal {
+
+
+// {FrameWriter} offers a stack writer abstraction for writing
+// FrameDescriptions. The main service the class provides is managing
+// {top_offset_}, i.e. the offset of the next slot to write to.
+//
+// Note: Not in an anonymous namespace due to the friend class declaration
+// in DeoptFrameDumper.
+class FrameWriter {
+ public:
+  static const int NO_INPUT_INDEX = -1;
+  FrameWriter(DeoptFrameDumper* dumper, FrameDescription* frame,
+              CodeTracer::Scope* trace_scope)
+      : dumper_(dumper),
+        frame_(frame),
+        trace_scope_(trace_scope),
+        top_offset_(frame->GetFrameSize()) {}
+
+  void PushRawValue(intptr_t value, const char* debug_hint) {
+    PushValue(value);
+    if (trace_scope_ != nullptr) {
+      DebugPrintOutputValue(value, debug_hint);
+    }
+  }
+
+  void PushRawObject(Tagged<Object> obj, const char* debug_hint) {
+    intptr_t value = obj.ptr();
+    PushValue(value);
+    if (trace_scope_ != nullptr) {
+      DebugPrintOutputObject(obj, top_offset_, debug_hint);
+    }
+  }
+
+  // There is no check against the allowed addresses for bottommost frames, as
+  // the caller's pc could be anything. The caller's pc pushed here should never
+  // be re-signed.
+  void PushBottommostCallerPc(intptr_t pc) {
+    top_offset_ -= kPCOnStackSize;
+    frame_->SetFrameSlot(top_offset_, pc);
+    DebugPrintOutputPc(pc, "bottommost caller's pc\n");
+  }
+
+  void PushApprovedCallerPc(intptr_t pc) {
+    top_offset_ -= kPCOnStackSize;
+    frame_->SetCallerPc(top_offset_, pc);
+    DebugPrintOutputPc(pc, "caller's pc\n");
+  }
+
+  void PushCallerFp(intptr_t fp) {
+    top_offset_ -= kFPOnStackSize;
+    frame_->SetCallerFp(top_offset_, fp);
+    DebugPrintOutputValue(fp, "caller's fp\n");
+  }
+
+  void PushCallerConstantPool(intptr_t cp) {
+    top_offset_ -= kSystemPointerSize;
+    frame_->SetCallerConstantPool(top_offset_, cp);
+    DebugPrintOutputValue(cp, "caller's constant_pool\n");
+  }
+
+  void PushTranslatedValue(const TranslatedFrame::iterator& iterator,
+                           const char* debug_hint = "") {
+    Tagged<Object> obj = iterator->GetRawValue();
+    PushRawObject(obj, debug_hint);
+    if (trace_scope_ != nullptr) {
+      PrintF(trace_scope_->file(), " (input #%d)\n", iterator.input_index());
+    }
+    dumper_->QueueValueForMaterialization(output_address(top_offset_), obj,
+                                               iterator);
+  }
+
+  void PushFeedbackVectorForMaterialization(
+      const TranslatedFrame::iterator& iterator) {
+    // Push a marker temporarily.
+    PushRawObject(ReadOnlyRoots(dumper_->isolate()).arguments_marker(),
+                  "feedback vector");
+    dumper_->QueueFeedbackVectorForMaterialization(
+        output_address(top_offset_), iterator);
+  }
+
+  void PushStackJSArguments(TranslatedFrame::iterator& iterator,
+                            int parameters_count) {
+    std::vector<TranslatedFrame::iterator> parameters;
+    parameters.reserve(parameters_count);
+    for (int i = 0; i < parameters_count; ++i, ++iterator) {
+      parameters.push_back(iterator);
+    }
+    for (auto& parameter : base::Reversed(parameters)) {
+      PushTranslatedValue(parameter, "stack parameter");
+    }
+  }
+
+  unsigned top_offset() const { return top_offset_; }
+
+  FrameDescription* frame() { return frame_; }
+
+ private:
+  void PushValue(intptr_t value) {
+    CHECK_GE(top_offset_, 0);
+    top_offset_ -= kSystemPointerSize;
+    frame_->SetFrameSlot(top_offset_, value);
+  }
+
+  Address output_address(unsigned output_offset) {
+    Address output_address =
+        static_cast<Address>(frame_->GetTop()) + output_offset;
+    return output_address;
+  }
+
+  void DebugPrintOutputValue(intptr_t value, const char* debug_hint = "") {
+    if (trace_scope_ != nullptr) {
+      PrintF(trace_scope_->file(),
+             "    " V8PRIxPTR_FMT ": [top + %3d] <- " V8PRIxPTR_FMT " ;  %s",
+             output_address(top_offset_), top_offset_, value, debug_hint);
+    }
+  }
+
+  void DebugPrintOutputPc(intptr_t value, const char* debug_hint = "") {
+#ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
+    if (trace_scope_ != nullptr) {
+      PrintF(trace_scope_->file(),
+             "    " V8PRIxPTR_FMT ": [top + %3d] <- " V8PRIxPTR_FMT
+             " (signed) " V8PRIxPTR_FMT " (unsigned) ;  %s",
+             output_address(top_offset_), top_offset_, value,
+             PointerAuthentication::StripPAC(value), debug_hint);
+    }
+#else
+    DebugPrintOutputValue(value, debug_hint);
+#endif
+  }
+
+  void DebugPrintOutputObject(Tagged<Object> obj, unsigned output_offset,
+                              const char* debug_hint = "") {
+    if (trace_scope_ != nullptr) {
+      PrintF(trace_scope_->file(), "    " V8PRIxPTR_FMT ": [top + %3d] <- ",
+             output_address(output_offset), output_offset);
+      if (IsSmi(obj)) {
+        PrintF(trace_scope_->file(), V8PRIxPTR_FMT " <Smi %d>", obj.ptr(),
+               Smi::cast(obj).value());
+      } else {
+        ShortPrint(obj, trace_scope_->file());
+      }
+      PrintF(trace_scope_->file(), " ;  %s", debug_hint);
+    }
+  }
+
+  DeoptFrameDumper* dumper_;
+  FrameDescription* frame_;
+  CodeTracer::Scope* const trace_scope_;
+  unsigned top_offset_;
+};
+
+// We rely on this function not causing a GC. It is called from generated code
+// without having a real stack frame in place.
+DeoptFrameDumper* DeoptFrameDumper::New(Address raw_function, DeoptimizeKind kind,
+                              Address from, int fp_to_sp_delta,
+                              Isolate* isolate) {
+  DCHECK(v8_flags.turbofan_dumping || v8_flags.maglev_dumping);
+  Tagged<JSFunction> function = JSFunction::cast(Tagged<Object>(raw_function));
+  DeoptFrameDumper* dumper =
+      new DeoptFrameDumper(isolate, function, kind, from, fp_to_sp_delta);
+  isolate->set_current_deopt_frame_dumper(dumper);
+  return dumper;
+}
+
+DeoptFrameDumper* DeoptFrameDumper::Grab(Isolate* isolate) {
+  DeoptFrameDumper* result = isolate->GetAndClearCurrentDeoptFrameDumper();
+  result->DeleteFrameDescriptions();
+  return result;
+}
+
+
+void DeoptFrameDumper::ComputeOutputFrames(DeoptFrameDumper* dumper_) {
+  dumper_->DoComputeOutputFrames();
+}
+
+
+constexpr int kFrameDumpRegStackSaveBase = 0x13370000 + 0x100;
+
+DeoptFrameDumper::DeoptFrameDumper(Isolate* isolate, Tagged<JSFunction> function,
+                         DeoptimizeKind kind, Address from, int fp_to_sp_delta)
+    : isolate_(isolate),
+      function_(function),
+      deopt_exit_index_(kFixedExitSizeMarker),
+      deopt_kind_(kind),
+      from_(from),
+      fp_to_sp_delta_(fp_to_sp_delta),
+      deoptimizing_throw_(false),
+      catch_handler_data_(-1),
+      catch_handler_pc_offset_(-1),
+      restart_frame_index_(-1),
+      input_(nullptr),
+      output_count_(0),
+      output_(nullptr),
+      caller_frame_top_(0),
+      caller_fp_(0),
+      caller_pc_(0),
+      caller_constant_pool_(0),
+      actual_argument_count_(0),
+      stack_fp_(0),
+      trace_scope_(v8_flags.trace_deopt || v8_flags.log_deopt
+                       ? new CodeTracer::Scope(isolate->GetCodeTracer())
+                       : nullptr) {
+  // For now we can only handle eager deopts
+  DCHECK_EQ(kind, DeoptimizeKind::kEager);
+
+  DCHECK(!isolate->debug()->IsRestartFrameScheduled());
+
+  // DCHECK_NE(from, kNullAddress);
+  compiled_code_ = isolate_->heap()->FindCodeForInnerPointer(from);
+  // DCHECK(!compiled_code_.is_null());
+  // DCHECK(IsCode(compiled_code_));
+
+  // DCHECK(IsJSFunction(function));
+
+  DCHECK(AllowGarbageCollection::IsAllowed());
+  disallow_garbage_collection_ = new DisallowGarbageCollection();
+
+  // DCHECK(CodeKindCanDeoptimize(compiled_code_->kind()));
+
+  unsigned size = ComputeInputFrameSize();
+  uint64_t* saved_frame_size_ptr = (uint64_t*)kFrameDumpRegStackSaveBase;
+
+  DCHECK_LE(size, *saved_frame_size_ptr);
+
+  const int parameter_count =
+      function->shared()->internal_formal_parameter_count_with_receiver();
+  input_ = new (size) FrameDescription(size, parameter_count, isolate_);
+
+
+  Tagged<DeoptimizationData> deopt_data =
+      DeoptimizationData::cast(compiled_code_->deoptimization_data());
+  int deopt_start_offset = deopt_data->DeoptExitStart().value();
+  // int eager_deopt_count = deopt_data->EagerDeoptCount().value();
+
+  // int lazy_deopt_start_offset =
+  //     deopt_start_offset + eager_deopt_count * kEagerDeoptExitSize;
+
+  // parse jump offset before dump call
+  // Seems fragile AF but whatever :pray:
+  static constexpr int kCallBuiltinInstructionSize = 0x5;
+  static constexpr int kCallDeoptSize = 0x6;
+
+  static constexpr int kFromOffToJumpInstruction = kCallBuiltinInstructionSize + kCallDeoptSize;
+  char *deopt_jump_instruction = ((char *)(from_)) - kFromOffToJumpInstruction;
+
+  // Assert that we can't run into near jumps that get encoded differently
+  // Not sure if that holds but with this we should at least notice when stuff breaks
+
+  // DCHECK_EQ(*deopt_jump_instruction, 0x0f);
+  // DCHECK_EQ((*(deopt_jump_instruction + 1)) & 0xf0, 0x80);
+
+  Address instruction_start = compiled_code_->instruction_start();
+
+  int deopt_rel_offset = *((unsigned int*)(deopt_jump_instruction + 2));
+  int deopt_offset = deopt_rel_offset + ((int)(from_ - instruction_start - kCallBuiltinInstructionSize));
+
+  // patch from_ so GetDeoptInfo() still works
+  set_from(static_cast<Address>(instruction_start + deopt_offset));
+
+  // DCHECK_GE(deopt_offset, deopt_start_offset);
+  // DCHECK_LT(deopt_offset, lazy_deopt_start_offset);
+
+  int offset = deopt_offset - deopt_start_offset;
+  DCHECK_EQ(0, offset % kEagerDeoptExitSize);
+  deopt_exit_index_ = offset / kEagerDeoptExitSize;
+}
+
+Handle<JSFunction> DeoptFrameDumper::function() const {
+  return Handle<JSFunction>(function_, isolate());
+}
+
+Handle<Code> DeoptFrameDumper::compiled_code() const {
+  return Handle<Code>(compiled_code_, isolate());
+}
+
+DeoptFrameDumper::~DeoptFrameDumper() {
+  CHECK(input_ == nullptr && output_ == nullptr);
+  CHECK_NULL(disallow_garbage_collection_);
+  delete trace_scope_;
+}
+
+void DeoptFrameDumper::DeleteFrameDescriptions() {
+  delete input_;
+  for (int i = 0; i < output_count_; ++i) {
+    if (output_[i] != input_) delete output_[i];
+  }
+  delete[] output_;
+  input_ = nullptr;
+  output_ = nullptr;
+
+  CHECK(!AllowGarbageCollection::IsAllowed());
+  CHECK_NOT_NULL(disallow_garbage_collection_);
+}
+
+void DeoptFrameDumper::ReenableGC() {
+  delete disallow_garbage_collection_;
+  disallow_garbage_collection_ = nullptr;
+}
+
+
+namespace {
+
+int LookupCatchHandler(Isolate* isolate, TranslatedFrame* translated_frame,
+                       int* data_out) {
+  switch (translated_frame->kind()) {
+    case TranslatedFrame::kUnoptimizedFunction: {
+      int bytecode_offset = translated_frame->bytecode_offset().ToInt();
+      HandlerTable table(
+          translated_frame->raw_shared_info()->GetBytecodeArray(isolate));
+      return table.LookupRange(bytecode_offset, data_out, nullptr);
+    }
+    case TranslatedFrame::kJavaScriptBuiltinContinuationWithCatch: {
+      return 0;
+    }
+    default:
+      break;
+  }
+  return -1;
+}
+
+}  // namespace
+
+
+// We rely on this function not causing a GC.  It is called from generated code
+// without having a real stack frame in place.
+void DeoptFrameDumper::DoComputeOutputFrames() {
+  // When we call this function, the return address of the previous frame has
+  // been removed from the stack by the DeoptimizationEntry builtin, so the
+  // stack is not iterable by the StackFrameIteratorForProfiler.
+  /*
+  #if V8_TARGET_ARCH_STORES_RETURN_ADDRESS_ON_STACK
+    DCHECK_EQ(0, isolate()->isolate_data()->stack_is_iterable());
+  #endif
+  */
+  base::ElapsedTimer timer;
+
+  // Determine basic deoptimization information.  The optimized frame is
+  // described by the input data.
+  Tagged<DeoptimizationData> input_data =
+      DeoptimizationData::cast(compiled_code_->deoptimization_data());
+
+  {
+    // Read caller's PC, caller's FP and caller's constant pool values
+    // from input frame. Compute caller's frame top address.
+
+    Register fp_reg = JavaScriptFrame::fp_register();
+    stack_fp_ = input_->GetRegister(fp_reg.code());
+
+    caller_frame_top_ = stack_fp_ + ComputeInputFrameAboveFpFixedSize();
+
+    Address fp_address = input_->GetFramePointerAddress();
+    caller_fp_ = Memory<intptr_t>(fp_address);
+    caller_pc_ =
+        Memory<intptr_t>(fp_address + CommonFrameConstants::kCallerPCOffset);
+    actual_argument_count_ = static_cast<int>(
+        Memory<intptr_t>(fp_address + StandardFrameConstants::kArgCOffset));
+
+    if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+      caller_constant_pool_ = Memory<intptr_t>(
+          fp_address + CommonFrameConstants::kConstantPoolOffset);
+    }
+  }
+
+  StackGuard* const stack_guard = isolate()->stack_guard();
+  CHECK_GT(static_cast<uintptr_t>(caller_frame_top_),
+           stack_guard->real_jslimit());
+
+  // BytecodeOffset bytecode_offset =
+  //     input_data->GetBytecodeOffsetOrBuiltinContinuationId(deopt_exit_index_);
+  auto translations = input_data->FrameTranslation();
+  unsigned translation_index =
+      input_data->TranslationIndex(deopt_exit_index_).value();
+
+  FILE* trace_file =
+      verbose_tracing_enabled() ? trace_scope()->file() : nullptr;
+  DeoptimizationFrameTranslation::Iterator state_iterator(translations,
+                                                          translation_index);
+  translated_state_.Init(
+      isolate_, input_->GetFramePointerAddress(), stack_fp_, &state_iterator,
+      input_data->LiteralArray(), input_->GetRegisterValues(), trace_file,
+      IsHeapObject(function_)
+          ? function_->shared()
+                ->internal_formal_parameter_count_without_receiver()
+          : 0,
+      actual_argument_count_ - kJSArgcReceiverSlots);
+
+  bytecode_offset_in_outermost_frame_ =
+      translated_state_.frames()[0].bytecode_offset();
+
+  // Do the input frame to output frame(s) translation.
+  size_t count = translated_state_.frames().size();
+
+  if (is_restart_frame()) {
+    // If the debugger requested to restart a particular frame, only materialize
+    // up to that frame.
+    count = restart_frame_index_ + 1;
+  } else if (deoptimizing_throw_) {
+    // If we are supposed to go to the catch handler, find the catching frame
+    // for the catch and make sure we only deoptimize up to that frame.
+    size_t catch_handler_frame_index = count;
+    for (size_t i = count; i-- > 0;) {
+      catch_handler_pc_offset_ = LookupCatchHandler(
+          isolate(), &(translated_state_.frames()[i]), &catch_handler_data_);
+      if (catch_handler_pc_offset_ >= 0) {
+        catch_handler_frame_index = i;
+        break;
+      }
+    }
+    CHECK_LT(catch_handler_frame_index, count);
+    count = catch_handler_frame_index + 1;
+  }
+
+  DCHECK_NULL(output_);
+  output_ = new FrameDescription*[count];
+  for (size_t i = 0; i < count; ++i) {
+    output_[i] = nullptr;
+  }
+
+  output_count_ = static_cast<int>(count);
+  bytecode_offset_in_innermost_frame_ =
+    translated_state_.frames()[output_count_ - 1].bytecode_offset();
+
+  // Translate each output frame.
+  int frame_index = 0;
+  size_t total_output_frame_size = 0;
+  for (size_t i = 0; i < count; ++i, ++frame_index) {
+    TranslatedFrame* translated_frame = &(translated_state_.frames()[i]);
+    const bool handle_exception = deoptimizing_throw_ && i == count - 1;
+
+    if (v8_flags.turbofan_dumping && i == count - 1) {
+      // for now we only handle unoptimized *top* frames
+      CHECK(translated_frame->kind() == TranslatedFrame::kUnoptimizedFunction);
+    }
+
+    switch (translated_frame->kind()) {
+      case TranslatedFrame::kUnoptimizedFunction:
+        DoComputeUnoptimizedFrame(translated_frame, frame_index,
+                                  handle_exception);
+        break;
+      case TranslatedFrame::kInlinedExtraArguments:
+        DoComputeInlinedExtraArguments(translated_frame, frame_index);
+        break;
+      case TranslatedFrame::kConstructCreateStub:
+        DoComputeConstructCreateStubFrame(translated_frame, frame_index);
+        break;
+      case TranslatedFrame::kConstructInvokeStub:
+        DoComputeConstructInvokeStubFrame(translated_frame, frame_index);
+        break;
+      case TranslatedFrame::kBuiltinContinuation:
+#if V8_ENABLE_WEBASSEMBLY
+      case TranslatedFrame::kJSToWasmBuiltinContinuation:
+#endif  // V8_ENABLE_WEBASSEMBLY
+        DoComputeBuiltinContinuation(translated_frame, frame_index,
+                                     BuiltinContinuationMode::STUB);
+        break;
+      case TranslatedFrame::kJavaScriptBuiltinContinuation:
+        DoComputeBuiltinContinuation(translated_frame, frame_index,
+                                     BuiltinContinuationMode::JAVASCRIPT);
+        break;
+      case TranslatedFrame::kJavaScriptBuiltinContinuationWithCatch:
+        DoComputeBuiltinContinuation(
+            translated_frame, frame_index,
+            handle_exception
+                ? BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION
+                : BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH);
+        break;
+#if V8_ENABLE_WEBASSEMBLY
+      case TranslatedFrame::kWasmInlinedIntoJS:
+        FATAL("inlined wasm frames may not appear in JS deopts");
+      case TranslatedFrame::kLiftoffFunction:
+        FATAL("wasm liftoff frames may not appear in JS deopts");
+#endif
+      case TranslatedFrame::kInvalid:
+        FATAL("invalid frame");
+    }
+    total_output_frame_size += output_[frame_index]->GetFrameSize();
+  }
+
+  FrameDescription* topmost = output_[count - 1];
+  topmost->GetRegisterValues()->SetRegister(kRootRegister.code(),
+                                            isolate()->isolate_root());
+#ifdef V8_COMPRESS_POINTERS
+  topmost->GetRegisterValues()->SetRegister(kPtrComprCageBaseRegister.code(),
+                                            isolate()->cage_base());
+#endif
+
+  /*
+  // Don't reset the tiering state for OSR code since we might reuse OSR code
+  // after deopt, and we still want to tier up to non-OSR code even if OSR code
+  // deoptimized.
+  bool osr_early_exit = Deoptimizer::GetDeoptInfo().deopt_reason ==
+                        DeoptimizeReason::kOSREarlyExit;
+  if (IsJSFunction(function_) &&
+      (compiled_code_->osr_offset().IsNone()
+           ? function_->code() == compiled_code_
+           : (!osr_early_exit &&
+              DeoptExitIsInsideOsrLoop(isolate(), function_,
+                                       bytecode_offset_in_outermost_frame_,
+                                       compiled_code_->osr_offset())))) {
+    function_->reset_tiering_state();
+    function_->SetInterruptBudget(isolate_, CodeKind::INTERPRETED_FUNCTION);
+  }
+  */
+
+  // Print some helpful diagnostic information.
+  /*
+  if (verbose_tracing_enabled()) {
+    TraceDeoptEnd(timer.Elapsed().InMillisecondsF());
+  }
+  */
+
+  // The following invariant is fairly tricky to guarantee, since the size of
+  // an optimized frame and its deoptimized counterparts usually differs. We
+  // thus need to consider the case in which deoptimized frames are larger than
+  // the optimized frame in stack checks in optimized code. We do this by
+  // applying an offset to stack checks (see kArchStackPointerGreaterThan in the
+  // code generator).
+  // Note that we explicitly allow deopts to exceed the limit by a certain
+  // number of slack bytes.
+  CHECK_GT(
+      static_cast<uintptr_t>(caller_frame_top_) - total_output_frame_size,
+      stack_guard->real_jslimit() - kStackLimitSlackForDeoptimizationInBytes);
+}
+
+// static
+bool DeoptFrameDumper::DeoptExitIsInsideOsrLoop(Isolate* isolate,
+                                           Tagged<JSFunction> function,
+                                           BytecodeOffset deopt_exit_offset,
+                                           BytecodeOffset osr_offset) {
+  DisallowGarbageCollection no_gc;
+  HandleScope scope(isolate);
+  DCHECK(!deopt_exit_offset.IsNone());
+  DCHECK(!osr_offset.IsNone());
+
+  Handle<BytecodeArray> bytecode_array(
+      function->shared()->GetBytecodeArray(isolate), isolate);
+  DCHECK(interpreter::BytecodeArrayIterator::IsValidOffset(
+      bytecode_array, deopt_exit_offset.ToInt()));
+
+  interpreter::BytecodeArrayIterator it(bytecode_array, osr_offset.ToInt());
+  DCHECK_EQ(it.current_bytecode(), interpreter::Bytecode::kJumpLoop);
+
+  for (; !it.done(); it.Advance()) {
+    const int current_offset = it.current_offset();
+    // If we've reached the deopt exit, it's contained in the current loop
+    // (this is covered by IsInRange below, but this check lets us avoid
+    // useless iteration).
+    if (current_offset == deopt_exit_offset.ToInt()) return true;
+    // We're only interested in loop ranges.
+    if (it.current_bytecode() != interpreter::Bytecode::kJumpLoop) continue;
+    // Is the deopt exit contained in the current loop?
+    if (base::IsInRange(deopt_exit_offset.ToInt(), it.GetJumpTargetOffset(),
+                        current_offset)) {
+      return true;
+    }
+    // We've reached nesting level 0, i.e. the current JumpLoop concludes a
+    // top-level loop.
+    const int loop_nesting_level = it.GetImmediateOperand(1);
+    if (loop_nesting_level == 0) return false;
+  }
+
+  UNREACHABLE();
+}
+namespace {
+
+// Get the dispatch builtin for unoptimized frames.
+Builtin DispatchBuiltinFor(bool deopt_to_baseline, bool advance_bc,
+                           bool is_restart_frame) {
+  if (is_restart_frame) return Builtin::kRestartFrameTrampoline;
+
+  if (deopt_to_baseline) {
+    return advance_bc ? Builtin::kBaselineOrInterpreterEnterAtNextBytecode
+                      : Builtin::kBaselineOrInterpreterEnterAtBytecode;
+  } else {
+    return advance_bc ? Builtin::kInterpreterEnterAtNextBytecode
+                      : Builtin::kInterpreterEnterAtBytecode;
+  }
+}
+
+}  // namespace
+
+void DeoptFrameDumper::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
+                                            int frame_index,
+                                            bool goto_catch_handler) {
+  Tagged<SharedFunctionInfo> shared = translated_frame->raw_shared_info();
+  TranslatedFrame::iterator value_iterator = translated_frame->begin();
+  const bool is_bottommost = (0 == frame_index);
+  const bool is_topmost = (output_count_ - 1 == frame_index);
+
+  const int real_bytecode_offset = translated_frame->bytecode_offset().ToInt();
+  const int bytecode_offset =
+      goto_catch_handler ? catch_handler_pc_offset_ : real_bytecode_offset;
+
+  const int parameters_count =
+      shared->internal_formal_parameter_count_with_receiver();
+
+  // If this is the bottom most frame or the previous frame was the inlined
+  // extra arguments frame, then we already have extra arguments in the stack
+  // (including any extra padding). Therefore we should not try to add any
+  // padding.
+  bool should_pad_arguments =
+      !is_bottommost && (translated_state_.frames()[frame_index - 1]).kind() !=
+                            TranslatedFrame::kInlinedExtraArguments;
+
+  const int locals_count = translated_frame->height();
+  UnoptimizedFrameInfo frame_info = UnoptimizedFrameInfo::Precise(
+      parameters_count, locals_count, is_topmost, should_pad_arguments);
+  const uint32_t output_frame_size = frame_info.frame_size_in_bytes();
+
+  TranslatedFrame::iterator function_iterator = value_iterator++;
+
+  Tagged<BytecodeArray> bytecode_array;
+  base::Optional<Tagged<DebugInfo>> debug_info =
+      shared->TryGetDebugInfo(isolate());
+  if (debug_info.has_value() && debug_info.value()->HasBreakInfo()) {
+    bytecode_array = debug_info.value()->DebugBytecodeArray(isolate());
+  } else {
+    bytecode_array = shared->GetBytecodeArray(isolate());
+  }
+
+  // Allocate and store the output frame description.
+  FrameDescription* output_frame = new (output_frame_size)
+      FrameDescription(output_frame_size, parameters_count, isolate());
+  FrameWriter frame_writer(this, output_frame, verbose_trace_scope());
+
+  CHECK(frame_index >= 0 && frame_index < output_count_);
+  CHECK_NULL(output_[frame_index]);
+  output_[frame_index] = output_frame;
+
+  // Compute this frame's PC and state.
+  // For interpreted frames, the PC will be a special builtin that
+  // continues the bytecode dispatch. Note that non-topmost and lazy-style
+  // bailout handlers also advance the bytecode offset before dispatch, hence
+  // simulating what normal handlers do upon completion of the operation.
+  // For baseline frames, the PC will be a builtin to convert the interpreter
+  // frame to a baseline frame before continuing execution of baseline code.
+  // We can't directly continue into baseline code, because of CFI.
+  Builtins* builtins = isolate_->builtins();
+  const bool advance_bc =
+      (!is_topmost || (deopt_kind_ == DeoptimizeKind::kLazy)) &&
+      !goto_catch_handler;
+  const bool deopt_to_baseline =
+      shared->HasBaselineCode() && v8_flags.deopt_to_baseline;
+  const bool restart_frame = goto_catch_handler && is_restart_frame();
+  Tagged<Code> dispatch_builtin = builtins->code(
+      DispatchBuiltinFor(deopt_to_baseline, advance_bc, restart_frame));
+
+  // The top address of the frame is computed from the previous frame's top and
+  // this frame's size.
+  const intptr_t top_address =
+      is_bottommost ? caller_frame_top_ - output_frame_size
+                    : output_[frame_index - 1]->GetTop() - output_frame_size;
+  output_frame->SetTop(top_address);
+
+  // Compute the incoming parameter translation.
+  ReadOnlyRoots roots(isolate());
+  if (should_pad_arguments) {
+    for (int i = 0; i < ArgumentPaddingSlots(parameters_count); ++i) {
+      frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+    }
+  }
+
+  frame_writer.PushStackJSArguments(value_iterator, parameters_count);
+
+  DCHECK_EQ(output_frame->GetLastArgumentSlotOffset(should_pad_arguments),
+            frame_writer.top_offset());
+
+  // There are no translation commands for the caller's pc and fp, the
+  // context, the function and the bytecode offset.  Synthesize
+  // their values and set them up
+  // explicitly.
+  //
+  // The caller's pc for the bottommost output frame is the same as in the
+  // input frame. For all subsequent output frames, it can be read from the
+  // previous one. This frame's pc can be computed from the non-optimized
+  // function code and bytecode offset of the bailout.
+  if (is_bottommost) {
+    frame_writer.PushBottommostCallerPc(caller_pc_);
+  } else {
+    frame_writer.PushApprovedCallerPc(output_[frame_index - 1]->GetPc());
+  }
+
+  // The caller's frame pointer for the bottommost output frame is the same
+  // as in the input frame.  For all subsequent output frames, it can be
+  // read from the previous one.  Also compute and set this frame's frame
+  // pointer.
+  const intptr_t caller_fp =
+      is_bottommost ? caller_fp_ : output_[frame_index - 1]->GetFp();
+  frame_writer.PushCallerFp(caller_fp);
+
+  const intptr_t fp_value = top_address + frame_writer.top_offset();
+  output_frame->SetFp(fp_value);
+  if (is_topmost) {
+    Register fp_reg = UnoptimizedFrame::fp_register();
+    output_frame->SetRegister(fp_reg.code(), fp_value);
+  }
+
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    // For the bottommost output frame the constant pool pointer can be gotten
+    // from the input frame. For subsequent output frames, it can be read from
+    // the previous frame.
+    const intptr_t caller_cp =
+        is_bottommost ? caller_constant_pool_
+                      : output_[frame_index - 1]->GetConstantPool();
+    frame_writer.PushCallerConstantPool(caller_cp);
+  }
+
+  // For the bottommost output frame the context can be gotten from the input
+  // frame. For all subsequent output frames it can be gotten from the function
+  // so long as we don't inline functions that need local contexts.
+
+  // When deoptimizing into a catch block, we need to take the context
+  // from a register that was specified in the handler table.
+  TranslatedFrame::iterator context_pos = value_iterator++;
+  if (goto_catch_handler) {
+    // Skip to the translated value of the register specified
+    // in the handler table.
+    for (int i = 0; i < catch_handler_data_ + 1; ++i) {
+      context_pos++;
+    }
+  }
+  // Read the context from the translations.
+  Tagged<Object> context = context_pos->GetRawValue();
+  output_frame->SetContext(static_cast<intptr_t>(context.ptr()));
+  frame_writer.PushTranslatedValue(context_pos, "context");
+
+  // The function was mentioned explicitly in the BEGIN_FRAME.
+  frame_writer.PushTranslatedValue(function_iterator, "function");
+
+  // Actual argument count.
+  int argc;
+  if (is_bottommost) {
+    argc = actual_argument_count_;
+  } else {
+    TranslatedFrame::Kind previous_frame_kind =
+        (translated_state_.frames()[frame_index - 1]).kind();
+    argc = previous_frame_kind == TranslatedFrame::kInlinedExtraArguments
+               ? output_[frame_index - 1]->parameter_count()
+               : parameters_count;
+  }
+  frame_writer.PushRawValue(argc, "actual argument count\n");
+
+  // Set the bytecode array pointer.
+  frame_writer.PushRawObject(bytecode_array, "bytecode array\n");
+
+  // The bytecode offset was mentioned explicitly in the BEGIN_FRAME.
+  const int raw_bytecode_offset =
+      BytecodeArray::kHeaderSize - kHeapObjectTag + bytecode_offset;
+  Tagged<Smi> smi_bytecode_offset = Smi::FromInt(raw_bytecode_offset);
+  frame_writer.PushRawObject(smi_bytecode_offset, "bytecode offset\n");
+
+  // We need to materialize the closure before getting the feedback vector.
+  frame_writer.PushFeedbackVectorForMaterialization(function_iterator);
+
+  if (verbose_tracing_enabled()) {
+    PrintF(trace_scope()->file(), "    -------------------------\n");
+  }
+
+  // Translate the rest of the interpreter registers in the frame.
+  // The return_value_offset is counted from the top. Here, we compute the
+  // register index (counted from the start).
+  const int return_value_first_reg =
+      locals_count - translated_frame->return_value_offset();
+  const int return_value_count = translated_frame->return_value_count();
+  for (int i = 0; i < locals_count; ++i, ++value_iterator) {
+    // Ensure we write the return value if we have one and we are returning
+    // normally to a lazy deopt point.
+    if (is_topmost && !goto_catch_handler &&
+        deopt_kind_ == DeoptimizeKind::kLazy && i >= return_value_first_reg &&
+        i < return_value_first_reg + return_value_count) {
+      const int return_index = i - return_value_first_reg;
+      if (return_index == 0) {
+        frame_writer.PushRawValue(input_->GetRegister(kReturnRegister0.code()),
+                                  "return value 0\n");
+        // We do not handle the situation when one return value should go into
+        // the accumulator and another one into an ordinary register. Since
+        // the interpreter should never create such situation, just assert
+        // this does not happen.
+        CHECK_LE(return_value_first_reg + return_value_count, locals_count);
+      } else {
+        CHECK_EQ(return_index, 1);
+        frame_writer.PushRawValue(input_->GetRegister(kReturnRegister1.code()),
+                                  "return value 1\n");
+      }
+    } else {
+      // This is not return value, just write the value from the translations.
+      frame_writer.PushTranslatedValue(value_iterator, "stack parameter");
+    }
+  }
+
+  uint32_t register_slots_written = static_cast<uint32_t>(locals_count);
+  DCHECK_LE(register_slots_written, frame_info.register_stack_slot_count());
+  // Some architectures must pad the stack frame with extra stack slots
+  // to ensure the stack frame is aligned. Do this now.
+  while (register_slots_written < frame_info.register_stack_slot_count()) {
+    register_slots_written++;
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+
+  // Translate the accumulator register (depending on frame position).
+  if (is_topmost) {
+    for (int i = 0; i < ArgumentPaddingSlots(1); ++i) {
+      frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+    }
+    // For topmost frame, put the accumulator on the stack. The
+    // {NotifyDeoptimized} builtin pops it off the topmost frame (possibly
+    // after materialization).
+    if (goto_catch_handler) {
+      // If we are lazy deopting to a catch handler, we set the accumulator to
+      // the exception (which lives in the result register).
+      intptr_t accumulator_value =
+          input_->GetRegister(kInterpreterAccumulatorRegister.code());
+      frame_writer.PushRawObject(Tagged<Object>(accumulator_value),
+                                 "accumulator\n");
+    } else {
+      // If we are lazily deoptimizing make sure we store the deopt
+      // return value into the appropriate slot.
+      if (deopt_kind_ == DeoptimizeKind::kLazy &&
+          translated_frame->return_value_offset() == 0 &&
+          translated_frame->return_value_count() > 0) {
+        CHECK_EQ(translated_frame->return_value_count(), 1);
+        frame_writer.PushRawValue(input_->GetRegister(kReturnRegister0.code()),
+                                  "return value 0\n");
+      } else {
+        frame_writer.PushTranslatedValue(value_iterator, "accumulator");
+      }
+    }
+    ++value_iterator;  // Move over the accumulator.
+  } else {
+    // For non-topmost frames, skip the accumulator translation. For those
+    // frames, the return value from the callee will become the accumulator.
+    ++value_iterator;
+  }
+  CHECK_EQ(translated_frame->end(), value_iterator);
+  CHECK_EQ(0u, frame_writer.top_offset());
+
+  const intptr_t pc =
+      static_cast<intptr_t>(dispatch_builtin->instruction_start());
+  if (is_topmost) {
+    // Only the pc of the topmost frame needs to be signed since it is
+    // authenticated at the end of the DeoptimizationEntry builtin.
+    const intptr_t top_most_pc = PointerAuthentication::SignAndCheckPC(
+        isolate(), pc, frame_writer.frame()->GetTop());
+    output_frame->SetPc(top_most_pc);
+  } else {
+    output_frame->SetPc(pc);
+  }
+
+  // Update constant pool.
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    intptr_t constant_pool_value =
+        static_cast<intptr_t>(dispatch_builtin->constant_pool());
+    output_frame->SetConstantPool(constant_pool_value);
+    if (is_topmost) {
+      Register constant_pool_reg =
+          UnoptimizedFrame::constant_pool_pointer_register();
+      output_frame->SetRegister(constant_pool_reg.code(), constant_pool_value);
+    }
+  }
+
+  // Clear the context register. The context might be a de-materialized object
+  // and will be materialized by {Runtime_NotifyDeoptimized}. For additional
+  // safety we use Tagged<Smi>(0) instead of the potential {arguments_marker}
+  // here.
+  if (is_topmost) {
+    intptr_t context_value = static_cast<intptr_t>(Smi::zero().ptr());
+    Register context_reg = JavaScriptFrame::context_register();
+    output_frame->SetRegister(context_reg.code(), context_value);
+    // Set the continuation for the topmost frame.
+    Tagged<Code> continuation = builtins->code(Builtin::kNotifyDumpFrame);
+    output_frame->SetContinuation(
+        static_cast<intptr_t>(continuation->instruction_start()));
+  }
+}
+
+void DeoptFrameDumper::DoComputeInlinedExtraArguments(
+    TranslatedFrame* translated_frame, int frame_index) {
+  // Inlined arguments frame can not be the topmost, nor the bottom most frame.
+  CHECK(frame_index < output_count_ - 1);
+  CHECK_GT(frame_index, 0);
+  CHECK_NULL(output_[frame_index]);
+
+  // During deoptimization we need push the extra arguments of inlined functions
+  // (arguments with index greater than the formal parameter count).
+  // For more info, see the design document:
+  // https://docs.google.com/document/d/150wGaUREaZI6YWqOQFD5l2mWQXaPbbZjcAIJLOFrzMs
+
+  TranslatedFrame::iterator value_iterator = translated_frame->begin();
+  const int argument_count_without_receiver = translated_frame->height() - 1;
+  const int formal_parameter_count =
+      translated_frame->raw_shared_info()
+          ->internal_formal_parameter_count_without_receiver();
+  const int extra_argument_count =
+      argument_count_without_receiver - formal_parameter_count;
+  // The number of pushed arguments is the maximum of the actual argument count
+  // and the formal parameter count + the receiver.
+  const int padding = ArgumentPaddingSlots(
+      std::max(argument_count_without_receiver, formal_parameter_count) + 1);
+  const int output_frame_size =
+      (std::max(0, extra_argument_count) + padding) * kSystemPointerSize;
+  if (verbose_tracing_enabled()) {
+    PrintF(trace_scope_->file(),
+           "  translating inlined arguments frame => variable_size=%d\n",
+           output_frame_size);
+  }
+
+  // Allocate and store the output frame description.
+  FrameDescription* output_frame = new (output_frame_size) FrameDescription(
+      output_frame_size, JSParameterCount(argument_count_without_receiver),
+      isolate());
+  // The top address of the frame is computed from the previous frame's top and
+  // this frame's size.
+  const intptr_t top_address =
+      output_[frame_index - 1]->GetTop() - output_frame_size;
+  output_frame->SetTop(top_address);
+  // This is not a real frame, we take PC and FP values from the parent frame.
+  output_frame->SetPc(output_[frame_index - 1]->GetPc());
+  output_frame->SetFp(output_[frame_index - 1]->GetFp());
+  output_[frame_index] = output_frame;
+
+  FrameWriter frame_writer(this, output_frame, verbose_trace_scope());
+
+  ReadOnlyRoots roots(isolate());
+  for (int i = 0; i < padding; ++i) {
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+  if (extra_argument_count > 0) {
+    // The receiver and arguments with index below the formal parameter
+    // count are in the fake adaptor frame, because they are used to create the
+    // arguments object. We should however not push them, since the interpreter
+    // frame with do that.
+    value_iterator++;  // Skip function.
+    value_iterator++;  // Skip receiver.
+    for (int i = 0; i < formal_parameter_count; i++) value_iterator++;
+    frame_writer.PushStackJSArguments(value_iterator, extra_argument_count);
+  }
+}
+
+void DeoptFrameDumper::DoComputeConstructCreateStubFrame(
+    TranslatedFrame* translated_frame, int frame_index) {
+  TranslatedFrame::iterator value_iterator = translated_frame->begin();
+  const bool is_topmost = (output_count_ - 1 == frame_index);
+  // The construct frame could become topmost only if we inlined a constructor
+  // call which does a tail call (otherwise the tail callee's frame would be
+  // the topmost one). So it could only be the DeoptimizeKind::kLazy case.
+  CHECK(!is_topmost || deopt_kind_ == DeoptimizeKind::kLazy);
+  DCHECK_EQ(translated_frame->kind(), TranslatedFrame::kConstructCreateStub);
+
+  const int parameters_count = translated_frame->height();
+  ConstructStubFrameInfo frame_info =
+      ConstructStubFrameInfo::Precise(parameters_count, is_topmost);
+  const uint32_t output_frame_size = frame_info.frame_size_in_bytes();
+
+  TranslatedFrame::iterator function_iterator = value_iterator++;
+  if (verbose_tracing_enabled()) {
+    PrintF(trace_scope()->file(),
+           "  translating construct create stub => variable_frame_size=%d, "
+           "frame_size=%d\n",
+           frame_info.frame_size_in_bytes_without_fixed(), output_frame_size);
+  }
+
+  // Allocate and store the output frame description.
+  FrameDescription* output_frame = new (output_frame_size)
+      FrameDescription(output_frame_size, parameters_count, isolate());
+  FrameWriter frame_writer(this, output_frame, verbose_trace_scope());
+  DCHECK(frame_index > 0 && frame_index < output_count_);
+  DCHECK_NULL(output_[frame_index]);
+  output_[frame_index] = output_frame;
+
+  // The top address of the frame is computed from the previous frame's top and
+  // this frame's size.
+  const intptr_t top_address =
+      output_[frame_index - 1]->GetTop() - output_frame_size;
+  output_frame->SetTop(top_address);
+
+  ReadOnlyRoots roots(isolate());
+  for (int i = 0; i < ArgumentPaddingSlots(parameters_count); ++i) {
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+  // The allocated receiver of a construct stub frame is passed as the
+  // receiver parameter through the translation. It might be encoding
+  // a captured object, so we need save it for later.
+  TranslatedFrame::iterator receiver_iterator = value_iterator;
+
+  // Compute the incoming parameter translation.
+  frame_writer.PushStackJSArguments(value_iterator, parameters_count);
+
+  DCHECK_EQ(output_frame->GetLastArgumentSlotOffset(),
+            frame_writer.top_offset());
+
+  // Read caller's PC from the previous frame.
+  const intptr_t caller_pc = output_[frame_index - 1]->GetPc();
+  frame_writer.PushApprovedCallerPc(caller_pc);
+
+  // Read caller's FP from the previous frame, and set this frame's FP.
+  const intptr_t caller_fp = output_[frame_index - 1]->GetFp();
+  frame_writer.PushCallerFp(caller_fp);
+
+  const intptr_t fp_value = top_address + frame_writer.top_offset();
+  output_frame->SetFp(fp_value);
+  if (is_topmost) {
+    Register fp_reg = JavaScriptFrame::fp_register();
+    output_frame->SetRegister(fp_reg.code(), fp_value);
+  }
+
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    // Read the caller's constant pool from the previous frame.
+    const intptr_t caller_cp = output_[frame_index - 1]->GetConstantPool();
+    frame_writer.PushCallerConstantPool(caller_cp);
+  }
+
+  // A marker value is used to mark the frame.
+  intptr_t marker = StackFrame::TypeToMarker(StackFrame::CONSTRUCT);
+  frame_writer.PushRawValue(marker, "context (construct stub sentinel)\n");
+
+  frame_writer.PushTranslatedValue(value_iterator++, "context");
+
+  // Number of incoming arguments.
+  const uint32_t argc = parameters_count;
+  frame_writer.PushRawObject(Smi::FromInt(argc), "argc\n");
+
+  // The constructor function was mentioned explicitly in the
+  // CONSTRUCT_STUB_FRAME.
+  frame_writer.PushTranslatedValue(function_iterator, "constructor function\n");
+
+  // The deopt info contains the implicit receiver or the new target at the
+  // position of the receiver. Copy it to the top of stack, with the hole value
+  // as padding to maintain alignment.
+  frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  frame_writer.PushTranslatedValue(receiver_iterator, "new target\n");
+
+  if (is_topmost) {
+    for (int i = 0; i < ArgumentPaddingSlots(1); ++i) {
+      frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+    }
+    // Ensure the result is restored back when we return to the stub.
+    Register result_reg = kReturnRegister0;
+    intptr_t result = input_->GetRegister(result_reg.code());
+    frame_writer.PushRawValue(result, "subcall result\n");
+  }
+
+  CHECK_EQ(translated_frame->end(), value_iterator);
+  CHECK_EQ(0u, frame_writer.top_offset());
+
+  // Compute this frame's PC.
+  Tagged<Code> construct_stub =
+      isolate_->builtins()->code(Builtin::kJSConstructStubGeneric);
+  Address start = construct_stub->instruction_start();
+  const int pc_offset =
+      isolate_->heap()->construct_stub_create_deopt_pc_offset().value();
+  intptr_t pc_value = static_cast<intptr_t>(start + pc_offset);
+  if (is_topmost) {
+    // Only the pc of the topmost frame needs to be signed since it is
+    // authenticated at the end of the DeoptimizationEntry builtin.
+    output_frame->SetPc(PointerAuthentication::SignAndCheckPC(
+        isolate(), pc_value, frame_writer.frame()->GetTop()));
+  } else {
+    output_frame->SetPc(pc_value);
+  }
+
+  // Update constant pool.
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    intptr_t constant_pool_value =
+        static_cast<intptr_t>(construct_stub->constant_pool());
+    output_frame->SetConstantPool(constant_pool_value);
+    if (is_topmost) {
+      Register constant_pool_reg =
+          JavaScriptFrame::constant_pool_pointer_register();
+      output_frame->SetRegister(constant_pool_reg.code(), constant_pool_value);
+    }
+  }
+
+  // Clear the context register. The context might be a de-materialized object
+  // and will be materialized by {Runtime_NotifyDeoptimized}. For additional
+  // safety we use Tagged<Smi>(0) instead of the potential {arguments_marker}
+  // here.
+  if (is_topmost) {
+    intptr_t context_value = static_cast<intptr_t>(Smi::zero().ptr());
+    Register context_reg = JavaScriptFrame::context_register();
+    output_frame->SetRegister(context_reg.code(), context_value);
+
+    // Set the continuation for the topmost frame.
+    DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
+    Tagged<Code> continuation =
+        isolate_->builtins()->code(Builtin::kNotifyDumpFrame);
+    output_frame->SetContinuation(
+        static_cast<intptr_t>(continuation->instruction_start()));
+  }
+}
+
+void DeoptFrameDumper::DoComputeConstructInvokeStubFrame(
+    TranslatedFrame* translated_frame, int frame_index) {
+  TranslatedFrame::iterator value_iterator = translated_frame->begin();
+  const bool is_topmost = (output_count_ - 1 == frame_index);
+  // The construct frame could become topmost only if we inlined a constructor
+  // call which does a tail call (otherwise the tail callee's frame would be
+  // the topmost one). So it could only be the DeoptimizeKind::kLazy case.
+  CHECK(!is_topmost || deopt_kind_ == DeoptimizeKind::kLazy);
+  DCHECK_EQ(translated_frame->kind(), TranslatedFrame::kConstructInvokeStub);
+  DCHECK_EQ(translated_frame->height(), 0);
+
+  FastConstructStubFrameInfo frame_info =
+      FastConstructStubFrameInfo::Precise(is_topmost);
+  const uint32_t output_frame_size = frame_info.frame_size_in_bytes();
+  if (verbose_tracing_enabled()) {
+    PrintF(trace_scope()->file(),
+           "  translating construct invoke stub => variable_frame_size=%d, "
+           "frame_size=%d\n",
+           frame_info.frame_size_in_bytes_without_fixed(), output_frame_size);
+  }
+
+  // Allocate and store the output frame description.
+  FrameDescription* output_frame =
+      new (output_frame_size) FrameDescription(output_frame_size, 0, isolate());
+  FrameWriter frame_writer(this, output_frame, verbose_trace_scope());
+  DCHECK(frame_index > 0 && frame_index < output_count_);
+  DCHECK_NULL(output_[frame_index]);
+  output_[frame_index] = output_frame;
+
+  // The top address of the frame is computed from the previous frame's top and
+  // this frame's size.
+  const intptr_t top_address =
+      output_[frame_index - 1]->GetTop() - output_frame_size;
+  output_frame->SetTop(top_address);
+
+  // The allocated receiver of a construct stub frame is passed as the
+  // receiver parameter through the translation. It might be encoding
+  // a captured object, so we need save it for later.
+  TranslatedFrame::iterator receiver_iterator = value_iterator;
+  value_iterator++;
+
+  // Read caller's PC from the previous frame.
+  const intptr_t caller_pc = output_[frame_index - 1]->GetPc();
+  frame_writer.PushApprovedCallerPc(caller_pc);
+
+  // Read caller's FP from the previous frame, and set this frame's FP.
+  const intptr_t caller_fp = output_[frame_index - 1]->GetFp();
+  frame_writer.PushCallerFp(caller_fp);
+
+  const intptr_t fp_value = top_address + frame_writer.top_offset();
+  output_frame->SetFp(fp_value);
+  if (is_topmost) {
+    Register fp_reg = JavaScriptFrame::fp_register();
+    output_frame->SetRegister(fp_reg.code(), fp_value);
+  }
+
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    // Read the caller's constant pool from the previous frame.
+    const intptr_t caller_cp = output_[frame_index - 1]->GetConstantPool();
+    frame_writer.PushCallerConstantPool(caller_cp);
+  }
+  intptr_t marker = StackFrame::TypeToMarker(StackFrame::FAST_CONSTRUCT);
+  frame_writer.PushRawValue(marker, "fast construct stub sentinel\n");
+  frame_writer.PushTranslatedValue(value_iterator++, "context");
+  frame_writer.PushTranslatedValue(receiver_iterator, "implicit receiver");
+
+  // The FastConstructFrame needs to be aligned in some architectures.
+  ReadOnlyRoots roots(isolate());
+  for (int i = 0; i < ArgumentPaddingSlots(1); ++i) {
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+  if (is_topmost) {
+    for (int i = 0; i < ArgumentPaddingSlots(1); ++i) {
+      frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+    }
+    // Ensure the result is restored back when we return to the stub.
+    Register result_reg = kReturnRegister0;
+    intptr_t result = input_->GetRegister(result_reg.code());
+    frame_writer.PushRawValue(result, "subcall result\n");
+  }
+
+  CHECK_EQ(translated_frame->end(), value_iterator);
+  CHECK_EQ(0u, frame_writer.top_offset());
+
+  // Compute this frame's PC.
+  Tagged<Code> construct_stub = isolate_->builtins()->code(
+      Builtin::kInterpreterPushArgsThenFastConstructFunction);
+  Address start = construct_stub->instruction_start();
+  const int pc_offset =
+      isolate_->heap()->construct_stub_invoke_deopt_pc_offset().value();
+  intptr_t pc_value = static_cast<intptr_t>(start + pc_offset);
+  if (is_topmost) {
+    // Only the pc of the topmost frame needs to be signed since it is
+    // authenticated at the end of the DeoptimizationEntry builtin.
+    output_frame->SetPc(PointerAuthentication::SignAndCheckPC(
+        isolate(), pc_value, frame_writer.frame()->GetTop()));
+  } else {
+    output_frame->SetPc(pc_value);
+  }
+
+  // Update constant pool.
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    intptr_t constant_pool_value =
+        static_cast<intptr_t>(construct_stub->constant_pool());
+    output_frame->SetConstantPool(constant_pool_value);
+    if (is_topmost) {
+      Register constant_pool_reg =
+          JavaScriptFrame::constant_pool_pointer_register();
+      output_frame->SetRegister(constant_pool_reg.code(), constant_pool_value);
+    }
+  }
+
+  // Clear the context register. The context might be a de-materialized object
+  // and will be materialized by {Runtime_NotifyDeoptimized}. For additional
+  // safety we use Tagged<Smi>(0) instead of the potential {arguments_marker}
+  // here.
+  if (is_topmost) {
+    intptr_t context_value = static_cast<intptr_t>(Smi::zero().ptr());
+    Register context_reg = JavaScriptFrame::context_register();
+    output_frame->SetRegister(context_reg.code(), context_value);
+
+    // Set the continuation for the topmost frame.
+    DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
+    Tagged<Code> continuation =
+        isolate_->builtins()->code(Builtin::kNotifyDumpFrame);
+    output_frame->SetContinuation(
+        static_cast<intptr_t>(continuation->instruction_start()));
+  }
+}
+
+namespace {
+
+bool BuiltinContinuationModeIsJavaScript(BuiltinContinuationMode mode) {
+  switch (mode) {
+    case BuiltinContinuationMode::STUB:
+      return false;
+    case BuiltinContinuationMode::JAVASCRIPT:
+    case BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH:
+    case BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION:
+      return true;
+  }
+  UNREACHABLE();
+}
+
+StackFrame::Type BuiltinContinuationModeToFrameType(
+    BuiltinContinuationMode mode) {
+  switch (mode) {
+    case BuiltinContinuationMode::STUB:
+      return StackFrame::BUILTIN_CONTINUATION;
+    case BuiltinContinuationMode::JAVASCRIPT:
+      return StackFrame::JAVA_SCRIPT_BUILTIN_CONTINUATION;
+    case BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH:
+      return StackFrame::JAVA_SCRIPT_BUILTIN_CONTINUATION_WITH_CATCH;
+    case BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION:
+      return StackFrame::JAVA_SCRIPT_BUILTIN_CONTINUATION_WITH_CATCH;
+  }
+  UNREACHABLE();
+}
+
+}  // namespace
+
+Builtin DeoptFrameDumper::TrampolineForBuiltinContinuation(
+    BuiltinContinuationMode mode, bool must_handle_result) {
+  switch (mode) {
+    case BuiltinContinuationMode::STUB:
+      return must_handle_result ? Builtin::kContinueToCodeStubBuiltinWithResult
+                                : Builtin::kContinueToCodeStubBuiltin;
+    case BuiltinContinuationMode::JAVASCRIPT:
+    case BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH:
+    case BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION:
+      return must_handle_result
+                 ? Builtin::kContinueToJavaScriptBuiltinWithResult
+                 : Builtin::kContinueToJavaScriptBuiltin;
+  }
+  UNREACHABLE();
+}
+
+// BuiltinContinuationFrames capture the machine state that is expected as input
+// to a builtin, including both input register values and stack parameters. When
+// the frame is reactivated (i.e. the frame below it returns), a
+// ContinueToBuiltin stub restores the register state from the frame and tail
+// calls to the actual target builtin, making it appear that the stub had been
+// directly called by the frame above it. The input values to populate the frame
+// are taken from the deopt's FrameState.
+//
+// Frame translation happens in two modes, EAGER and LAZY. In EAGER mode, all of
+// the parameters to the Builtin are explicitly specified in the TurboFan
+// FrameState node. In LAZY mode, there is always one fewer parameters specified
+// in the FrameState than expected by the Builtin. In that case, construction of
+// BuiltinContinuationFrame adds the final missing parameter during
+// deoptimization, and that parameter is always on the stack and contains the
+// value returned from the callee of the call site triggering the LAZY deopt
+// (e.g. rax on x64). This requires that continuation Builtins for LAZY deopts
+// must have at least one stack parameter.
+//
+//                TO
+//    |          ....           |
+//    +-------------------------+
+//    | arg padding (arch dept) |<- at most 1*kSystemPointerSize
+//    +-------------------------+
+//    |     builtin param 0     |<- FrameState input value n becomes
+//    +-------------------------+
+//    |           ...           |
+//    +-------------------------+
+//    |     builtin param m     |<- FrameState input value n+m-1, or in
+//    +-----needs-alignment-----+   the LAZY case, return LAZY result value
+//    | ContinueToBuiltin entry |
+//    +-------------------------+
+// |  |    saved frame (FP)     |
+// |  +=====needs=alignment=====+<- fpreg
+// |  |constant pool (if ool_cp)|
+// v  +-------------------------+
+//    |BUILTIN_CONTINUATION mark|
+//    +-------------------------+
+//    |  JSFunction (or zero)   |<- only if JavaScript builtin
+//    +-------------------------+
+//    |  frame height above FP  |
+//    +-------------------------+
+//    |         context         |<- this non-standard context slot contains
+//    +-------------------------+   the context, even for non-JS builtins.
+//    |      builtin index      |
+//    +-------------------------+
+//    | builtin input GPR reg0  |<- populated from deopt FrameState using
+//    +-------------------------+   the builtin's CallInterfaceDescriptor
+//    |          ...            |   to map a FrameState's 0..n-1 inputs to
+//    +-------------------------+   the builtin's n input register params.
+//    | builtin input GPR regn  |
+//    +-------------------------+
+//    | reg padding (arch dept) |
+//    +-----needs--alignment----+
+//    | res padding (arch dept) |<- only if {is_topmost}; result is pop'd by
+//    +-------------------------+<- kNotifyDeopt ASM stub and moved to acc
+//    |      result  value      |<- reg, as ContinueToBuiltin stub expects.
+//    +-----needs-alignment-----+<- spreg
+//
+void DeoptFrameDumper::DoComputeBuiltinContinuation(
+    TranslatedFrame* translated_frame, int frame_index,
+    BuiltinContinuationMode mode) {
+  TranslatedFrame::iterator result_iterator = translated_frame->end();
+
+  bool is_js_to_wasm_builtin_continuation = false;
+
+  TranslatedFrame::iterator value_iterator = translated_frame->begin();
+
+  const BytecodeOffset bytecode_offset = translated_frame->bytecode_offset();
+  Builtin builtin = Builtins::GetBuiltinFromBytecodeOffset(bytecode_offset);
+  CallInterfaceDescriptor continuation_descriptor =
+      Builtins::CallInterfaceDescriptorFor(builtin);
+
+  const RegisterConfiguration* config = RegisterConfiguration::Default();
+
+  const bool is_bottommost = (0 == frame_index);
+  const bool is_topmost = (output_count_ - 1 == frame_index);
+
+  const int parameters_count = translated_frame->height();
+  BuiltinContinuationFrameInfo frame_info =
+      BuiltinContinuationFrameInfo::Precise(parameters_count,
+                                            continuation_descriptor, config,
+                                            is_topmost, deopt_kind_, mode);
+
+  const unsigned output_frame_size = frame_info.frame_size_in_bytes();
+  const unsigned output_frame_size_above_fp =
+      frame_info.frame_size_in_bytes_above_fp();
+
+  // Validate types of parameters. They must all be tagged except for argc for
+  // JS builtins.
+  bool has_argc = false;
+  const int register_parameter_count =
+      continuation_descriptor.GetRegisterParameterCount();
+  for (int i = 0; i < register_parameter_count; ++i) {
+    MachineType type = continuation_descriptor.GetParameterType(i);
+    int code = continuation_descriptor.GetRegisterParameter(i).code();
+    // Only tagged and int32 arguments are supported, and int32 only for the
+    // arguments count on JavaScript builtins.
+    if (type == MachineType::Int32()) {
+      CHECK_EQ(code, kJavaScriptCallArgCountRegister.code());
+      has_argc = true;
+    } else {
+      // Any other argument must be a tagged value.
+      CHECK(IsAnyTagged(type.representation()));
+    }
+  }
+  CHECK_EQ(BuiltinContinuationModeIsJavaScript(mode), has_argc);
+
+  if (verbose_tracing_enabled()) {
+    PrintF(trace_scope()->file(),
+           "  translating BuiltinContinuation to %s,"
+           " => register_param_count=%d,"
+           " stack_param_count=%d, frame_size=%d\n",
+           Builtins::name(builtin), register_parameter_count,
+           frame_info.stack_parameter_count(), output_frame_size);
+  }
+
+  FrameDescription* output_frame = new (output_frame_size) FrameDescription(
+      output_frame_size, frame_info.stack_parameter_count(), isolate());
+  output_[frame_index] = output_frame;
+  FrameWriter frame_writer(this, output_frame, verbose_trace_scope());
+
+  // The top address of the frame is computed from the previous frame's top and
+  // this frame's size.
+  const intptr_t top_address =
+      is_bottommost ? caller_frame_top_ - output_frame_size
+                    : output_[frame_index - 1]->GetTop() - output_frame_size;
+  output_frame->SetTop(top_address);
+
+  // Get the possible JSFunction for the case that this is a
+  // JavaScriptBuiltinContinuationFrame, which needs the JSFunction pointer
+  // like a normal JavaScriptFrame.
+  const intptr_t maybe_function = value_iterator->GetRawValue().ptr();
+  ++value_iterator;
+
+  ReadOnlyRoots roots(isolate());
+  const int padding = ArgumentPaddingSlots(frame_info.stack_parameter_count());
+  for (int i = 0; i < padding; ++i) {
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+  if (mode == BuiltinContinuationMode::STUB) {
+    DCHECK_EQ(
+        Builtins::CallInterfaceDescriptorFor(builtin).GetStackArgumentOrder(),
+        StackArgumentOrder::kDefault);
+    for (uint32_t i = 0; i < frame_info.translated_stack_parameter_count();
+         ++i, ++value_iterator) {
+      frame_writer.PushTranslatedValue(value_iterator, "stack parameter");
+    }
+    if (frame_info.frame_has_result_stack_slot()) {
+      if (is_js_to_wasm_builtin_continuation) {
+        frame_writer.PushTranslatedValue(result_iterator,
+                                         "return result on lazy deopt\n");
+      } else {
+        DCHECK_EQ(result_iterator, translated_frame->end());
+        frame_writer.PushRawObject(
+            roots.the_hole_value(),
+            "placeholder for return result on lazy deopt\n");
+      }
+    }
+  } else {
+    // JavaScript builtin.
+    if (frame_info.frame_has_result_stack_slot()) {
+      frame_writer.PushRawObject(
+          roots.the_hole_value(),
+          "placeholder for return result on lazy deopt\n");
+    }
+    switch (mode) {
+      case BuiltinContinuationMode::STUB:
+        UNREACHABLE();
+      case BuiltinContinuationMode::JAVASCRIPT:
+        break;
+      case BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH: {
+        frame_writer.PushRawObject(roots.the_hole_value(),
+                                   "placeholder for exception on lazy deopt\n");
+      } break;
+      case BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION: {
+        intptr_t accumulator_value =
+            input_->GetRegister(kInterpreterAccumulatorRegister.code());
+        frame_writer.PushRawObject(Tagged<Object>(accumulator_value),
+                                   "exception (from accumulator)\n");
+      } break;
+    }
+    frame_writer.PushStackJSArguments(
+        value_iterator, frame_info.translated_stack_parameter_count());
+  }
+
+  DCHECK_EQ(output_frame->GetLastArgumentSlotOffset(),
+            frame_writer.top_offset());
+
+  std::vector<TranslatedFrame::iterator> register_values;
+  int total_registers = config->num_general_registers();
+  register_values.resize(total_registers, {value_iterator});
+
+  for (int i = 0; i < register_parameter_count; ++i, ++value_iterator) {
+    int code = continuation_descriptor.GetRegisterParameter(i).code();
+    register_values[code] = value_iterator;
+  }
+
+  // The context register is always implicit in the CallInterfaceDescriptor but
+  // its register must be explicitly set when continuing to the builtin. Make
+  // sure that it's harvested from the translation and copied into the register
+  // set (it was automatically added at the end of the FrameState by the
+  // instruction selector).
+  Tagged<Object> context = value_iterator->GetRawValue();
+  const intptr_t value = context.ptr();
+  TranslatedFrame::iterator context_register_value = value_iterator++;
+  register_values[kContextRegister.code()] = context_register_value;
+  output_frame->SetContext(value);
+  output_frame->SetRegister(kContextRegister.code(), value);
+
+  // Set caller's PC (JSFunction continuation).
+  if (is_bottommost) {
+    frame_writer.PushBottommostCallerPc(caller_pc_);
+  } else {
+    frame_writer.PushApprovedCallerPc(output_[frame_index - 1]->GetPc());
+  }
+
+  // Read caller's FP from the previous frame, and set this frame's FP.
+  const intptr_t caller_fp =
+      is_bottommost ? caller_fp_ : output_[frame_index - 1]->GetFp();
+  frame_writer.PushCallerFp(caller_fp);
+
+  const intptr_t fp_value = top_address + frame_writer.top_offset();
+  output_frame->SetFp(fp_value);
+
+  DCHECK_EQ(output_frame_size_above_fp, frame_writer.top_offset());
+
+  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
+    // Read the caller's constant pool from the previous frame.
+    const intptr_t caller_cp =
+        is_bottommost ? caller_constant_pool_
+                      : output_[frame_index - 1]->GetConstantPool();
+    frame_writer.PushCallerConstantPool(caller_cp);
+  }
+
+  // A marker value is used in place of the context.
+  const intptr_t marker =
+      StackFrame::TypeToMarker(BuiltinContinuationModeToFrameType(mode));
+  frame_writer.PushRawValue(marker,
+                            "context (builtin continuation sentinel)\n");
+
+  if (BuiltinContinuationModeIsJavaScript(mode)) {
+    frame_writer.PushRawValue(maybe_function, "JSFunction\n");
+  } else {
+    frame_writer.PushRawValue(0, "unused\n");
+  }
+
+  // The delta from the SP to the FP; used to reconstruct SP in
+  // Isolate::UnwindAndFindHandler.
+  frame_writer.PushRawObject(Smi::FromInt(output_frame_size_above_fp),
+                             "frame height at deoptimization\n");
+
+  // The context even if this is a stub continuation frame. We can't use the
+  // usual context slot, because we must store the frame marker there.
+  frame_writer.PushTranslatedValue(context_register_value,
+                                   "builtin JavaScript context\n");
+
+  // The builtin to continue to.
+  frame_writer.PushRawObject(Smi::FromInt(static_cast<int>(builtin)),
+                             "builtin index\n");
+
+  const int allocatable_register_count =
+      config->num_allocatable_general_registers();
+  for (int i = 0; i < allocatable_register_count; ++i) {
+    int code = config->GetAllocatableGeneralCode(i);
+    base::ScopedVector<char> str(128);
+    if (verbose_tracing_enabled()) {
+      if (BuiltinContinuationModeIsJavaScript(mode) &&
+          code == kJavaScriptCallArgCountRegister.code()) {
+        SNPrintF(
+            str,
+            "tagged argument count %s (will be untagged by continuation)\n",
+            RegisterName(Register::from_code(code)));
+      } else {
+        SNPrintF(str, "builtin register argument %s\n",
+                 RegisterName(Register::from_code(code)));
+      }
+    }
+    frame_writer.PushTranslatedValue(
+        register_values[code], verbose_tracing_enabled() ? str.begin() : "");
+  }
+
+  // Some architectures must pad the stack frame with extra stack slots
+  // to ensure the stack frame is aligned.
+  const int padding_slot_count =
+      BuiltinContinuationFrameConstants::PaddingSlotCount(
+          allocatable_register_count);
+  for (int i = 0; i < padding_slot_count; ++i) {
+    frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+  }
+
+  if (is_topmost) {
+    for (int i = 0; i < ArgumentPaddingSlots(1); ++i) {
+      frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
+    }
+
+    // Ensure the result is restored back when we return to the stub.
+    if (frame_info.frame_has_result_stack_slot()) {
+      Register result_reg = kReturnRegister0;
+      frame_writer.PushRawValue(input_->GetRegister(result_reg.code()),
+                                "callback result\n");
+    } else {
+      frame_writer.PushRawObject(roots.undefined_value(), "callback result\n");
+    }
+  }
+
+  CHECK_EQ(result_iterator, value_iterator);
+  CHECK_EQ(0u, frame_writer.top_offset());
+
+  // Clear the context register. The context might be a de-materialized object
+  // and will be materialized by {Runtime_NotifyDeoptimized}. For additional
+  // safety we use Tagged<Smi>(0) instead of the potential {arguments_marker}
+  // here.
+  if (is_topmost) {
+    intptr_t context_value = static_cast<intptr_t>(Smi::zero().ptr());
+    Register context_reg = JavaScriptFrame::context_register();
+    output_frame->SetRegister(context_reg.code(), context_value);
+  }
+
+  // Ensure the frame pointer register points to the callee's frame. The builtin
+  // will build its own frame once we continue to it.
+  Register fp_reg = JavaScriptFrame::fp_register();
+  output_frame->SetRegister(fp_reg.code(), fp_value);
+  // For JSToWasmBuiltinContinuations use ContinueToCodeStubBuiltin, and not
+  // ContinueToCodeStubBuiltinWithResult because we don't want to overwrite the
+  // return value that we have already set.
+  Tagged<Code> continue_to_builtin =
+      isolate()->builtins()->code(TrampolineForBuiltinContinuation(
+          mode, frame_info.frame_has_result_stack_slot() &&
+                    !is_js_to_wasm_builtin_continuation));
+  if (is_topmost) {
+    // Only the pc of the topmost frame needs to be signed since it is
+    // authenticated at the end of the DeoptimizationEntry builtin.
+    const intptr_t top_most_pc = PointerAuthentication::SignAndCheckPC(
+        isolate(),
+        static_cast<intptr_t>(continue_to_builtin->instruction_start()),
+        frame_writer.frame()->GetTop());
+    output_frame->SetPc(top_most_pc);
+  } else {
+    output_frame->SetPc(
+        static_cast<intptr_t>(continue_to_builtin->instruction_start()));
+  }
+
+  Tagged<Code> continuation =
+      isolate()->builtins()->code(Builtin::kNotifyDumpFrame);
+  output_frame->SetContinuation(
+      static_cast<intptr_t>(continuation->instruction_start()));
+}
+
+void DeoptFrameDumper::MaterializeHeapObjects() {
+  translated_state_.Prepare(static_cast<Address>(stack_fp_));
+  if (v8_flags.deopt_every_n_times > 0) {
+    // Doing a GC here will find problems with the deoptimized frames.
+    isolate_->heap()->CollectAllGarbage(GCFlag::kNoFlags,
+                                        GarbageCollectionReason::kTesting);
+  }
+
+  if (v8_flags.verify_heap_on_jit_dump) {
+    HeapVerifier::VerifyHeapIfEnabled(isolate_->heap());
+  }
+
+  for (auto& materialization : values_to_materialize_) {
+    Handle<Object> value = materialization.value_->GetValue();
+
+    if (verbose_tracing_enabled()) {
+      PrintF(trace_scope()->file(),
+             "Materialization [" V8PRIxPTR_FMT "] <- " V8PRIxPTR_FMT " ;  ",
+             static_cast<intptr_t>(materialization.output_slot_address_),
+             (*value).ptr());
+      ShortPrint(*value, trace_scope()->file());
+      PrintF(trace_scope()->file(), "\n");
+    }
+
+    *(reinterpret_cast<Address*>(materialization.output_slot_address_)) =
+        (*value).ptr();
+  }
+
+  for (auto& fbv_materialization : feedback_vector_to_materialize_) {
+    Handle<Object> closure = fbv_materialization.value_->GetValue();
+    DCHECK(IsJSFunction(*closure));
+    Tagged<Object> feedback_vector =
+        Tagged<JSFunction>::cast(*closure)->raw_feedback_cell()->value();
+    CHECK(IsFeedbackVector(feedback_vector));
+    *(reinterpret_cast<Address*>(fbv_materialization.output_slot_address_)) =
+        feedback_vector.ptr();
+  }
+
+  translated_state_.VerifyMaterializedObjects();
+
+  /*
+  bool feedback_updated = translated_state_.DoUpdateFeedback();
+  if (verbose_tracing_enabled() && feedback_updated) {
+    FILE* file = trace_scope()->file();
+    DeoptFrameDumper::DeoptInfo info = DeoptFrameDumper::GetDeoptInfo();
+    PrintF(file, "Feedback updated from deoptimization at ");
+    OFStream outstr(file);
+    info.position.Print(outstr, compiled_code_);
+    PrintF(file, ", %s\n", DeoptimizeReasonToString(info.deopt_reason));
+  }
+  */
+
+  isolate_->materialized_object_store()->Remove(
+      static_cast<Address>(stack_fp_));
+}
+
+void DeoptFrameDumper::QueueValueForMaterialization(
+    Address output_address, Tagged<Object> obj,
+    const TranslatedFrame::iterator& iterator) {
+  if (obj == ReadOnlyRoots(isolate_).arguments_marker()) {
+    values_to_materialize_.push_back({output_address, iterator});
+  }
+}
+
+void DeoptFrameDumper::QueueFeedbackVectorForMaterialization(
+    Address output_address, const TranslatedFrame::iterator& iterator) {
+  feedback_vector_to_materialize_.push_back({output_address, iterator});
+}
+
+unsigned DeoptFrameDumper::ComputeInputFrameAboveFpFixedSize() const {
+  unsigned fixed_size = CommonFrameConstants::kFixedFrameSizeAboveFp;
+  // TODO(jkummerow): If {IsSmi(function_)} can indeed be true, then
+  // {function_} should not have type {JSFunction}.
+  if (!IsSmi(function_)) {
+    fixed_size += ComputeIncomingArgumentSize(function_->shared());
+  }
+  return fixed_size;
+}
+
+namespace {
+
+// Get the actual deopt call PC from the return address of the deopt, which
+// points to immediately after the deopt call).
+//
+// See also the DeoptFrameDumper constructor.
+Address GetDeoptCallPCFromReturnPC(Address return_pc, Tagged<Code> code) {
+  DCHECK_GT(DeoptFrameDumper::kEagerDeoptExitSize, 0);
+  // DCHECK_GT(DeoptFrameDumper::kLazyDeoptExitSize, 0);
+  Tagged<DeoptimizationData> deopt_data =
+      DeoptimizationData::cast(code->deoptimization_data());
+  Address deopt_start =
+      code->instruction_start() + deopt_data->DeoptExitStart().value();
+  int eager_deopt_count = deopt_data->EagerDeoptCount().value();
+  Address lazy_deopt_start =
+      deopt_start + eager_deopt_count * DeoptFrameDumper::kEagerDeoptExitSize;
+  // The deoptimization exits are sorted so that lazy deopt exits appear
+  // after eager deopts.
+  static_assert(static_cast<int>(DeoptimizeKind::kLazy) ==
+                    static_cast<int>(kLastDeoptimizeKind),
+                "lazy deopts are expected to be emitted last");
+  if (return_pc <= lazy_deopt_start) {
+    return return_pc - DeoptFrameDumper::kEagerDeoptExitSize;
+  } else {
+    UNREACHABLE();
+  }
+}
+
+}  // namespace
+
+unsigned DeoptFrameDumper::ComputeInputFrameSize() const {
+  // The fp-to-sp delta already takes the context, constant pool pointer and the
+  // function into account so we have to avoid double counting them.
+  unsigned fixed_size_above_fp = ComputeInputFrameAboveFpFixedSize();
+  unsigned result = fixed_size_above_fp + fp_to_sp_delta_;
+  DCHECK(CodeKindCanDeoptimize(compiled_code_->kind()));
+  unsigned stack_slots = compiled_code_->stack_slots();
+  if (compiled_code_->is_maglevved() && !deoptimizing_throw_) {
+    // Maglev code can deopt in deferred code which has spilled registers across
+    // the call. These will be included in the fp_to_sp_delta, but the expected
+    // frame size won't include them, so we need to check for less-equal rather
+    // than equal. For deoptimizing throws, these will have already been trimmed
+    // off.
+    CHECK_LE(fixed_size_above_fp + (stack_slots * kSystemPointerSize) -
+                 CommonFrameConstants::kFixedFrameSizeAboveFp,
+             result);
+    // With slow asserts we can check this exactly, by looking up the safepoint.
+    if (v8_flags.enable_slow_asserts) {
+      Address deopt_call_pc = GetDeoptCallPCFromReturnPC(from_, compiled_code_);
+      MaglevSafepointTable table(isolate_, deopt_call_pc, compiled_code_);
+      MaglevSafepointEntry safepoint = table.FindEntry(deopt_call_pc);
+      unsigned extra_spills = safepoint.num_extra_spill_slots();
+      CHECK_EQ(fixed_size_above_fp + (stack_slots * kSystemPointerSize) -
+                   CommonFrameConstants::kFixedFrameSizeAboveFp +
+                   extra_spills * kSystemPointerSize,
+               result);
+    }
+  } else {
+    unsigned outgoing_size = 0;
+    CHECK_EQ(fixed_size_above_fp + (stack_slots * kSystemPointerSize) -
+                 CommonFrameConstants::kFixedFrameSizeAboveFp + outgoing_size,
+             result);
+  }
+  return result;
+}
+
+// static
+unsigned DeoptFrameDumper::ComputeIncomingArgumentSize(
+    Tagged<SharedFunctionInfo> shared) {
+  int parameter_slots = shared->internal_formal_parameter_count_with_receiver();
+  return parameter_slots * kSystemPointerSize;
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/deoptimizer/deopt-frame-dumper.h b/src/deoptimizer/deopt-frame-dumper.h
new file mode 100644
index 00000000000..5b8224882a7
--- /dev/null
+++ b/src/deoptimizer/deopt-frame-dumper.h
@@ -0,0 +1,203 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_DEOPTIMIZER_FRAME_DUMPER_H_
+#define V8_DEOPTIMIZER_FRAME_DUMPER_H_
+
+#include <vector>
+
+#include "src/builtins/builtins.h"
+#include "src/codegen/source-position.h"
+#include "src/deoptimizer/deoptimize-reason.h"
+#include "src/deoptimizer/frame-description.h"
+#include "src/deoptimizer/translated-state.h"
+#include "src/diagnostics/code-tracer.h"
+#include "src/objects/js-function.h"
+
+
+namespace v8 {
+namespace internal {
+
+enum class BuiltinContinuationMode;
+
+class DeoptimizedFrameInfo;
+class Isolate;
+
+class DeoptFrameDumper : public Malloced {
+ public:
+  struct DeoptInfo {
+    DeoptInfo(SourcePosition position, DeoptimizeReason deopt_reason,
+              uint32_t node_id, int deopt_id)
+        : position(position),
+          deopt_reason(deopt_reason),
+          node_id(node_id),
+          deopt_id(deopt_id) {}
+
+    const SourcePosition position;
+    const DeoptimizeReason deopt_reason;
+    const uint32_t node_id;
+    const int deopt_id;
+  };
+
+  // Whether the deopt exit is contained by the outermost loop containing the
+  // osr'd loop. For example:
+  //
+  //  for (;;) {
+  //    for (;;) {
+  //    }  // OSR is triggered on this backedge.
+  //  }  // This is the outermost loop containing the osr'd loop.
+  static bool DeoptExitIsInsideOsrLoop(Isolate* isolate,
+                                       Tagged<JSFunction> function,
+                                       BytecodeOffset deopt_exit_offset,
+                                       BytecodeOffset osr_offset);
+
+  Handle<JSFunction> function() const;
+  Handle<Code> compiled_code() const;
+
+  // Where the deopt exit occurred *in the outermost frame*, i.e in the
+  // function we generated OSR'd code for. If the deopt occurred in an inlined
+  // function, this would point at the corresponding outermost Call bytecode.
+  BytecodeOffset bytecode_offset_in_outermost_frame() const {
+    return bytecode_offset_in_outermost_frame_;
+  }
+
+  BytecodeOffset bytecode_offset_in_innermost_frame() const {
+    return bytecode_offset_in_innermost_frame_;
+  }
+
+  unsigned int deopt_exit_index() const {
+    return deopt_exit_index_;
+  }
+
+  static DeoptFrameDumper* New(Address raw_function, DeoptimizeKind kind,
+                          Address from, int fp_to_sp_delta, Isolate* isolate);
+  static DeoptFrameDumper* Grab(Isolate* isolate);
+
+  ~DeoptFrameDumper();
+
+  void MaterializeHeapObjects();
+  void ReenableGC();
+
+  static void ComputeOutputFrames(DeoptFrameDumper* deoptimizer);
+
+
+  Isolate* isolate() const { return isolate_; }
+
+  static constexpr int kMaxNumberOfEntries = 16384;
+
+  // This marker is passed to DeoptFrameDumper::New as {deopt_exit_index} on
+  // platforms that have fixed deopt sizes. The actual deoptimization id is then
+  // calculated from the return address.
+  static constexpr unsigned kFixedExitSizeMarker = kMaxUInt32;
+
+  // Size of deoptimization exit sequence.
+  V8_EXPORT_PRIVATE static const int kEagerDeoptExitSize;
+  V8_EXPORT_PRIVATE static const int kLazyDeoptExitSize;
+
+
+ private:
+  void QueueValueForMaterialization(Address output_address, Tagged<Object> obj,
+                                    const TranslatedFrame::iterator& iterator);
+  void QueueFeedbackVectorForMaterialization(
+      Address output_address, const TranslatedFrame::iterator& iterator);
+
+  DeoptFrameDumper(Isolate* isolate, Tagged<JSFunction> function,
+              DeoptimizeKind kind, Address from, int fp_to_sp_delta);
+  void DeleteFrameDescriptions();
+
+  void DoComputeOutputFrames();
+  void DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
+                                 int frame_index, bool goto_catch_handler);
+  void DoComputeInlinedExtraArguments(TranslatedFrame* translated_frame,
+                                      int frame_index);
+  void DoComputeConstructCreateStubFrame(TranslatedFrame* translated_frame,
+                                         int frame_index);
+  void DoComputeConstructInvokeStubFrame(TranslatedFrame* translated_frame,
+                                         int frame_index);
+
+  static Builtin TrampolineForBuiltinContinuation(BuiltinContinuationMode mode,
+                                                  bool must_handle_result);
+
+  void DoComputeBuiltinContinuation(TranslatedFrame* translated_frame,
+                                    int frame_index,
+                                    BuiltinContinuationMode mode);
+
+  unsigned ComputeInputFrameAboveFpFixedSize() const;
+  unsigned ComputeInputFrameSize() const;
+
+  static unsigned ComputeIncomingArgumentSize(
+      Tagged<SharedFunctionInfo> shared);
+
+  // Tracing.
+  bool tracing_enabled() const { return trace_scope_ != nullptr; }
+  bool verbose_tracing_enabled() const {
+    return v8_flags.trace_deopt_verbose && tracing_enabled();
+  }
+  CodeTracer::Scope* trace_scope() const { return trace_scope_; }
+  CodeTracer::Scope* verbose_trace_scope() const {
+    return v8_flags.trace_deopt_verbose ? trace_scope() : nullptr;
+  }
+
+  bool is_restart_frame() const { return restart_frame_index_ >= 0; }
+
+  void set_from(Address from) {
+    from_ = from;
+  }
+
+  Isolate* isolate_;
+  Tagged<JSFunction> function_;
+  Tagged<Code> compiled_code_;
+  unsigned deopt_exit_index_;
+  BytecodeOffset bytecode_offset_in_outermost_frame_ = BytecodeOffset::None();
+  BytecodeOffset bytecode_offset_in_innermost_frame_ = BytecodeOffset::None();
+  DeoptimizeKind deopt_kind_;
+  Address from_;
+  int fp_to_sp_delta_;
+  bool deoptimizing_throw_;
+  int catch_handler_data_;
+  int catch_handler_pc_offset_;
+  int restart_frame_index_;
+
+  // Input frame description.
+  FrameDescription* input_;
+  // Number of output frames.
+  int output_count_;
+  // Array of output frame descriptions.
+  FrameDescription** output_;
+
+  // Caller frame details computed from input frame.
+  intptr_t caller_frame_top_;
+  intptr_t caller_fp_;
+  intptr_t caller_pc_;
+  intptr_t caller_constant_pool_;
+
+  // The argument count of the bottom most frame.
+  int actual_argument_count_;
+
+  // Key for lookup of previously materialized objects.
+  intptr_t stack_fp_;
+
+  TranslatedState translated_state_;
+  struct ValueToMaterialize {
+    Address output_slot_address_;
+    TranslatedFrame::iterator value_;
+  };
+  std::vector<ValueToMaterialize> values_to_materialize_;
+  std::vector<ValueToMaterialize> feedback_vector_to_materialize_;
+
+  DisallowGarbageCollection* disallow_garbage_collection_;
+
+  // Note: This is intentionally not a unique_ptr s.t. the DeoptFrameDumper
+  // satisfies is_standard_layout, needed for offsetof().
+  CodeTracer::Scope* const trace_scope_;
+
+  friend class DeoptimizedFrameInfo;
+  friend class FrameDescription;
+  friend class FrameWriter;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_DEOPTIMIZER_FRAME_DUMPER_H_
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index 09029d7ca1f..0b09dc0c7a1 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -825,6 +825,7 @@ void Deoptimizer::DoComputeOutputFrames() {
 
   // Do the input frame to output frame(s) translation.
   size_t count = translated_state_.frames().size();
+
   if (is_restart_frame()) {
     // If the debugger requested to restart a particular frame, only materialize
     // up to that frame.
@@ -849,12 +850,21 @@ void Deoptimizer::DoComputeOutputFrames() {
   output_ = new FrameDescription* [count] {};
   output_count_ = static_cast<int>(count);
 
+  bytecode_offset_in_innermost_frame_ =
+    translated_state_.frames()[output_count_ - 1].bytecode_offset();
+
+
   // Translate each output frame.
   int frame_index = 0;
   size_t total_output_frame_size = 0;
   for (size_t i = 0; i < count; ++i, ++frame_index) {
     TranslatedFrame* translated_frame = &(translated_state_.frames()[i]);
     const bool handle_exception = deoptimizing_throw_ && i == count - 1;
+
+    if (i == count - 1) {
+      top_frame_is_unoptimized_ = (translated_frame->kind() == TranslatedFrame::kUnoptimizedFunction);
+    }
+
     switch (translated_frame->kind()) {
       case TranslatedFrame::kUnoptimizedFunction:
         DoComputeUnoptimizedFrame(translated_frame, frame_index,
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index c938a916e71..c3d4afb50e4 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -72,6 +72,16 @@ class Deoptimizer : public Malloced {
     return bytecode_offset_in_outermost_frame_;
   }
 
+  BytecodeOffset bytecode_offset_in_innermost_frame() const {
+    return bytecode_offset_in_innermost_frame_;
+  }
+
+  bool TopFrameIsUnoptimized() const { return top_frame_is_unoptimized_; }
+
+  unsigned int deopt_exit_index() const {
+    return deopt_exit_index_;
+  }
+
   static Deoptimizer* New(Address raw_function, DeoptimizeKind kind,
                           Address from, int fp_to_sp_delta, Isolate* isolate);
   static Deoptimizer* Grab(Isolate* isolate);
@@ -205,11 +215,14 @@ class Deoptimizer : public Malloced {
 
   bool is_restart_frame() const { return restart_frame_index_ >= 0; }
 
+
   Isolate* isolate_;
   Tagged<JSFunction> function_;
   Tagged<Code> compiled_code_;
   unsigned deopt_exit_index_;
   BytecodeOffset bytecode_offset_in_outermost_frame_ = BytecodeOffset::None();
+  BytecodeOffset bytecode_offset_in_innermost_frame_ = BytecodeOffset::None();
+  bool top_frame_is_unoptimized_ = false;
   DeoptimizeKind deopt_kind_;
   Address from_;
   int fp_to_sp_delta_;
diff --git a/src/deoptimizer/x64/deoptimizer-x64.cc b/src/deoptimizer/x64/deoptimizer-x64.cc
index e91e062a9df..5f4e0674219 100644
--- a/src/deoptimizer/x64/deoptimizer-x64.cc
+++ b/src/deoptimizer/x64/deoptimizer-x64.cc
@@ -5,6 +5,8 @@
 #if V8_TARGET_ARCH_X64
 
 #include "src/deoptimizer/deoptimizer.h"
+#include "src/deoptimizer/deopt-frame-dumper.h"
+
 #include "src/execution/isolate-data.h"
 
 namespace v8 {
@@ -28,6 +30,9 @@ const int Deoptimizer::kLazyDeoptExitSize = 8;
 const int Deoptimizer::kLazyDeoptExitSize = 4;
 #endif
 
+const int DeoptFrameDumper::kEagerDeoptExitSize = 4;
+// const int DeoptFrameDumper::kLazyDeoptExitSize = 4;
+
 Float32 RegisterValues::GetFloatRegister(unsigned n) const {
   return Float32::FromBits(
       static_cast<uint32_t>(double_registers_[n].get_bits()));
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index 76ee1466041..e2dbfe40b1a 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -5,6 +5,7 @@
 #include <iomanip>
 #include <memory>
 
+#include "src/base/logging.h"
 #include "src/common/globals.h"
 #include "src/diagnostics/disasm.h"
 #include "src/diagnostics/disassembler.h"
@@ -3016,7 +3017,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
   if (IsString(*this, cage_base)) {
     HeapStringAllocator allocator;
     StringStream accumulator(&allocator);
-    String::cast(*this)->StringShortPrint(&accumulator);
+    String::cast(*this)->StringShortPrint(&accumulator, false);
     os << accumulator.ToCString().get();
     return;
   }
@@ -3366,6 +3367,340 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
   }
 }
 
+void HeapObject::HeapObjectDifferentialFuzzingPrint(std::ostream& os, int depth) {
+  PtrComprCageBase cage_base = GetPtrComprCageBase();
+
+  if (IsString(*this, cage_base)) {
+    HeapStringAllocator allocator;
+    StringStream string_stream(&allocator);
+    String::cast(*this)->StringShortPrint(&string_stream, false);
+    os << string_stream.ToCString();
+    return;
+  }
+  if (IsJSObject(*this, cage_base)) {
+    HeapStringAllocator allocator;
+    StringStream accumulator(&allocator);
+    JSObject::cast(*this)->JSObjectDifferentialFuzzingPrint(&accumulator, depth);
+    os << accumulator.ToCString().get();
+    return;
+  }
+  switch (map(cage_base)->instance_type()) {
+    case MAP_TYPE: {
+      os << "<Map";
+      Tagged<Map> mapInstance = Map::cast(*this);
+      os << "(";
+      os << mapInstance->instance_type();
+      os << ")>";
+    } break;
+    case AWAIT_CONTEXT_TYPE: {
+      os << "<AwaitContext generator= ";
+      HeapStringAllocator allocator;
+      StringStream accumulator(&allocator);
+      DifferentialFuzzingPrint(Context::cast(*this)->extension(), &accumulator);
+      os << accumulator.ToCString().get();
+      os << '>';
+      break;
+    }
+    case BLOCK_CONTEXT_TYPE:
+      os << "<BlockContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case CATCH_CONTEXT_TYPE:
+      os << "<CatchContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case DEBUG_EVALUATE_CONTEXT_TYPE:
+      os << "<DebugEvaluateContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case EVAL_CONTEXT_TYPE:
+      os << "<EvalContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case FUNCTION_CONTEXT_TYPE:
+      os << "<FunctionContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case MODULE_CONTEXT_TYPE:
+      os << "<ModuleContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case NATIVE_CONTEXT_TYPE:
+      os << "<NativeContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case SCRIPT_CONTEXT_TYPE:
+      os << "<ScriptContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case WITH_CONTEXT_TYPE:
+      os << "<WithContext[" << Context::cast(*this)->length() << "]>";
+      break;
+    case SCRIPT_CONTEXT_TABLE_TYPE:
+      os << "<ScriptContextTable["
+         << ScriptContextTable::cast(*this)->capacity() << "]>";
+      break;
+    case HASH_TABLE_TYPE:
+      os << "<HashTable[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case ORDERED_HASH_MAP_TYPE:
+      os << "<OrderedHashMap[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case ORDERED_HASH_SET_TYPE:
+      os << "<OrderedHashSet[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case ORDERED_NAME_DICTIONARY_TYPE:
+      os << "<OrderedNameDictionary[" << FixedArray::cast(*this)->length()
+         << "]>";
+      break;
+    case NAME_DICTIONARY_TYPE:
+      os << "<NameDictionary[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case SWISS_NAME_DICTIONARY_TYPE:
+      os << "<SwissNameDictionary["
+         << SwissNameDictionary::cast(*this)->Capacity() << "]>";
+      break;
+    case GLOBAL_DICTIONARY_TYPE:
+      os << "<GlobalDictionary[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case NUMBER_DICTIONARY_TYPE:
+      os << "<NumberDictionary[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case SIMPLE_NUMBER_DICTIONARY_TYPE:
+      os << "<SimpleNumberDictionary[" << FixedArray::cast(*this)->length()
+         << "]>";
+      break;
+    case FIXED_ARRAY_TYPE:
+      os << "<FixedArray[" << FixedArray::cast(*this)->length() << "]>";
+      break;
+    case OBJECT_BOILERPLATE_DESCRIPTION_TYPE:
+      os << "<ObjectBoilerplateDescription["
+         << ObjectBoilerplateDescription::cast(*this)->capacity() << "]>";
+      break;
+    case FIXED_DOUBLE_ARRAY_TYPE:
+      os << "<FixedDoubleArray[" << FixedDoubleArray::cast(*this)->length()
+         << "]>";
+      break;
+    case BYTE_ARRAY_TYPE:
+      os << "<ByteArray[" << ByteArray::cast(*this)->length() << "]>";
+      break;
+    case BYTECODE_ARRAY_TYPE:
+      os << "<BytecodeArray[" << BytecodeArray::cast(*this)->length() << "]>";
+      break;
+    case EXTERNAL_POINTER_ARRAY_TYPE:
+      os << "<ExternalPointerArray["
+         << ExternalPointerArray::cast(*this)->length() << "]>";
+      break;
+    case DESCRIPTOR_ARRAY_TYPE:
+      os << "<DescriptorArray["
+         << DescriptorArray::cast(*this)->number_of_descriptors() << "]>";
+      break;
+    case TRANSITION_ARRAY_TYPE:
+      os << "<TransitionArray[" << TransitionArray::cast(*this)->length()
+         << "]>";
+      break;
+    case PROPERTY_ARRAY_TYPE:
+      os << "<PropertyArray[" << PropertyArray::cast(*this)->length() << "]>";
+      break;
+    case FEEDBACK_CELL_TYPE: {
+      {
+        ReadOnlyRoots roots = GetReadOnlyRoots();
+        os << "<FeedbackCell[";
+        if (map() == roots.no_closures_cell_map()) {
+          os << "no feedback";
+        } else if (map() == roots.one_closure_cell_map()) {
+          os << "one closure";
+        } else if (map() == roots.many_closures_cell_map()) {
+          os << "many closures";
+        } else {
+          os << "!!!INVALID MAP!!!";
+        }
+        os << "]>";
+      }
+      break;
+    }
+    case CLOSURE_FEEDBACK_CELL_ARRAY_TYPE:
+      os << "<ClosureFeedbackCellArray["
+         << ClosureFeedbackCellArray::cast(*this)->length() << "]>";
+      break;
+    case FEEDBACK_VECTOR_TYPE:
+      os << "<FeedbackVector[" << FeedbackVector::cast(*this)->length() << "]>";
+      break;
+    case FREE_SPACE_TYPE:
+      os << "<FreeSpace[" << FreeSpace::cast(*this)->size(kRelaxedLoad) << "]>";
+      break;
+
+    case PREPARSE_DATA_TYPE: {
+      Tagged<PreparseData> data = PreparseData::cast(*this);
+      os << "<PreparseData[data=" << data->data_length()
+         << " children=" << data->children_length() << "]>";
+      break;
+    }
+
+    case UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_TYPE: {
+      Tagged<UncompiledDataWithoutPreparseData> data =
+          UncompiledDataWithoutPreparseData::cast(*this);
+      os << "<UncompiledDataWithoutPreparseData (" << data->start_position()
+         << ", " << data->end_position() << ")]>";
+      break;
+    }
+
+    case UNCOMPILED_DATA_WITH_PREPARSE_DATA_TYPE: {
+      Tagged<UncompiledDataWithPreparseData> data =
+          UncompiledDataWithPreparseData::cast(*this);
+      os << "<UncompiledDataWithPreparseData (" << data->start_position()
+         << ", " << data->end_position()
+         << ") preparsed=" << Brief(data->preparse_data()) << ">";
+      break;
+    }
+
+    case SHARED_FUNCTION_INFO_TYPE: {
+      Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(*this);
+      std::unique_ptr<char[]> debug_name = shared->DebugNameCStr();
+      if (debug_name[0] != '\0') {
+        os << "<SharedFunctionInfo " << debug_name.get() << ">";
+      } else {
+        os << "<SharedFunctionInfo>";
+      }
+      break;
+    }
+    case JS_MESSAGE_OBJECT_TYPE:
+      os << "<JSMessageObject>";
+      break;
+#define MAKE_STRUCT_CASE(TYPE, Name, name)    \
+  case TYPE:                                  \
+    os << "<" #Name;                          \
+    Name::cast(*this)->BriefPrintDetails(os); \
+    os << ">";                                \
+    break;
+    STRUCT_LIST(MAKE_STRUCT_CASE)
+#undef MAKE_STRUCT_CASE
+    case ALLOCATION_SITE_TYPE: {
+      os << "<AllocationSite";
+      AllocationSite::cast(*this)->BriefPrintDetails(os);
+      os << ">";
+      break;
+    }
+    case SCOPE_INFO_TYPE: {
+      Tagged<ScopeInfo> scope = ScopeInfo::cast(*this);
+      os << "<ScopeInfo";
+      if (!scope->IsEmpty()) os << " " << scope->scope_type();
+      os << ">";
+      break;
+    }
+    case CODE_TYPE: {
+      Tagged<Code> code = Code::cast(*this);
+      os << "<Code " << CodeKindToString(code->kind());
+      if (code->is_builtin()) {
+        os << " " << Builtins::name(code->builtin_id());
+      }
+      os << ">";
+      break;
+    }
+    case HOLE_TYPE: {
+#define PRINT_HOLE(Type, Value, _) \
+  if (Is##Type(*this)) {           \
+    os << "<" #Value ">";          \
+    break;                         \
+  }
+      HOLE_LIST(PRINT_HOLE)
+#undef PRINT_HOLE
+      UNREACHABLE();
+    }
+    case INSTRUCTION_STREAM_TYPE: {
+      Tagged<InstructionStream> istream = InstructionStream::cast(*this);
+      Tagged<Code> code = istream->code(kAcquireLoad);
+      os << "<InstructionStream " << CodeKindToString(code->kind());
+      if (code->is_builtin()) {
+        os << " " << Builtins::name(code->builtin_id());
+      }
+      os << ">";
+      break;
+    }
+    case ODDBALL_TYPE: {
+      if (IsUndefined(*this)) {
+        os << "<undefined>";
+      } else if (IsNull(*this)) {
+        os << "<null>";
+      } else if (IsTrue(*this)) {
+        os << "<true>";
+      } else if (IsFalse(*this)) {
+        os << "<false>";
+      } else {
+        os << "<Odd Oddball: ";
+        os << Oddball::cast(*this)->to_string()->ToCString().get();
+        os << ">";
+      }
+      break;
+    }
+    case SYMBOL_TYPE: {
+      Tagged<Symbol> symbol = Symbol::cast(*this);
+      symbol->SymbolShortPrint(os);
+      break;
+    }
+    case HEAP_NUMBER_TYPE: {
+      UNREACHABLE();
+    }
+    case BIGINT_TYPE: {
+      os << "<BigInt ";
+      BigInt::cast(*this)->BigIntShortPrint(os);
+      os << ">";
+      break;
+    }
+    case JS_PROXY_TYPE:
+      os << "<JSProxy>";
+      break;
+    case FOREIGN_TYPE:
+      os << "<Foreign>";
+      break;
+    case CELL_TYPE: {
+      os << "<Cell value= ";
+      HeapStringAllocator allocator;
+      StringStream accumulator(&allocator);
+      ShortPrint(Cell::cast(*this)->value(), &accumulator);
+      os << accumulator.ToCString().get();
+      os << '>';
+      break;
+    }
+    case PROPERTY_CELL_TYPE: {
+      Tagged<PropertyCell> cell = PropertyCell::cast(*this);
+      os << "<PropertyCell name=";
+      ShortPrint(cell->name(), os);
+      os << " value=";
+      HeapStringAllocator allocator;
+      StringStream accumulator(&allocator);
+      ShortPrint(cell->value(kAcquireLoad), &accumulator);
+      os << accumulator.ToCString().get();
+      os << '>';
+      break;
+    }
+    case CONST_TRACKING_LET_CELL_TYPE: {
+      os << "<ConstTrackingLetCell>";
+      break;
+    }
+    case ACCESSOR_INFO_TYPE: {
+      Tagged<AccessorInfo> info = AccessorInfo::cast(*this);
+      os << "<AccessorInfo ";
+      os << "name=" << Brief(info->name());
+      os << ">";
+      break;
+    }
+    case FUNCTION_TEMPLATE_INFO_TYPE: {
+      Tagged<FunctionTemplateInfo> info = FunctionTemplateInfo::cast(*this);
+      os << "<FunctionTemplateInfo ";
+      Isolate* isolate;
+      if (GetIsolateFromHeapObject(*this, &isolate)) {
+        os << "callback= " << reinterpret_cast<void*>(info->callback(isolate));
+      } else {
+        os << "callback= " << kUnavailableString;
+      }
+      os << ", data= " << Brief(info->callback_data(kAcquireLoad));
+      os << ", has_side_effects= ";
+      if (info->has_side_effects()) {
+        os << "true>";
+      } else {
+        os << "false>";
+      }
+      break;
+    }
+    default:
+      os << "<Other heap object (" << map()->instance_type() << ")>";
+      break;
+  }
+}
+
 void HeapNumber::HeapNumberShortPrint(std::ostream& os) {
   static constexpr uint64_t kUint64AllBitsSet =
       static_cast<uint64_t>(int64_t{-1});
diff --git a/src/execution/isolate-inl.h b/src/execution/isolate-inl.h
index 256db29d140..5c826c933b7 100644
--- a/src/execution/isolate-inl.h
+++ b/src/execution/isolate-inl.h
@@ -23,6 +23,7 @@
 namespace v8 {
 namespace internal {
 
+
 // static
 V8_INLINE Isolate::PerIsolateThreadData*
 Isolate::CurrentPerIsolateThreadData() {
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index d5b2a87726c..4bfa456d0b6 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -4746,6 +4746,242 @@ VirtualMemoryCage* Isolate::GetPtrComprCodeCageForTesting() {
                                      : isolate_group_->GetPtrComprCage();
 }
 
+void Isolate::SetHeapAllocationAllowedWithoutGC(bool is_allowed) {
+  is_heap_allocation_allowed_without_gc = is_allowed;
+}
+
+bool Isolate::IsHeapAllocationAllowedWithoutGC() {
+  return is_heap_allocation_allowed_without_gc;
+}
+
+std::string Isolate::GetDumpPositionFilename() {
+  DCHECK_NE(dumping_seed, 0);
+
+  std::ostringstream os;
+  os << dumping_seed << "_position_dump.json";
+
+  return os.str();
+}
+
+std::string Isolate::GetDumpOutFilename() {
+  DCHECK_NE(dumping_seed, 0);
+
+  std::ostringstream os;
+  os << dumping_seed << "_output_dump.txt";
+
+  return os.str();
+}
+
+unsigned int Isolate::GetFunctionId(Tagged<JSFunction> function) {
+  Tagged<Script> script = Script::cast(function->shared()->script());
+
+  int token_pos = function->shared()->function_token_position();
+  DCHECK_NE(token_pos, kNoSourcePosition);
+
+  unsigned int line_num = (unsigned int)(script->GetLineNumber(token_pos));
+  unsigned int col_num = (unsigned int)(script->GetColumnNumber(token_pos));
+
+  DCHECK(line_num < kMaxUInt16);
+  DCHECK(col_num < kMaxUInt16);
+
+  return (col_num << 16) | (line_num);
+}
+
+void Isolate::RecordDumpPosition(Tagged<JSFunction> function, unsigned int bytecode_offset) {
+  DCHECK(IsDumpingEnabled());
+
+  DCHECK(v8_flags.generate_dump_positions);
+
+  unsigned int func_id = GetFunctionId(function);
+
+  func_dumping_position_cache[func_id].to_bytecode_offsets.insert(bytecode_offset);
+}
+
+bool Isolate::IsAllowedInterpreterDumpPosition(Tagged<JSFunction> function, unsigned int bytecode_offset) {
+  DCHECK(IsDumpingEnabled());
+  unsigned int func_id = GetFunctionId(function);
+
+  if (!func_dumping_position_cache.contains(func_id)) {
+    // encountered function that did not get compiled!
+    // no valid dump position since no turbofan dumps can exist here!
+    return false;
+  }
+
+  DumpPosition dump_position = func_dumping_position_cache[func_id];
+
+  return (dump_position.to_bytecode_offsets.contains(bytecode_offset));
+}
+
+void Isolate::WriteAndResetDumpOut() {
+  DCHECK(IsDumpingEnabled());
+  std::string fpath = "/tmp/" + GetDumpOutFilename();
+  std::ofstream ofs(fpath, std::ofstream::out);
+
+  ofs << dump_output;
+  ofs.close();
+
+  dump_output.clear();
+}
+
+
+template<typename T>
+std::optional<std::string> Isolate::DumpValue(T value, T& last_value) {
+    if (value == last_value) {
+        return {};
+    }
+    last_value = value;
+    return std::to_string(value);
+}
+
+template<typename T>
+std::optional<std::string> Isolate::DumpValuePlain(T value, T& last_value) {
+    if (value == last_value) {
+        return {};
+    }
+    last_value = value;
+    return value;
+}
+
+std::optional<std::string> Isolate::DumpBytecodeOffset(int bytecode_offset) {
+  return DumpValue(bytecode_offset, last_frame.bytecode_offset);
+}
+
+std::optional<std::string> Isolate::DumpFunctionId(int function_id) {
+  return DumpValue(function_id, last_frame.function_id);
+}
+
+std::optional<std::string> Isolate::DumpArgCount(int arg_count) {
+  return DumpValue(arg_count, last_frame.arg_count);
+}
+
+std::optional<std::string> Isolate::DumpRegCount(int reg_count) {
+  return DumpValue(reg_count, last_frame.reg_count);
+}
+
+std::optional<std::string> Isolate::DumpAcc(std::string acc) {
+  if (acc == last_frame.acc) {
+      return {};
+  }
+  last_frame.acc = acc;
+  return acc;
+}
+
+std::optional<std::string> Isolate::DumpArg(unsigned int index, std::string arg) {
+  if (index >= last_frame.args.size()) {
+    last_frame.args.resize(index + 1);
+  }
+  return DumpValuePlain(arg, last_frame.args[index]);
+}
+
+std::optional<std::string> Isolate::DumpReg(unsigned int index, std::string reg) {
+  if (index >= last_frame.regs.size()) {
+    last_frame.regs.resize(index + 1);
+  }
+  return DumpValuePlain(reg, last_frame.regs[index]);
+}
+
+void Isolate::AppendDumpOut(std::string data) {
+  dump_output.append(data);
+}
+
+void Isolate::WriteDumpPositionToFile(std::string filename) {
+  DCHECK(IsDumpingEnabled());
+
+  if (func_dumping_position_cache.size() == 0) return;
+
+  std::string fpath = "/tmp/" + filename;
+  std::ofstream ofs(fpath, std::ofstream::out);
+
+  ofs << '{';
+  size_t func_idx = 0;
+  size_t func_count = func_dumping_position_cache.size();
+  for (auto& [func_id, dump_position]: func_dumping_position_cache) {
+    ofs << '"' << func_id << "\": ";
+
+    size_t to_idx = 0;
+    size_t to_count = func_dumping_position_cache[func_id].to_bytecode_offsets.size();
+
+    ofs << "[";
+    for (unsigned int to_i: func_dumping_position_cache[func_id].to_bytecode_offsets) {
+      if (to_idx != to_count - 1) {
+        ofs << to_i << ", ";
+      } else {
+        ofs << to_i;
+      }
+      to_idx += 1;
+    }
+    ofs << "]";
+
+    if (func_idx != func_count - 1) {
+      ofs << ", ";
+    }
+    func_idx += 1;
+  }
+  ofs << '}';
+  ofs << '\n';
+  ofs.close();
+
+  // We expect the cache to be cleared after we write
+  // it out to a file
+  ClearDumpPositionCache();
+  DCHECK_EQ(func_dumping_position_cache.size(), 0);
+}
+
+void Isolate::ReadDumpPositionFromFile(std::string filename, bool remove_file_after) {
+  DCHECK_EQ(func_dumping_position_cache.size(), 0);
+  DCHECK(!IsIsolateDumpDislabed());
+
+  std::string fpath = "/tmp/" + filename;
+
+  std::ifstream ifs(fpath);
+  DCHECK(ifs.is_open());
+
+  std::stringstream buffer;
+  buffer << ifs.rdbuf();
+  ifs.close();
+
+  std::string json_str = buffer.str();
+  size_t start_pos = json_str.find('{');
+  size_t end_pos = json_str.rfind('}');
+  std::string data = json_str.substr(start_pos, end_pos - start_pos + 1);
+
+  size_t pos = 0;
+  while ((pos = data.find('"', pos)) != std::string::npos) {
+      pos += 1;
+      std::string func_id_str;
+      unsigned int func_id;
+      while (data[pos] != '"') {
+          func_id_str += data[pos];
+          pos += 1;
+      }
+
+      func_id = std::stoi(func_id_str);
+
+      // Parse "to"
+      pos = data.find('[', pos);
+      pos += 1;
+      while (data[pos] != ']') {
+          std::string num_str;
+          while (data[pos] != ',' && data[pos] != ']') {
+              num_str += data[pos];
+              pos += 1;
+          }
+
+          unsigned int to_value = std::stoi(num_str);
+          func_dumping_position_cache[func_id].to_bytecode_offsets.insert(to_value);
+
+          if (data[pos] != ']') pos += 2;
+      }
+  }
+  if (remove_file_after) {
+    DCHECK_EQ(std::remove(fpath.c_str()), 0);
+  }
+}
+
+void Isolate::ClearDumpPositionCache() {
+  func_dumping_position_cache.clear();
+}
+
 void Isolate::VerifyStaticRoots() {
 #if V8_STATIC_ROOTS_BOOL
   static_assert(ReadOnlyHeap::IsReadOnlySpaceShared(),
@@ -5237,6 +5473,10 @@ bool Isolate::Init(SnapshotData* startup_snapshot_data,
 
   isolate_data_.continuation_preserved_embedder_data_ =
       *factory()->undefined_value();
+  if (v8_flags.load_dump_positions && v8_flags.dumping_seed != 0 && (!IsIsolateDumpDislabed())) {
+    SetDumpingSeed(v8_flags.dumping_seed);
+    ReadDumpPositionFromFile();
+  }
 
   {
     HandleScope scope(this);
@@ -5480,6 +5720,18 @@ void Isolate::DumpAndResetStats() {
     // v8_enable_builtins_profiling=true
     CHECK_NULL(v8_flags.turbo_profiling_output);
   }
+
+  if (IsDumpingEnabled()) {
+    if (v8_flags.dumping_seed != 0) {
+      // if the flag was not set during fuzzing in the repl than
+      // we assume that the flag is present
+      SetDumpingSeed(v8_flags.dumping_seed);
+    }
+
+    WriteAndResetDumpOut();
+
+    if (v8_flags.generate_dump_positions) WriteDumpPositionToFile();
+  } 
 }
 
 void Isolate::IncreaseConcurrentOptimizationPriority(
@@ -5558,6 +5810,7 @@ bool Isolate::NeedsSourcePositions() const {
       v8_flags.print_maglev_code || v8_flags.perf_prof || v8_flags.log_maps ||
       v8_flags.log_ic || v8_flags.log_function_events ||
       v8_flags.heap_snapshot_on_oom ||
+      v8_flags.turbofan_dumping || v8_flags.maglev_dumping || v8_flags.sparkplug_dumping || v8_flags.interpreter_dumping ||
       // Dynamic conditions; changing any of these conditions triggers source
       // position collection for the entire heap
       // (CollectSourcePositionsForAllBytecodeArrays).
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index c39eb6ac255..2a61ae4e350 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -121,6 +121,7 @@ class CompilationStatistics;
 class Counters;
 class Debug;
 class Deoptimizer;
+class DeoptFrameDumper;
 class DescriptorLookupCache;
 class EmbeddedFileWriterInterface;
 class EternalHandles;
@@ -1275,6 +1276,17 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
     DCHECK_NOT_NULL(deoptimizer);
     current_deoptimizer_ = deoptimizer;
   }
+  DeoptFrameDumper* GetAndClearCurrentDeoptFrameDumper() {
+    DeoptFrameDumper* result = current_deopt_frame_dumper_;
+    CHECK_NOT_NULL(result);
+    current_deopt_frame_dumper_ = nullptr;
+    return result;
+  }
+  void set_current_deopt_frame_dumper(DeoptFrameDumper* deopt_frame_dumper) {
+    DCHECK_NULL(current_deopt_frame_dumper_);
+    DCHECK_NOT_NULL(deopt_frame_dumper);
+    current_deopt_frame_dumper_ = deopt_frame_dumper;
+  }
   bool deoptimizer_lazy_throw() const { return deoptimizer_lazy_throw_; }
   void set_deoptimizer_lazy_throw(bool value) {
     deoptimizer_lazy_throw_ = value;
@@ -2243,6 +2255,119 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
     battery_saver_mode_enabled_ = battery_saver_mode_enabled;
   }
 
+  void SetDumpPositionFilename(const char *full_file_name);
+  void RecordDumpPosition(Tagged<JSFunction> function, unsigned int bytecode_offset);
+  bool IsAllowedInterpreterDumpPosition(Tagged<JSFunction> function, unsigned int bytecode_offset);
+
+  unsigned int GetFunctionId(Tagged<JSFunction> function);
+
+  void SetHeapAllocationAllowedWithoutGC(bool is_allowed);
+  bool IsHeapAllocationAllowedWithoutGC();
+  void SetDumpingSeed(unsigned int seed) {
+    DCHECK_NE(seed, 0);
+    dumping_seed = seed;
+  }
+
+  void SetIsolateDumpDislabed() {
+    isolate_dumping_disabled = true;
+  }
+
+  bool IsIsolateDumpDislabed() const {
+    return isolate_dumping_disabled;
+  }
+
+  void SetInScriptDumpingEnabled(bool is_enabled) {
+    in_script_dumping_enabled = is_enabled;
+  }
+
+  bool AnyDumpingFlagsSet() const {
+    return v8_flags.turbofan_dumping || v8_flags.interpreter_dumping || v8_flags.maglev_dumping || v8_flags.sparkplug_dumping;
+  }
+
+  bool IsInScriptDumpingEnabled() const {
+    return (in_script_dumping_enabled || v8_flags.overwrite_temporary_script_dumping_deactivation) && AnyDumpingFlagsSet();
+  }
+
+  bool IsDumpingEnabled() const {
+    return !IsIsolateDumpDislabed() && IsInScriptDumpingEnabled();
+  }
+
+  bool DumpNextInterpreterFrame() const {
+    return dump_next_interpreter_frame && v8_flags.jit_return_dump;
+  }
+
+  void SetDumpNextInterpreterFrame(bool dump_next) {
+    dump_next_interpreter_frame = dump_next;
+  }
+
+  std::string GetDumpPositionFilename();
+  std::string GetDumpOutFilename();
+
+  void WriteAndResetDumpOut();
+  void AppendDumpOut(std::string data);
+  void ReadDumpPositionFromFile(std::string filename, bool remove_file_after = true);
+  void WriteDumpPositionToFile(std::string filename);
+  void ClearDumpPositionCache();
+
+  void WriteDumpPositionToFile() {
+    WriteDumpPositionToFile(GetDumpPositionFilename());
+  }
+
+  void ReadDumpPositionFromFile(bool remove_file_after = true) {
+    ReadDumpPositionFromFile(GetDumpPositionFilename(), remove_file_after);
+  }
+
+  typedef enum JITType {
+    kTurbofan = 1 << 0,
+    kMaglev = 1 << 1,
+    kSparkplug = 1 << 2
+  } JITType;
+
+  uint8_t GetJITState() {
+    uint8_t tmp = current_run_jit_state;
+    // reset jit state to not brick in repl
+    current_run_jit_state = 0;
+    return tmp;
+  }
+
+  void RecordJITUsage(JITType type) {
+    current_run_jit_state |= type;
+  }
+
+  template<typename T>
+  std::optional<std::string> DumpValue(T value, T& last_value);
+
+  template<typename T>
+  std::optional<std::string> DumpValuePlain(T value, T& last_value);
+
+  std::optional<std::string> DumpBytecodeOffset(int bytecode_offset);
+
+  std::optional<std::string> DumpFunctionId(int function_id);
+
+  std::optional<std::string> DumpArgCount(int arg_count);
+
+  std::optional<std::string> DumpRegCount(int reg_count);
+
+  std::optional<std::string> DumpAcc(std::string acc);
+
+  std::optional<std::string> DumpArg(unsigned int index, std::string arg);
+
+  std::optional<std::string> DumpReg(unsigned int index, std::string reg);
+
+  void ClearLastFrame() {
+    last_frame = {-1,
+                  "NOT GONNA HAPPEN123",
+                  -1,
+                  std::vector<std::string>(64),
+                  -1,
+                  std::vector<std::string>(128),
+                  -1};
+  }
+  
+  void DiffPrintClear() {
+    ClearLastFrame();
+  }
+
  private:
   explicit Isolate(IsolateGroup* isolate_group);
   ~Isolate();
@@ -2392,6 +2517,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   StubCache* store_stub_cache_ = nullptr;
   StubCache* define_own_stub_cache_ = nullptr;
   Deoptimizer* current_deoptimizer_ = nullptr;
+  DeoptFrameDumper* current_deopt_frame_dumper_ = nullptr;
   bool deoptimizer_lazy_throw_ = false;
   MaterializedObjectStore* materialized_object_store_ = nullptr;
   bool capture_stack_trace_for_uncaught_exceptions_ = false;
@@ -2728,6 +2854,47 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // Isolate::Delete() are used for Isolate creation and deletion.
   void* operator new(size_t, void* ptr) { return ptr; }
 
+  struct DumpPosition {
+    std::set<unsigned int, std::greater<unsigned int>> to_bytecode_offsets;
+  };
+
+  std::unordered_map<unsigned int, DumpPosition> func_dumping_position_cache;
+
+  long long prev_function_id = -1;
+
+  bool is_heap_allocation_allowed_without_gc = false;
+
+  unsigned int dumping_seed;
+
+  bool isolate_dumping_disabled = false;
+  bool in_script_dumping_enabled = false;
+  bool dump_next_interpreter_frame = false;
+
+  uint8_t current_run_jit_state = 0;
+
+  struct LastFrame {
+    int bytecode_offset;
+    std::string acc;
+    int arg_count;
+    std::vector<std::string> args;
+    int reg_count;
+    std::vector<std::string> regs;
+    int function_id;
+  };
+
+  // initialize struct and reserve 64 elements in args
+  LastFrame last_frame = {-1,
+                          "NOT GONNA HAPPEN123",
+                          -1,
+                          std::vector<std::string>(64),
+                          -1,
+                          std::vector<std::string>(128),
+                          -1};
+
+  std::string dump_output;
+
+
+
 #if USE_SIMULATOR
   SimulatorData* simulator_data_ = nullptr;
 #endif
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index 57888f789cc..7dd04dbaea6 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -2183,6 +2183,40 @@ DEFINE_BOOL(trace_deopt, false, "trace deoptimization")
 DEFINE_BOOL(log_deopt, false, "log deoptimization")
 DEFINE_BOOL(trace_deopt_verbose, false, "extra verbose deoptimization tracing")
 DEFINE_IMPLICATION(trace_deopt_verbose, trace_deopt)
+
+// dumping
+DEFINE_BOOL(interpreter_dumping, false, "enable frame dumping in the interpreter")
+DEFINE_BOOL(turbofan_dumping, false, "enable frame dumping in TurboFan")
+DEFINE_BOOL(maglev_dumping, false, "enable frame dumping in TurboFan")
+DEFINE_BOOL(sparkplug_dumping, false, "enable frame dumping in Sparkplug")
+
+// needed for function id
+DEFINE_NEG_IMPLICATION(interpreter_dumping, enable_lazy_source_positions)
+DEFINE_NEG_IMPLICATION(turbofan_dumping, enable_lazy_source_positions)
+DEFINE_NEG_IMPLICATION(maglev_dumping, enable_lazy_source_positions)
+
+
+DEFINE_BOOL(generate_dump_positions, false, "create a JSON file with all dumping source positions for the interpreter")
+DEFINE_BOOL(load_dump_positions, false, "load JSON file generated by --generate_dump_positions")
+DEFINE_INT(dumping_seed, 0, "seed for the source position dump file that is generated with --generate-dump-positions")
+
+DEFINE_BOOL(verify_heap_on_jit_dump, false, "verify the heap on every turbofan dump (will find deopt bugs early for perf cost)")
+DEFINE_BOOL(jit_return_dump, false, "dump the first interpreter frame after we return from JIT code")
+// don't allow interpreter_dumping in combination with jit_return_dump.
+// otherwise we get double frames and this only makes sense in JIT runs anyways 
+DEFINE_NEG_IMPLICATION(jit_return_dump, interpreter_dumping)
+
+DEFINE_BOOL(turbofan_dumping_print_deopt_frames, false, "print actual deopt frames as well")
+
+DEFINE_BOOL(overwrite_temporary_script_dumping_deactivation, false, "Ignores Disable/EnableFrameDumping in script")
+
+DEFINE_BOOL(dumping_debug, false, "print debug info for in the dumps")
+
+DEFINE_INT(dumpling_depth, 3, "depth used in dumping")
+DEFINE_INT(dumpling_prop_count, 5, "amount of properties/elements to dump for every object")
+
+
+
 DEFINE_BOOL(trace_file_names, false,
             "include file names in trace-opt/trace-deopt output")
 DEFINE_BOOL(always_turbofan, false, "always try to optimize functions")
diff --git a/src/heap/heap-allocator-inl.h b/src/heap/heap-allocator-inl.h
index 2cf24afd6e2..6f421b34933 100644
--- a/src/heap/heap-allocator-inl.h
+++ b/src/heap/heap-allocator-inl.h
@@ -69,11 +69,11 @@ V8_WARN_UNUSED_RESULT V8_INLINE AllocationResult HeapAllocator::AllocateRaw(
     int size_in_bytes, AllocationOrigin origin, AllocationAlignment alignment) {
   DCHECK(!heap_->IsInGC());
   DCHECK(AllowHandleAllocation::IsAllowed());
-  DCHECK(AllowHeapAllocation::IsAllowed());
   DCHECK(local_heap_->IsRunning());
 #if DEBUG
   local_heap_->VerifyCurrent();
 #endif
+  DCHECK(AllowHeapAllocation::IsAllowed() || heap_->isolate()->IsHeapAllocationAllowedWithoutGC());
 
   if (v8_flags.single_generation.value() && type == AllocationType::kYoung) {
     return AllocateRaw(size_in_bytes, AllocationType::kOld, origin, alignment);
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index d82d9043691..c9c063e4d4b 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -1466,6 +1466,8 @@ bool GCCallbacksScope::CheckReenter() const {
 }
 
 void Heap::HandleGCRequest() {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+
   if (IsStressingScavenge() && stress_scavenge_observer_->HasRequestedGC()) {
     CollectGarbage(NEW_SPACE, GarbageCollectionReason::kTesting);
     stress_scavenge_observer_->RequestedGCDone();
@@ -1515,6 +1517,8 @@ void Heap::StartMinorMSIncrementalMarkingIfNeeded() {
 void Heap::CollectAllGarbage(GCFlags gc_flags,
                              GarbageCollectionReason gc_reason,
                              const v8::GCCallbackFlags gc_callback_flags) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+
   current_gc_flags_ = gc_flags;
   CollectGarbage(OLD_SPACE, gc_reason, gc_callback_flags);
   DCHECK_EQ(GCFlags(GCFlag::kNoFlags), current_gc_flags_);
@@ -1582,6 +1586,8 @@ void ReportDuplicates(int size, std::vector<Tagged<HeapObject>>* objects) {
 }  // anonymous namespace
 
 void Heap::CollectAllAvailableGarbage(GarbageCollectionReason gc_reason) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+
   // Min and max number of attempts for GC. The method will continue with more
   // GCs until the root set is stable.
   static constexpr int kMaxNumberOfAttempts = 7;
@@ -1774,6 +1780,8 @@ void Heap::ResetOldGenerationAndGlobalAllocationLimit() {
 void Heap::CollectGarbage(AllocationSpace space,
                           GarbageCollectionReason gc_reason,
                           const v8::GCCallbackFlags gc_callback_flags) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+  
   if (V8_UNLIKELY(!deserialization_complete_)) {
     // During isolate initialization heap always grows. GC is only requested
     // if a new page allocation fails. In such a case we should crash with
@@ -2217,6 +2225,8 @@ bool Heap::CollectionRequested() {
 }
 
 void Heap::CollectGarbageForBackground(LocalHeap* local_heap) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+
   CHECK(local_heap->is_main_thread());
   CollectAllGarbage(current_gc_flags_,
                     GarbageCollectionReason::kBackgroundAllocationFailure,
@@ -2502,6 +2512,8 @@ void Heap::ResumeConcurrentThreadsInClients(
 
 bool Heap::CollectGarbageShared(LocalHeap* local_heap,
                                 GarbageCollectionReason gc_reason) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return true; // ignored
+
   CHECK(deserialization_complete());
   DCHECK(isolate()->has_shared_space());
 
@@ -2512,6 +2524,10 @@ bool Heap::CollectGarbageShared(LocalHeap* local_heap,
 
 bool Heap::CollectGarbageFromAnyThread(LocalHeap* local_heap,
                                        GarbageCollectionReason gc_reason) {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) {
+    return isolate() == local_heap->heap()->isolate() && local_heap->is_main_thread();
+  }
+  
   DCHECK(local_heap->IsRunning());
 
   if (isolate() == local_heap->heap()->isolate() &&
@@ -4234,6 +4250,8 @@ void Heap::CheckMemoryPressure() {
 }
 
 void Heap::CollectGarbageOnMemoryPressure() {
+  if (isolate()->IsHeapAllocationAllowedWithoutGC()) return;
+
   const int kGarbageThresholdInBytes = 8 * MB;
   const double kGarbageThresholdAsFractionOfTotalMemory = 0.1;
   // This constant is the maximum response time in RAIL performance model.
diff --git a/src/heap/local-heap.cc b/src/heap/local-heap.cc
index 4ca93ad03ef..fc38f3eff34 100644
--- a/src/heap/local-heap.cc
+++ b/src/heap/local-heap.cc
@@ -157,6 +157,15 @@ void LocalHeap::SetUpSharedMarking() {
   }
 }
 
+void LocalHeap::Safepoint() {
+  DCHECK(AllowSafepoints::IsAllowed() || heap()->isolate()->IsHeapAllocationAllowedWithoutGC());
+  ThreadState current = state_.load_relaxed();
+
+  if (V8_UNLIKELY(current.IsRunningWithSlowPathFlag())) {
+    SafepointSlowPath();
+  }
+}
+
 void LocalHeap::EnsurePersistentHandles() {
   if (!persistent_handles_) {
     persistent_handles_ = heap_->isolate()->NewPersistentHandles();
diff --git a/src/heap/local-heap.h b/src/heap/local-heap.h
index a501eefc842..4140ca4becd 100644
--- a/src/heap/local-heap.h
+++ b/src/heap/local-heap.h
@@ -49,14 +49,7 @@ class V8_EXPORT_PRIVATE LocalHeap {
 
   // Frequently invoked by local thread to check whether safepoint was requested
   // from the main thread.
-  void Safepoint() {
-    DCHECK(AllowSafepoints::IsAllowed());
-    ThreadState current = state_.load_relaxed();
-
-    if (V8_UNLIKELY(current.IsRunningWithSlowPathFlag())) {
-      SafepointSlowPath();
-    }
-  }
+  void Safepoint();
 
   LocalHandles* handles() { return handles_.get(); }
 
diff --git a/src/heap/memory-measurement.cc b/src/heap/memory-measurement.cc
index a743733d92a..a536e998455 100644
--- a/src/heap/memory-measurement.cc
+++ b/src/heap/memory-measurement.cc
@@ -46,6 +46,10 @@ class MemoryMeasurementResultBuilder {
     other_.push_back(NewResult(estimate, lower_bound, upper_bound));
   }
   void AddWasm(size_t code, size_t metadata) {
+    if (v8_flags.predictable) {
+      code = 0;
+      metadata = 1;
+    }
     Handle<JSObject> wasm = NewJSObject();
     AddProperty(wasm, factory_->NewStringFromAsciiChecked("code"),
                 NewNumber(code));
@@ -70,6 +74,11 @@ class MemoryMeasurementResultBuilder {
  private:
   Handle<JSObject> NewResult(size_t estimate, size_t lower_bound,
                              size_t upper_bound) {
+    if (v8_flags.predictable) {
+      estimate = 1;
+      lower_bound = 1;
+      upper_bound = 1;
+    }
     Handle<JSObject> result = NewJSObject();
     Handle<Object> estimate_obj = NewNumber(estimate);
     AddProperty(result, factory_->jsMemoryEstimate_string(), estimate_obj);
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index 934dc03acec..a8ee0096ce2 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -48,6 +48,13 @@ InterpreterAssembler::InterpreterAssembler(CodeAssemblerState* state,
 #ifdef V8_TRACE_UNOPTIMIZED
   TraceBytecode(Runtime::kTraceUnoptimizedBytecodeEntry);
 #endif
+  // we cant do this generally for temporary enable/disable
+  // since we might have produced bytecode before the dumping
+  // is enabled in the same function
+  // this will introduce a little bit of overhead but its fine
+  if (!isolate()->IsIsolateDumpDislabed()) {
+    GenerateInterpreterDumpingHandler();
+  }
   RegisterCallGenerationCallbacks([this] { CallPrologue(); },
                                   [this] { CallEpilogue(); });
 
@@ -68,6 +75,10 @@ InterpreterAssembler::~InterpreterAssembler() {
   UnregisterCallGenerationCallbacks();
 }
 
+void InterpreterAssembler::GenerateInterpreterDumpingHandler() {
+  TraceBytecode(Runtime::kTraceSmileyface);
+}
+
 TNode<RawPtrT> InterpreterAssembler::GetInterpretedFramePointer() {
   if (!interpreted_frame_pointer_.IsBound()) {
     interpreted_frame_pointer_ = LoadParentFramePointer();
@@ -1254,6 +1265,7 @@ void InterpreterAssembler::InlineShortStar(TNode<WordT> target_bytecode) {
 #ifdef V8_TRACE_UNOPTIMIZED
   TraceBytecode(Runtime::kTraceUnoptimizedBytecodeEntry);
 #endif
+  GenerateInterpreterDumpingHandler();
 
   StoreRegisterForShortStar(GetAccumulator(), target_bytecode);
 
diff --git a/src/interpreter/interpreter-assembler.h b/src/interpreter/interpreter-assembler.h
index 4c930fb8507..22c1dd95b3e 100644
--- a/src/interpreter/interpreter-assembler.h
+++ b/src/interpreter/interpreter-assembler.h
@@ -24,6 +24,8 @@ class V8_EXPORT_PRIVATE InterpreterAssembler : public CodeStubAssembler {
   InterpreterAssembler(const InterpreterAssembler&) = delete;
   InterpreterAssembler& operator=(const InterpreterAssembler&) = delete;
 
+  void GenerateInterpreterDumpingHandler();
+
   // Returns the 32-bit unsigned count immediate for bytecode operand
   // |operand_index| in the current bytecode.
   TNode<Uint32T> BytecodeOperandCount(int operand_index);
diff --git a/src/maglev/maglev-assembler-inl.h b/src/maglev/maglev-assembler-inl.h
index 3546fe5073d..7b0b381c00f 100644
--- a/src/maglev/maglev-assembler-inl.h
+++ b/src/maglev/maglev-assembler-inl.h
@@ -638,7 +638,7 @@ void MoveArgumentsForBuiltin(MaglevAssembler* masm, Args&&... args) {
 inline void MaglevAssembler::CallBuiltin(Builtin builtin) {
   // Special case allowing calls to DoubleToI, which takes care to preserve all
   // registers and therefore doesn't require special spill handling.
-  DCHECK(allow_call() || builtin == Builtin::kDoubleToI);
+  DCHECK(allow_call() || builtin == Builtin::kDoubleToI || builtin == Builtin::kDumpTurboFrame);
 
   // Temporaries have to be reset before calling CallBuiltin, in case it uses
   // temporaries that alias register parameters.
diff --git a/src/maglev/maglev-assembler.h b/src/maglev/maglev-assembler.h
index 2f6c96d404a..caf06eb61ec 100644
--- a/src/maglev/maglev-assembler.h
+++ b/src/maglev/maglev-assembler.h
@@ -319,7 +319,9 @@ class V8_EXPORT_PRIVATE MaglevAssembler : public MacroAssembler {
   template <typename NodeT>
   inline Label* GetDeoptLabel(NodeT* node, DeoptimizeReason reason);
   inline bool IsDeoptLabel(Label* label);
+  inline bool IsTopFrameJavascript(Label *label);
   inline void EmitEagerDeoptStress(Label* label);
+  inline void EmitDumpMaglevFrame(Label* label);
   template <typename NodeT>
   inline void EmitEagerDeopt(NodeT* node, DeoptimizeReason reason);
   template <typename NodeT>
@@ -719,6 +721,16 @@ inline bool MaglevAssembler::IsDeoptLabel(Label* label) {
   return false;
 }
 
+inline bool MaglevAssembler::IsTopFrameJavascript(Label* label) {
+  DCHECK(IsDeoptLabel(label));
+  for (auto deopt : code_gen_state_->eager_deopts()) {
+    if (deopt->deopt_entry_label() == label) {
+      return deopt->top_frame().type() == DeoptFrame::FrameType::kInterpretedFrame;
+    }
+  }
+  UNREACHABLE();
+}
+
 template <typename NodeT>
 inline Label* MaglevAssembler::GetDeoptLabel(NodeT* node,
                                              DeoptimizeReason reason) {
diff --git a/src/maglev/x64/maglev-assembler-x64-inl.h b/src/maglev/x64/maglev-assembler-x64-inl.h
index f9d17553a11..b545743faed 100644
--- a/src/maglev/x64/maglev-assembler-x64-inl.h
+++ b/src/maglev/x64/maglev-assembler-x64-inl.h
@@ -841,6 +841,12 @@ inline void MaglevAssembler::JumpToDeopt(Label* target) {
   jmp(target);
 }
 
+inline void MaglevAssembler::EmitDumpMaglevFrame(Label* target) {
+  DCHECK(v8_flags.maglev_dumping);
+
+  CallBuiltin(Builtin::kDumpTurboFrame);
+}
+
 inline void MaglevAssembler::EmitEagerDeoptStress(Label* target) {
   if (V8_LIKELY(v8_flags.deopt_every_n_times <= 0)) {
     return;
@@ -881,6 +887,9 @@ inline void MaglevAssembler::JumpIf(Condition cond, Label* target,
   }
   DCHECK_IMPLIES(IsDeoptLabel(target), distance == Label::kFar);
   j(cond, target, distance);
+  if (v8_flags.maglev_dumping && IsDeoptLabel(target) && IsTopFrameJavascript(target) && !isolate()->IsIsolateDumpDislabed()) {
+    EmitDumpMaglevFrame(target);
+  }
 }
 
 inline void MaglevAssembler::JumpIfRoot(Register with, RootIndex index,
diff --git a/src/objects/dumping.cc b/src/objects/dumping.cc
new file mode 100644
index 00000000000..7a396525db0
--- /dev/null
+++ b/src/objects/dumping.cc
@@ -0,0 +1,25 @@
+#include "src/objects/dumping.h"
+#include "src/common/globals.h"
+
+namespace v8 {
+namespace internal {
+
+int GetOne() {
+  return 1;
+}
+
+// void FramePrint(UnoptimizedFrame* frame,
+//                 Tagged<JSFunction> function,
+//                 Isolate* isolate,
+//                 int bytecode_offset,
+//                 int deopt_point_id,
+//                 DumpFrameType frame_dump_type) {
+//   static const char kAccumulator[] = "accumulator";
+//   static const int kRegFieldWidth = static_cast<int>(sizeof(kAccumulator) - 1);
+//
+//   Handle<BytecodeArray> bytecode_array(
+//     function->shared()->GetBytecodeArray(isolate), isolate);
+// }
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/objects/dumping.h b/src/objects/dumping.h
new file mode 100644
index 00000000000..c66a804fa7d
--- /dev/null
+++ b/src/objects/dumping.h
@@ -0,0 +1,39 @@
+#ifndef V8_OBJECTS_DUMPING_H_
+#define V8_OBJECTS_DUMPING_H_
+
+#include "src/execution/frames.h"
+#include "src/interpreter/bytecodes.h"
+
+
+namespace v8 {
+namespace internal {
+
+typedef enum DumpFrameType {
+  kTurbofanFrame = 0,
+  kMaglevFrame = 1,
+  kDeoptFrame = 2,
+  kSparkplugFrame = 3,
+  kInterpreterFrame = 4,
+  kReturnJITFrame = 5,
+} DumpFrameType;
+
+
+void PrintDumpedFrame(UnoptimizedFrame* frame,
+                Tagged<JSFunction> function,
+                Isolate* isolate,
+                int bytecode_offset,
+                int deopt_point_id,
+                DumpFrameType frame_dump_type);
+
+void DoPrint(UnoptimizedFrame* frame, Tagged<JSFunction> function,
+    Isolate* isolate,
+    int bytecode_offset,
+    const uint8_t* bytecode_address, int deopt_point_id,
+    DumpFrameType frame_dump_type,
+    Handle<BytecodeArray> bytecode_array,
+    Handle<Object> accumulator,
+    interpreter::Bytecode bytecode);
+} // namespace internal
+}  // namespace v8
+
+#endif  // V8_OBJECTS_DUMPING_H_
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index 3ecab0b17ca..4fe69ec54fe 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -418,6 +418,7 @@ class HeapObject : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   void Print();
   static void Print(Tagged<Object> obj);
   static void Print(Tagged<Object> obj, std::ostream& os);
+  void HeapObjectDifferentialFuzzingPrint(std::ostream& os, int depth);
 #ifdef OBJECT_PRINT
   void PrintHeader(std::ostream& os, const char* id);
 #endif
diff --git a/src/objects/js-objects.cc b/src/objects/js-objects.cc
index 6c99451fab8..9f8e4d00039 100644
--- a/src/objects/js-objects.cc
+++ b/src/objects/js-objects.cc
@@ -26,6 +26,7 @@
 #include "src/objects/allocation-site-inl.h"
 #include "src/objects/api-callbacks.h"
 #include "src/objects/arguments-inl.h"
+#include "src/objects/descriptor-array.h"
 #include "src/objects/dictionary.h"
 #include "src/objects/elements.h"
 #include "src/objects/field-type.h"
@@ -88,6 +89,7 @@
 #include "src/strings/string-builder-inl.h"
 #include "src/strings/string-stream.h"
 #include "src/utils/ostreams.h"
+#include "src/json/json-stringifier.h"
 
 #if V8_ENABLE_WEBASSEMBLY
 #include "src/debug/debug-wasm-objects.h"
@@ -2788,6 +2790,394 @@ void JSObject::SetNormalizedElement(Handle<JSObject> object, uint32_t index,
   object->set_elements(*dictionary);
 }
 
+std::string StringShortPrint(Tagged<String> s) {
+  std::stringstream out;
+  static const int kMaxShortPrintLength = 1024;
+
+  if (!s->LooksValid()) {
+    out << "<Invalid String>";
+    return out.str();
+  }
+
+  const int len = s->length();
+  out << std::format("<String[{}]: ", len);
+  
+  if (len > kMaxShortPrintLength) {
+    out << "...<truncated>>>";
+    return out.str();
+  }
+
+  for (int i = 0; i < len; i++) {
+    out << (char)s->Get(i);
+  }
+  out << '>';
+  return out.str();
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintInternalIndexRange(StringStream *accumulator, int depth, bool is_fast_object) {
+  Isolate* isolate = GetIsolate();
+
+  HandleScope scope(isolate);
+  Tagged<DescriptorArray> descriptors = map()->instance_descriptors(isolate);
+
+  InternalIndex::Range index_range(0);
+
+  std::optional<Tagged<NameDictionary>> dict;
+  std::optional<ReadOnlyRoots> roots;
+
+  if (is_fast_object) {
+    index_range = map()->IterateOwnDescriptors();
+  } else {
+    dict = property_dictionary();
+    roots = dict.value()->GetReadOnlyRoots();
+
+    index_range = dict.value()->IterateEntries();
+  }
+
+  accumulator->Add("{");
+
+  // property_name => full property acc
+  // this is sorts by property name!
+  std::map<std::string, std::string> property_string_cache;
+
+  for (InternalIndex i : index_range) {
+    std::stringstream property_acc;
+
+    Handle<String> key_name;
+
+    PropertyDetails details = PropertyDetails::Empty();
+    {
+      DisallowGarbageCollection no_gc;
+      if (is_fast_object) {
+        Tagged<Name> name;
+        name = descriptors->GetKey(i);
+        if (!IsString(name)) continue;
+        key_name = handle(String::cast(name), isolate);
+      } else {
+        Tagged<Object> k;
+        if (!dict.value()->ToKey(roots.value(), i, &k)) continue;
+        if (!IsString(k)) continue;
+        key_name = handle(String::cast(k), isolate);
+      }
+
+      if (is_fast_object) {
+        details = descriptors->GetDetails(i);
+      } else {
+        details = dict.value()->DetailsAt(i);
+      }
+    }
+
+    std::string key_name_str = StringShortPrint(*key_name);
+    property_acc << key_name_str;
+
+    if (details.IsReadOnly()) property_acc << "[ro]";
+    if (details.IsConfigurable()) property_acc << "[c]";
+    if (details.IsEnumerable()) property_acc << "[e]";
+
+    if (is_fast_object) {
+      switch (details.location()) {
+        case PropertyLocation::kField: {
+          FieldIndex field_index = FieldIndex::ForDetails(map(), details);
+          property_acc << DifferentialFuzzingPrintObject(RawFastPropertyAt(field_index), depth - 1).c_str();
+          break;
+        }
+        case PropertyLocation::kDescriptor:
+          property_acc << DifferentialFuzzingPrintObject(descriptors->GetStrongValue(i), depth - 1).c_str();
+          break;
+      }
+    } else {
+      property_acc << DifferentialFuzzingPrintObject(dict.value()->ValueAt(i), depth - 1).c_str();
+    }
+
+    property_string_cache[key_name_str] = property_acc.str();
+  }
+
+  int i = 0;
+  for (const auto& [_, value] : property_string_cache) {
+    accumulator->Add(value.c_str());
+    if (i != ((int)property_string_cache.size()) - 1) {
+      accumulator->Add(", ");
+    }
+    i += 1;
+    if (i >= v8_flags.dumpling_prop_count) break;
+  }
+
+  accumulator->Add("}");
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintFastProperties(StringStream *accumulator, int depth) {
+  uint number_of_own_descriptors = map()->NumberOfOwnDescriptors();
+  if (number_of_own_descriptors == 0) {
+    accumulator->Add("{}");
+  } else {
+    JSObjectDifferentialFuzzingPrintInternalIndexRange(accumulator, depth, true);
+  }
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintDictProperties(StringStream *accumulator, int depth) {
+  if (IsJSGlobalObject(*this)) {
+    
+  } else {
+    if (property_dictionary()->Capacity() == 0) {
+      accumulator->Add("{}");
+    } else {
+      JSObjectDifferentialFuzzingPrintInternalIndexRange(accumulator, depth, false);
+    }
+  }
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintEnumCache(StringStream *accumulator) {
+  Isolate* isolate = GetIsolate();
+
+  Tagged<DescriptorArray> descriptors = map()->instance_descriptors(isolate);
+  Tagged<EnumCache> enum_cache = descriptors->enum_cache();
+
+  if (enum_cache->keys()->length() != 0) {
+    accumulator->Add("(");
+    accumulator->Add(std::to_string(enum_cache->keys()->length()).c_str());
+    accumulator->Add(")");
+  }
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintPrototype(StringStream *accumulator, int depth) {
+  Isolate* isolate = GetIsolate();
+
+  Tagged<Object> proto = map()->prototype();
+
+  // skip Object.prototype
+  if (isolate->IsInAnyContext(proto, Context::INITIAL_OBJECT_PROTOTYPE_INDEX)) {
+    return;
+  } 
+  accumulator->Add("{p:");
+  accumulator->Add(DifferentialFuzzingPrintObject(proto, depth - 1).c_str());
+  accumulator->Add("}");
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrintJSArray(StringStream *accumulator, int depth) {
+  Isolate* isolate = GetIsolate();
+  HandleScope scope(isolate);
+
+  DCHECK(!AllowGarbageCollection::IsAllowed());
+
+  Handle<JSArray> array = handle(JSArray::cast(*this), isolate);
+  int length = IsUndefined(array->length())
+               ? 0
+               : static_cast<int>(Object::Number(array->length()));
+
+  if (length == 0) return;
+
+  int dump_len = std::min((int)v8_flags.dumpling_prop_count, length);
+  accumulator->Add("[");
+
+  ElementsKind elements_kind = array->GetElementsKind();
+
+  if (elements_kind == PACKED_SMI_ELEMENTS || elements_kind == HOLEY_SMI_ELEMENTS || elements_kind == PACKED_ELEMENTS || elements_kind == HOLEY_ELEMENTS) {
+    Tagged<FixedArray> elements = FixedArray::cast(array->elements());
+
+    for (int i = 0; i < dump_len; i++) {
+      int cur_len = IsUndefined(array->length())
+                    ? 0
+                    : static_cast<int>(Object::Number(array->length()));
+      DCHECK(cur_len == length);
+      if (elements->is_the_hole(isolate, i)) {
+        accumulator->Add("<undefined>");
+      } else {
+        accumulator->Add(DifferentialFuzzingPrintObject(elements->get(i), depth - 1).c_str());
+      }
+      if (i != dump_len - 1) {
+        accumulator->Add(", ");
+      }
+    }
+  } else if (elements_kind == PACKED_DOUBLE_ELEMENTS || elements_kind == HOLEY_DOUBLE_ELEMENTS) {
+    Tagged<FixedDoubleArray> elements = FixedDoubleArray::cast(array->elements());
+
+    for (int i = 0; i < dump_len; i++) {
+      int cur_len = IsUndefined(array->length())
+                    ? 0
+                    : static_cast<int>(Object::Number(array->length()));
+      DCHECK(cur_len == length);
+      if (elements->is_the_hole(i)) {
+        accumulator->Add("<undefined>");
+      } else {
+        static const int kBufferSize = 100;
+        char chars[kBufferSize] = { 0 };
+        base::Vector<char> buffer(chars, kBufferSize);
+        accumulator->Add(DoubleToCString(elements->get_scalar(i), buffer));
+      }
+      if (i != dump_len - 1) {
+        accumulator->Add(", ");
+      }
+    }
+  }
+    
+  accumulator->Add("]");
+}
+
+void JSObject::JSObjectDifferentialFuzzingPrint(StringStream *accumulator, int depth) {
+  InstanceType instance_type = map()->instance_type();
+
+  switch (instance_type) {
+    case JS_ARRAY_TYPE: {
+      /*
+      // backing store length is not exposed to users so no guarantees on consistency
+      double length = IsUndefined(JSArray::cast(*this)->length())
+                      ? 0
+                      : Object::Number(JSArray::cast(*this)->length());
+      */
+      accumulator->Add("<JSArray>");
+      break;
+    }
+    case JS_BOUND_FUNCTION_TYPE: {
+      // Tagged<JSBoundFunction> bound_function = JSBoundFunction::cast(*this);
+      accumulator->Add("<JSBoundFunction>");
+      break;
+    }
+    case JS_WEAK_MAP_TYPE: {
+      accumulator->Add("<JSWeakMap>");
+      break;
+    }
+    case JS_WEAK_SET_TYPE: {
+      accumulator->Add("<JSWeakSet>");
+      break;
+    }
+    case JS_REG_EXP_TYPE: {
+      accumulator->Add("<JSRegExp");
+      Tagged<JSRegExp> regexp = JSRegExp::cast(*this);
+      if (IsString(regexp->source())) {
+        accumulator->Add(" ");
+        String::cast(regexp->source())->StringShortPrint(accumulator, false);
+      }
+      accumulator->Add(">");
+
+      break;
+    }
+    case JS_PROMISE_CONSTRUCTOR_TYPE:
+    case JS_REG_EXP_CONSTRUCTOR_TYPE:
+    case JS_ARRAY_CONSTRUCTOR_TYPE:
+#define TYPED_ARRAY_CONSTRUCTORS_SWITCH(Type, type, TYPE, Ctype) \
+  case TYPE##_TYPED_ARRAY_CONSTRUCTOR_TYPE:
+    TYPED_ARRAYS(TYPED_ARRAY_CONSTRUCTORS_SWITCH)
+#undef TYPED_ARRAY_CONSTRUCTORS_SWITCH
+    case JS_CLASS_CONSTRUCTOR_TYPE:
+    case JS_FUNCTION_TYPE: {
+      Tagged<JSFunction> function = JSFunction::cast(*this);
+      std::unique_ptr<char[]> fun_name = function->shared()->DebugNameCStr();
+      if (fun_name[0] != '\0') {
+        accumulator->Add("<JSFunction ");
+        accumulator->Add(fun_name.get());
+      } else {
+        accumulator->Add("<JSFunction");
+      }
+      // if (v8_flags.trace_file_names) {
+      //   Tagged<Object> source_name =
+      //       Script::cast(function->shared()->script())->name();
+      //   if (IsString(source_name)) {
+      //     Tagged<String> str = String::cast(source_name);
+      //     if (str->length() > 0) {
+      //       accumulator->Add(" <");
+      //       accumulator->Put(str);
+      //       accumulator->Add(">");
+      //     }
+      //   }
+      // }
+      accumulator->Put('>');
+      break;
+    }
+    case JS_GENERATOR_OBJECT_TYPE: {
+      accumulator->Add("<JSGenerator>");
+      break;
+    }
+    case JS_ASYNC_FUNCTION_OBJECT_TYPE: {
+      accumulator->Add("<JSAsyncFunctionObject>");
+      break;
+    }
+    case JS_ASYNC_GENERATOR_OBJECT_TYPE: {
+      accumulator->Add("<JS AsyncGenerator>");
+      break;
+    }
+    case JS_SHARED_ARRAY_TYPE:
+      accumulator->Add("<JSSharedArray>");
+      break;
+    case JS_SHARED_STRUCT_TYPE:
+      accumulator->Add("<JSSharedStruct>");
+      break;
+    case JS_MESSAGE_OBJECT_TYPE:
+      accumulator->Add("<JSMessageObject>");
+      break;
+    case JS_EXTERNAL_OBJECT_TYPE:
+      accumulator->Add("<JSExternalObject>");
+      break;
+
+    default: {
+      Tagged<Map> map_of_this = map();
+      Heap* heap = GetHeap();
+      Tagged<Object> constructor = map_of_this->GetConstructor();
+      bool printed = false;
+      if (IsHeapObject(constructor) &&
+          !heap->Contains(HeapObject::cast(constructor))) {
+        accumulator->Add("!!!INVALID CONSTRUCTOR!!!");
+      } else {
+        bool is_global_proxy = IsJSGlobalProxy(*this);
+        if (IsJSFunction(constructor)) {
+          Tagged<SharedFunctionInfo> sfi =
+              JSFunction::cast(constructor)->shared();
+          if (!InReadOnlySpace(sfi) && !heap->Contains(sfi)) {
+            accumulator->Add("!!!INVALID SHARED ON CONSTRUCTOR!!!");
+          } else {
+            Tagged<String> constructor_name = sfi->Name();
+            if (constructor_name->length() > 0) {
+              accumulator->Add(is_global_proxy ? "<GlobalObject " : "<");
+              accumulator->Put(constructor_name);
+              // accumulator->Add(map_of_this->is_deprecated() ? " deprecated" : "");
+              printed = true;
+            }
+          }
+        } else if (IsFunctionTemplateInfo(constructor)) {
+          accumulator->Add("<RemoteObject>");
+          printed = true;
+        }
+        if (!printed) {
+          accumulator->Add("<JS");
+          if (is_global_proxy) {
+            accumulator->Add("GlobalProxy");
+          } else if (IsJSGlobalObject(*this)) {
+            accumulator->Add("GlobalObject");
+          } else {
+            // Objects are serialized right now so we should't need to extra print
+            // accumulator->Add("Object");
+          }
+        }
+      }
+      if (IsJSPrimitiveWrapper(*this)) {
+        accumulator->Add(" value = ");
+        accumulator->Add(DifferentialFuzzingPrintObject(JSPrimitiveWrapper::cast(*this)->value(), depth - 1).c_str());
+      }
+      accumulator->Put('>');
+      break;
+    }
+  }
+
+  Isolate* isolate = GetIsolate();
+
+  if (depth > 0 && !IsUninitialized(*this, isolate)) {
+    if (instance_type == JS_OBJECT_TYPE) {
+      // stringify (we are not going for valid JSON here)
+      // just needs to be consistent and we are good
+      if (HasFastProperties()) {
+        JSObjectDifferentialFuzzingPrintFastProperties(accumulator, depth);
+      } else {
+        JSObjectDifferentialFuzzingPrintDictProperties(accumulator, depth);
+      }
+      JSObjectDifferentialFuzzingPrintEnumCache(accumulator);
+      JSObjectDifferentialFuzzingPrintPrototype(accumulator, depth);
+    } else if (instance_type == JS_ARRAY_TYPE) {
+      JSObjectDifferentialFuzzingPrintJSArray(accumulator, depth);
+    }
+  }
+
+}
+
+
 void JSObject::JSObjectShortPrint(StringStream* accumulator) {
   switch (map()->instance_type()) {
     case JS_ARRAY_TYPE: {
diff --git a/src/objects/js-objects.h b/src/objects/js-objects.h
index f0c9a00878a..568a1c4bb8d 100644
--- a/src/objects/js-objects.h
+++ b/src/objects/js-objects.h
@@ -806,6 +806,14 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
 
   // Dispatched behavior.
   void JSObjectShortPrint(StringStream* accumulator);
+  void JSObjectDifferentialFuzzingPrint(StringStream *accumulator, int depth);
+  void JSObjectDifferentialFuzzingPrintFastProperties(StringStream *accumulator, int depth);
+  void JSObjectDifferentialFuzzingPrintDictProperties(StringStream *accumulator, int depth);
+  void JSObjectDifferentialFuzzingPrintInternalIndexRange(StringStream *accumulator, int depth, bool is_fast_object);
+  void JSObjectDifferentialFuzzingPrintEnumCache(StringStream *accumulator);
+  void JSObjectDifferentialFuzzingPrintPrototype(StringStream *accumulator, int depth);
+  void JSObjectDifferentialFuzzingPrintJSArray(StringStream *accumulator, int depth);
+
   DECL_PRINTER(JSObject)
   DECL_VERIFIER(JSObject)
 #ifdef OBJECT_PRINT
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index 15e428ad2a8..2ed282c0739 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -78,6 +78,7 @@
 #include "src/objects/property-details.h"
 #include "src/roots/roots.h"
 #include "src/snapshot/deserializer.h"
+#include "src/utils/boxed-float.h"
 #include "src/utils/identity-map.h"
 #ifdef V8_INTL_SUPPORT
 #include "src/objects/js-break-iterator.h"
@@ -1823,6 +1824,56 @@ bool Object::IsCodeLike(Tagged<Object> obj, Isolate* isolate) {
   return IsJSReceiver(obj) && JSReceiver::cast(obj)->IsCodeLike(isolate);
 }
 
+void DifferentialFuzzingPrint(Tagged<Object> obj, FILE* out) {
+  OFStream os(out);
+  os << DifferentialFuzzingPrintObject(obj, v8_flags.dumpling_depth);
+}
+
+void DifferentialFuzzingPrint(Tagged<Object> obj, std::ostream& os) {
+  os << DifferentialFuzzingPrintObject(obj, v8_flags.dumpling_depth);
+}
+
+void DifferentialFuzzingPrint(Tagged<Object> obj, StringStream* accumulator) {
+  std::ostringstream os;
+  os << DifferentialFuzzingPrintObject(obj, v8_flags.dumpling_depth);
+  accumulator->Add(os.str().c_str());
+}
+
+std::string DifferentialFuzzingPrintObject(Tagged<Object> obj, int depth) {
+  std::ostringstream os;
+
+  Tagged<HeapObject> heap_object;
+
+  DCHECK(!obj.IsCleared());
+
+  if (IsNumber(obj)) {
+    static const int kBufferSize = 100;
+    char chars[kBufferSize];
+    base::Vector<char> buffer(chars, kBufferSize);
+    if (IsSmi(obj)) {
+      os << IntToCString(obj.ToSmi().value(), buffer);
+    }
+    else {
+      double number = HeapNumber::cast(obj)->value();
+      os << DoubleToCString(number, buffer);
+    }
+  } else if (obj.GetHeapObjectIfWeak(&heap_object)) {
+    os << "[weak] ";
+    heap_object->HeapObjectDifferentialFuzzingPrint(os, depth);
+  } else if (obj.GetHeapObjectIfStrong(&heap_object)) {
+    heap_object->HeapObjectDifferentialFuzzingPrint(os, depth);
+  } else {
+    UNREACHABLE();
+  }
+
+  std::string output = os.str();
+  std::string::size_type pos = 0;
+  while ((pos = output.find("\n",pos)) != std::string::npos) {
+      output.erase(pos, 1);
+  }
+  return output;
+}
+
 void ShortPrint(Tagged<Object> obj, FILE* out) {
   OFStream os(out);
   os << Brief(obj);
@@ -1869,6 +1920,7 @@ std::ostream& operator<<(std::ostream& os, const Brief& v) {
   return os;
 }
 
+
 // static
 void Smi::SmiPrint(Tagged<Smi> smi, std::ostream& os) { os << smi.value(); }
 
diff --git a/src/objects/objects.h b/src/objects/objects.h
index 60aef838e28..4e729bb5b11 100644
--- a/src/objects/objects.h
+++ b/src/objects/objects.h
@@ -688,6 +688,13 @@ inline bool IsShared(Tagged<Object> obj);
 inline bool IsApiCallResultType(Tagged<Object> obj);
 #endif  // DEBUG
 
+//? We want an informative printout for differential fuzzing without inconsistencies such as addresses
+V8_EXPORT_PRIVATE void DifferentialFuzzingPrint(Tagged<Object> obj, FILE* out = stdout);
+V8_EXPORT_PRIVATE void DifferentialFuzzingPrint(Tagged<Object> obj, std::ostream& os);
+V8_EXPORT_PRIVATE void DifferentialFuzzingPrint(Tagged<Object> obj, StringStream* accumulator);
+
+std::string DifferentialFuzzingPrintObject(Tagged<Object> obj, int depth);
+
 // Prints this object without details.
 V8_EXPORT_PRIVATE void ShortPrint(Tagged<Object> obj, FILE* out = stdout);
 
diff --git a/src/objects/string.cc b/src/objects/string.cc
index a806840fa73..2379d4274b1 100644
--- a/src/objects/string.cc
+++ b/src/objects/string.cc
@@ -649,7 +649,7 @@ const char* String::SuffixForDebugPrint() const {
   return "\"";
 }
 
-void String::StringShortPrint(StringStream* accumulator) {
+void String::StringShortPrint(StringStream* accumulator, bool full_debug_print) {
   if (!LooksValid()) {
     accumulator->Add("<Invalid String>");
     return;
@@ -657,17 +657,17 @@ void String::StringShortPrint(StringStream* accumulator) {
 
   const int len = length();
   accumulator->Add("<String[%u]: ", len);
-  accumulator->Add(PrefixForDebugPrint());
+  if (full_debug_print) accumulator->Add(PrefixForDebugPrint());
 
   if (len > kMaxShortPrintLength) {
     accumulator->Add("...<truncated>>");
-    accumulator->Add(SuffixForDebugPrint());
+    if (full_debug_print) accumulator->Add(SuffixForDebugPrint());
     accumulator->Put('>');
     return;
   }
 
   PrintUC16(accumulator, 0, len);
-  accumulator->Add(SuffixForDebugPrint());
+  if (full_debug_print) accumulator->Add(SuffixForDebugPrint());
   accumulator->Put('>');
 }
 
diff --git a/src/objects/string.h b/src/objects/string.h
index 820ec384adb..48efab912d5 100644
--- a/src/objects/string.h
+++ b/src/objects/string.h
@@ -474,7 +474,7 @@ V8_OBJECT class String : public Name {
   //   such as the string heap object address, may truncate long strings, etc.
   const char* PrefixForDebugPrint() const;
   const char* SuffixForDebugPrint() const;
-  void StringShortPrint(StringStream* accumulator);
+  void StringShortPrint(StringStream* accumulator, bool full_debug_print = true);
   void PrintUC16(std::ostream& os, int start = 0, int end = -1);
   void PrintUC16(StringStream* accumulator, int start, int end);
 
diff --git a/src/out/fuzzbuild/args.gn b/src/out/fuzzbuild/args.gn
new file mode 100644
index 00000000000..6e1c314b0e6
--- /dev/null
+++ b/src/out/fuzzbuild/args.gn
@@ -0,0 +1,10 @@
+is_debug = false
+dcheck_always_on = true
+v8_static_library = true
+v8_enable_slow_dchecks = true
+v8_enable_v8_checks = true
+v8_enable_verify_heap = true
+v8_enable_verify_csa = true
+v8_fuzzilli = true
+sanitizer_coverage_flags = "trace-pc-guard"
+target_cpu = "x64"
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 66b74e214e1..99aebfc5286 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -2,6 +2,10 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include <iomanip>
+#include <string>
+#include <vector>
+
 #include "src/asmjs/asm-js.h"
 #include "src/codegen/compilation-cache.h"
 #include "src/codegen/compiler.h"
@@ -9,12 +13,18 @@
 #include "src/common/globals.h"
 #include "src/common/message-template.h"
 #include "src/deoptimizer/deoptimizer.h"
+#include "src/deoptimizer/deopt-frame-dumper.h"
 #include "src/execution/arguments-inl.h"
 #include "src/execution/frames-inl.h"
 #include "src/execution/isolate-inl.h"
 #include "src/objects/js-array-buffer-inl.h"
 #include "src/objects/objects-inl.h"
 #include "src/objects/shared-function-info.h"
+#include "src/interpreter/bytecode-decoder.h"
+#include "src/objects/bytecode-array.h"
+#include "src/objects/dumping.h"
+#include "src/execution/isolate.h"
+
 
 namespace v8 {
 namespace internal {
@@ -249,6 +259,7 @@ bool TryGetOptimizedOsrCode(Isolate* isolate, Tagged<FeedbackVector> vector,
 //      }  // Type b: deopt exit < loop start < OSR backedge
 //    } // Type c: loop start < deopt exit < OSR backedge
 //  }  // The outermost loop
+
 void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate,
                                          Tagged<JSFunction> function,
                                          BytecodeOffset deopt_exit_offset) {
@@ -334,6 +345,235 @@ void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate,
 
 }  // namespace
 
+
+V8_INLINE void MaybePrint(std::string short_name,
+                std::string debug_name,
+                std::optional<std::string> maybe_value,
+                std::ostream &os) {
+  if (maybe_value.has_value()) {
+    if (V8_UNLIKELY(v8_flags.dumping_debug)) {
+      os << debug_name << maybe_value.value() << "\n";
+    } else {
+      os << short_name << maybe_value.value() << "\n";
+    }
+  }
+}
+
+void DoPrint(UnoptimizedFrame* frame, Tagged<JSFunction> function,
+    Isolate* isolate,
+    int bytecode_offset,
+    const uint8_t* bytecode_address, int deopt_point_id,
+    DumpFrameType frame_dump_type,
+    Handle<BytecodeArray> bytecode_array,
+    Handle<Object> accumulator,
+    interpreter::Bytecode bytecode) {
+  std::stringstream os;
+  std::stringstream header;
+
+  switch (frame_dump_type) {
+    case DumpFrameType::kInterpreterFrame:
+      header << "---I" << '\n';
+      break;
+    case DumpFrameType::kMaglevFrame:
+      header << "---M" << '\n';
+      break;
+    case DumpFrameType::kTurbofanFrame:
+      header << "---T" << '\n';
+      break;
+    case DumpFrameType::kSparkplugFrame:
+      header << "---S" << '\n';
+      break;
+    case DumpFrameType::kDeoptFrame:
+      header << "---D" << '\n';
+      break;
+    case DumpFrameType::kReturnJITFrame:
+      header << "---R" << '\n';
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  MaybePrint("b:", "Bytecode offset: ",
+             isolate->DumpBytecodeOffset(bytecode_offset), os);
+
+
+  if (V8_UNLIKELY(v8_flags.dumping_debug)) {
+    interpreter::BytecodeDecoder::Decode(os, bytecode_address);
+    os << std::endl;
+
+    int invocation_count = function->feedback_vector()->invocation_count();
+
+    os << "Invocation count: " << invocation_count << '\n';
+    os << "DeoptPoint ID: " << deopt_point_id << '\n';
+  }
+
+  unsigned int function_id = isolate->GetFunctionId(function);
+
+  MaybePrint("f:", "Function ID: ", isolate->DumpFunctionId(function_id), os);
+
+  int param_count = bytecode_array->parameter_count() - 1;
+  MaybePrint("n:", "Arg count: ", isolate->DumpArgCount(param_count), os);
+  int register_count = bytecode_array->register_count();
+  MaybePrint("m:", "Reg count: ", isolate->DumpRegCount(register_count), os);
+
+  // Print registers
+  std::stringstream check_acc;
+  DifferentialFuzzingPrint(*accumulator, check_acc);
+
+  MaybePrint("x:", "Accumulator: ", isolate->DumpAcc(check_acc.str()), os);
+
+  for (int i = 0; i < param_count; i++) {
+    std::stringstream check_arg;
+    Tagged<Object> arg_object = frame->GetParameter(i);
+    DifferentialFuzzingPrint(arg_object, check_arg);
+    std::string label = "a" + std::to_string(i) + ":";
+    MaybePrint(label, label, isolate->DumpArg(i, check_arg.str()), os);
+  }
+
+  for (int i = 0; i < register_count; i++) {
+    std::stringstream check_reg;
+    Tagged<Object> reg_object = frame->ReadInterpreterRegister(i);
+    DifferentialFuzzingPrint(reg_object, check_reg);
+    std::string label = "r" + std::to_string(i) + ":";
+    MaybePrint(label, label, isolate->DumpReg(i, check_reg.str()), os);
+  }
+
+  /*
+  Tagged<Object> context = frame->context();
+  os << "context" << ": ";
+  DifferentialFuzzingPrint(context, os);
+  os << '\n';
+
+  //////////////
+  Tagged<Object> receiver = frame->receiver();
+  os << "receiver" << ": ";
+  DifferentialFuzzingPrint(receiver, os);
+  os << '\n';
+
+  //////////////
+  os << "function" << ": ";
+  DifferentialFuzzingPrint(function, os);
+  os << '\n';
+  */
+
+
+  /*
+  std::tie(diff_print_updated, diff_print_str) = isolate->DiffPrintUpdateContributingValues(contributing_values);
+  // always print contributing_values
+  header << diff_print_str << "\n";
+  */
+
+  os << std::endl;
+
+  isolate->AppendDumpOut(header.str());
+  isolate->AppendDumpOut(os.str());
+}
+
+void PrintDumpedFrame(UnoptimizedFrame* frame, Tagged<JSFunction> function,
+    Isolate* isolate,
+    int bytecode_offset, int deopt_point_id,
+    DumpFrameType frame_dump_type) {
+
+  isolate->SetHeapAllocationAllowedWithoutGC(true);
+
+  Handle<BytecodeArray> bytecode_array(
+    function->shared()->GetBytecodeArray(isolate), isolate);
+
+
+  // accumulator is located directly after the registers in the stack frame
+  int accumulator_reg_idx = bytecode_array->register_count();
+  Tagged<Object> accumulator_t = frame->ReadInterpreterRegister(accumulator_reg_idx);
+  Handle<Object> accumulator(accumulator_t, isolate);
+
+  interpreter::BytecodeArrayIterator bytecode_iterator(bytecode_array);
+  while (bytecode_iterator.current_offset() +
+             bytecode_iterator.current_bytecode_size() <=
+         bytecode_offset) {
+    bytecode_iterator.Advance();
+  }
+
+  interpreter::Bytecode bytecode = bytecode_iterator.current_bytecode();
+
+  const uint8_t* base_address = reinterpret_cast<const uint8_t*>(
+      bytecode_array->GetFirstBytecodeAddress());
+  const uint8_t* bytecode_address = base_address + bytecode_offset;
+
+  DoPrint(frame, function, isolate, bytecode_offset, bytecode_address, deopt_point_id, frame_dump_type, bytecode_array, accumulator, bytecode);
+
+  isolate->SetHeapAllocationAllowedWithoutGC(false);
+}
+
+
+RUNTIME_FUNCTION(Runtime_NotifyDumpFrame) {
+  HandleScope scope(isolate);
+
+  DeoptFrameDumper* dumper = DeoptFrameDumper::Grab(isolate);
+  CHECK(!AllowGarbageCollection::IsAllowed());
+  CHECK(isolate->context().is_null());
+  CHECK_NOT_NULL(dumper);
+
+  // TODO(turbofan): We currently need the native context to materialize
+  // the arguments object, but only to get to its map.
+  Tagged<Context> saved_context = isolate->context();
+
+  isolate->set_context(dumper->function()->native_context());
+
+  // dumper->ReenableGC();
+  // Make sure to materialize objects before causing any allocation.
+  isolate->SetHeapAllocationAllowedWithoutGC(true);
+  dumper->MaterializeHeapObjects();
+  isolate->SetHeapAllocationAllowedWithoutGC(false);
+
+  JavaScriptStackFrameIterator frame_iterator(isolate);
+
+  UnoptimizedFrame *frame =
+      static_cast<UnoptimizedFrame*>(frame_iterator.frame());
+
+  // figure out if we reached this from turbofan or maglev
+  DumpFrameType dump_frame_type;
+  Handle<Code> compiled_code = dumper->compiled_code();
+
+  if (compiled_code->is_maglevved()) {
+    dump_frame_type = DumpFrameType::kMaglevFrame;
+  } else {
+    DCHECK(compiled_code->is_turbofanned());
+    dump_frame_type = DumpFrameType::kTurbofanFrame;
+  }
+
+  DCHECK(v8_flags.turbofan_dumping || v8_flags.maglev_dumping);
+
+  if (isolate->IsDumpingEnabled()) {
+    const BytecodeOffset deopt_exit_offset =
+      dumper->bytecode_offset_in_innermost_frame();
+
+    unsigned int deopt_point_id = dumper->deopt_exit_index();
+
+    int bytecode_offset = deopt_exit_offset.ToInt();
+
+    Tagged<JSFunction> function = frame->function();
+    if (v8_flags.generate_dump_positions) {
+      isolate->RecordDumpPosition(function, static_cast<unsigned int>(bytecode_offset));
+    }
+
+    Tagged<DeoptimizationData> deopt_data =
+      DeoptimizationData::cast(compiled_code->deoptimization_data());
+
+    unsigned int f1 = isolate->GetFunctionId(function);
+    unsigned int f2 = isolate->GetFunctionId(*dumper->function());
+    DCHECK(f1 == f2 || deopt_data->InlinedFunctionCount().value() != 0);
+
+    PrintDumpedFrame(frame, function, isolate,
+      bytecode_offset, deopt_point_id, dump_frame_type);
+  }
+
+  isolate->set_context(saved_context);
+  dumper->ReenableGC();
+  CHECK(AllowGarbageCollection::IsAllowed());
+  delete dumper;
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
+
 RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   HandleScope scope(isolate);
   DCHECK_EQ(0, args.length());
@@ -344,17 +584,16 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
 
   TimerEventScope<TimerEventDeoptimizeCode> timer(isolate);
   TRACE_EVENT0("v8", "V8.DeoptimizeCode");
+  // isolate->SetDeoptimizeInProgress(true);
+
   Handle<JSFunction> function = deoptimizer->function();
-  if (v8_flags.profile_guided_optimization) {
-    function->shared()->set_cached_tiering_decision(
-        CachedTieringDecision::kNormal);
-  }
   // For OSR the optimized code isn't installed on the function, so get the
   // code object from deoptimizer.
   Handle<Code> optimized_code = deoptimizer->compiled_code();
   const DeoptimizeKind deopt_kind = deoptimizer->deopt_kind();
+  Deoptimizer::DeoptInfo deopt_info = deoptimizer->GetDeoptInfo();
   const DeoptimizeReason deopt_reason =
-      deoptimizer->GetDeoptInfo().deopt_reason;
+      deopt_info.deopt_reason;
 
   // TODO(turbofan): We currently need the native context to materialize
   // the arguments object, but only to get to its map.
@@ -371,6 +610,8 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   JavaScriptFrame* top_frame = top_it.frame();
   isolate->set_context(Context::cast(top_frame->context()));
 
+  if (V8_LIKELY(v8_flags.jit_return_dump)) isolate->SetDumpNextInterpreterFrame(false);
+
   // Lazy deopts don't invalidate the underlying optimized code since the code
   // object itself is still valid (as far as we know); the called function
   // caused the deopt, not the function we're currently looking at.
@@ -378,6 +619,23 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
     return ReadOnlyRoots(isolate).undefined_value();
   }
 
+  if (v8_flags.turbofan_dumping_print_deopt_frames && deoptimizer->TopFrameIsUnoptimized() && isolate->IsDumpingEnabled()) {
+    DisallowGarbageCollection no_gc;
+    unsigned int deopt_point_id = deoptimizer->deopt_exit_index();
+
+    int bytecode_offset = deoptimizer->bytecode_offset_in_innermost_frame().ToInt();
+
+    UnoptimizedFrame* frame = static_cast<UnoptimizedFrame*>(top_frame);
+    Tagged<JSFunction> frame_function = frame->function();
+
+    if (v8_flags.generate_dump_positions) {
+      isolate->RecordDumpPosition(frame_function, static_cast<unsigned int>(bytecode_offset));
+    }
+
+    PrintDumpedFrame(frame, frame_function, isolate,
+        bytecode_offset, deopt_point_id, DumpFrameType::kDeoptFrame);
+  }
+
   // Some eager deopts also don't invalidate InstructionStream (e.g. when
   // preparing for OSR from Maglev to Turbofan).
   if (IsDeoptimizationWithoutCodeInvalidation(deopt_reason)) {
diff --git a/src/runtime/runtime-object.cc b/src/runtime/runtime-object.cc
index 19a44625c8a..223d4fe5f27 100644
--- a/src/runtime/runtime-object.cc
+++ b/src/runtime/runtime-object.cc
@@ -17,6 +17,7 @@
 #include "src/objects/property-descriptor.h"
 #include "src/objects/property-details.h"
 #include "src/objects/swiss-name-dictionary-inl.h"
+#include "src/objects/dumping.h"
 #include "src/runtime/runtime.h"
 
 namespace v8 {
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index 347bd222207..e482a235f84 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -12,6 +12,7 @@
 #include "src/api/api-inl.h"
 #include "src/base/macros.h"
 #include "src/base/numbers/double.h"
+#include "src/interpreter/bytecode-decoder.h"
 #include "src/codegen/compiler.h"
 #include "src/codegen/pending-optimization-table.h"
 #include "src/compiler-dispatcher/lazy-compile-dispatcher.h"
@@ -30,6 +31,7 @@
 #include "src/ic/stub-cache.h"
 #include "src/objects/bytecode-array.h"
 #include "src/objects/js-collection-inl.h"
+#include "src/objects/dumping.h"
 #include "src/utils/utils.h"
 #ifdef V8_ENABLE_MAGLEV
 #include "src/maglev/maglev-concurrent-dispatcher.h"
@@ -50,10 +52,15 @@
 #include "src/wasm/wasm-engine.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
 
+#include "src/execution/isolate.h"
+
+
 namespace v8 {
 namespace internal {
 
 namespace {
+
+
 V8_WARN_UNUSED_RESULT Tagged<Object> CrashUnlessFuzzing(Isolate* isolate) {
   CHECK(v8_flags.fuzzing);
   return ReadOnlyRoots(isolate).undefined_value();
@@ -238,6 +245,149 @@ RUNTIME_FUNCTION(Runtime_LeakHole) {
   return ReadOnlyRoots(isolate).the_hole_value();
 }
 
+void PrintRegisterRange(UnoptimizedFrame* frame, std::ostream& os,
+                        interpreter::BytecodeArrayIterator& bytecode_iterator,
+                        const int& reg_field_width, const char* arrow_direction,
+                        interpreter::Register first_reg, int range) {
+  for (int reg_index = first_reg.index(); reg_index < first_reg.index() + range;
+       reg_index++) {
+    Tagged<Object> reg_object = frame->ReadInterpreterRegister(reg_index);
+    os << "      [ " << std::setw(reg_field_width)
+       << interpreter::Register(reg_index).ToString() << arrow_direction;
+    ShortPrint(reg_object, os);
+    os << " ]" << std::endl;
+  }
+}
+
+void PrintRegisters(UnoptimizedFrame* frame, std::ostream& os, bool is_input,
+                    interpreter::BytecodeArrayIterator& bytecode_iterator,
+                    Handle<Object> accumulator) {
+  static const char kAccumulator[] = "accumulator";
+  static const int kRegFieldWidth = static_cast<int>(sizeof(kAccumulator) - 1);
+  interpreter::Bytecode bytecode = bytecode_iterator.current_bytecode();
+  const char* kArrowDirection = is_input ? " -> " : " <- ";
+  // Print accumulator.
+  os << "      [ " << kAccumulator << kArrowDirection;
+  ShortPrint(*accumulator, os);
+  os << " ]" << std::endl;
+
+  os << bytecode << std::endl;
+
+
+  // Print the registers.
+  int operand_count = interpreter::Bytecodes::NumberOfOperands(bytecode);
+  for (int operand_index = 0; operand_index < operand_count; operand_index++) {
+    interpreter::OperandType operand_type =
+        interpreter::Bytecodes::GetOperandType(bytecode, operand_index);
+    bool should_print =
+        is_input
+            ? interpreter::Bytecodes::IsRegisterInputOperandType(operand_type)
+            : interpreter::Bytecodes::IsRegisterOutputOperandType(operand_type);
+    if (should_print) {
+      interpreter::Register first_reg =
+          bytecode_iterator.GetRegisterOperand(operand_index);
+      int range = bytecode_iterator.GetRegisterOperandRange(operand_index);
+      PrintRegisterRange(frame, os, bytecode_iterator, kRegFieldWidth,
+                         kArrowDirection, first_reg, range);
+    }
+  }
+  if (!is_input && interpreter::Bytecodes::IsShortStar(bytecode)) {
+    PrintRegisterRange(frame, os, bytecode_iterator, kRegFieldWidth,
+                       kArrowDirection,
+                       interpreter::Register::FromShortStar(bytecode), 1);
+  }
+}
+
+void AdvanceToOffsetForTracing(
+    interpreter::BytecodeArrayIterator& bytecode_iterator, int offset) {
+  while (bytecode_iterator.current_offset() +
+             bytecode_iterator.current_bytecode_size() <=
+         offset) {
+    bytecode_iterator.Advance();
+  }
+  DCHECK(bytecode_iterator.current_offset() == offset ||
+         ((bytecode_iterator.current_offset() + 1) == offset &&
+          bytecode_iterator.current_operand_scale() >
+              interpreter::OperandScale::kSingle));
+}
+
+
+RUNTIME_FUNCTION(Runtime_TraceSmileyface) {
+
+  if (!isolate->IsDumpingEnabled()) return ReadOnlyRoots(isolate).undefined_value();
+  DCHECK_EQ(3, args.length());
+
+  SealHandleScope shs(isolate);
+
+  DisallowGarbageCollection no_gc;
+
+  JavaScriptStackFrameIterator frame_iterator(isolate);
+  UnoptimizedFrame* frame =
+    reinterpret_cast<UnoptimizedFrame*>(frame_iterator.frame());
+
+  Tagged<JSFunction> function = frame->function();
+  bool is_sparkplug = frame->is_baseline();
+  bool is_interpreter = !is_sparkplug;
+
+  // bypass interpreter_dumping flag in case we want to dump only the next interpreter frame (returns from JIT)
+  bool dump_next_interpreter_frame = isolate->DumpNextInterpreterFrame();
+  if (dump_next_interpreter_frame) {
+    DCHECK(!v8_flags.interpreter_dumping);
+    DCHECK(v8_flags.jit_return_dump);
+    isolate->SetDumpNextInterpreterFrame(false);
+  }
+
+  if ((is_sparkplug && !v8_flags.sparkplug) || (is_interpreter && !v8_flags.interpreter_dumping && !dump_next_interpreter_frame)) return ReadOnlyRoots(isolate).undefined_value();
+
+  Handle<BytecodeArray> bytecode_array = args.at<BytecodeArray>(0);
+  int bytecode_offset = args.smi_value_at(1);
+
+  Handle<Object> accumulator = args.at(2);
+
+  int offset = bytecode_offset - BytecodeArray::kHeaderSize + kHeapObjectTag;
+  interpreter::BytecodeArrayIterator bytecode_iterator(bytecode_array);
+  AdvanceToOffsetForTracing(bytecode_iterator, offset);
+
+  interpreter::Bytecode bytecode = bytecode_iterator.current_bytecode();
+
+  if (offset != bytecode_iterator.current_offset()) return ReadOnlyRoots(isolate).undefined_value();
+
+  const uint8_t* base_address = reinterpret_cast<const uint8_t*>(
+    bytecode_array->GetFirstBytecodeAddress());
+  const uint8_t* bytecode_address = base_address + offset;
+
+  int function_local_bytecode_offset = bytecode_iterator.current_offset();
+
+
+  DCHECK_GE(function_local_bytecode_offset, 0);
+
+  bool is_consistent_dump_position_with_compiler =
+    isolate->IsAllowedInterpreterDumpPosition(function, static_cast<unsigned int>(function_local_bytecode_offset));
+  bool consider_only_compiler_consistent_dump_positions = v8_flags.load_dump_positions;
+
+  if ((v8_flags.generate_dump_positions && is_sparkplug) || dump_next_interpreter_frame) {
+    isolate->RecordDumpPosition(function, static_cast<unsigned int>(function_local_bytecode_offset));
+  }
+
+  if (consider_only_compiler_consistent_dump_positions && !is_consistent_dump_position_with_compiler) {
+    DCHECK(!v8_flags.generate_dump_positions);
+    return ReadOnlyRoots(isolate).undefined_value();
+  }
+
+  DumpFrameType frame_dump_type;
+  if (dump_next_interpreter_frame) {
+    frame_dump_type = kReturnJITFrame;
+  } else if (is_sparkplug) {
+    frame_dump_type = kSparkplugFrame;
+  } else {
+    frame_dump_type = kInterpreterFrame;
+  }
+
+  DoPrint(frame, function, isolate, function_local_bytecode_offset, bytecode_address, -1, frame_dump_type, bytecode_array, accumulator, bytecode);
+
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
 RUNTIME_FUNCTION(Runtime_RunningInSimulator) {
   SealHandleScope shs(isolate);
   if (args.length() != 0) {
@@ -537,6 +687,16 @@ RUNTIME_FUNCTION(Runtime_IsTurbofanEnabled) {
   return isolate->heap()->ToBoolean(v8_flags.turbofan);
 }
 
+RUNTIME_FUNCTION(Runtime_EnableFrameDumping) {
+  isolate->SetInScriptDumpingEnabled(true);
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
+RUNTIME_FUNCTION(Runtime_DisableFrameDumping) {
+  isolate->SetInScriptDumpingEnabled(false);
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
 RUNTIME_FUNCTION(Runtime_CurrentFrameIsTurbofan) {
   HandleScope scope(isolate);
   DCHECK_EQ(args.length(), 0);
@@ -1523,6 +1683,7 @@ RUNTIME_FUNCTION(Runtime_DisassembleFunction) {
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
+/*
 namespace {
 
 int StackSize(Isolate* isolate) {
@@ -1541,28 +1702,49 @@ void PrintIndentation(int stack_size) {
 }
 
 }  // namespace
+*/
 
 RUNTIME_FUNCTION(Runtime_TraceEnter) {
+  DisallowGarbageCollection no_gc;
+
   SealHandleScope shs(isolate);
   if (args.length() != 0) {
     return CrashUnlessFuzzing(isolate);
   }
+  /*
   PrintIndentation(StackSize(isolate));
   JavaScriptFrame::PrintTop(isolate, stdout, true, false);
   PrintF(" {\n");
+  */
+  if (V8_LIKELY(isolate->IsDumpingEnabled())) {
+    isolate->AppendDumpOut(">\n");
+  }
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
 RUNTIME_FUNCTION(Runtime_TraceExit) {
+  DisallowGarbageCollection no_gc;
+
   SealHandleScope shs(isolate);
   if (args.length() != 1) {
     return CrashUnlessFuzzing(isolate);
   }
   Tagged<Object> obj = args[0];
-  PrintIndentation(StackSize(isolate));
-  PrintF("} -> ");
-  ShortPrint(obj);
-  PrintF("\n");
+
+  if (V8_LIKELY(isolate->IsDumpingEnabled())) {
+    isolate->AppendDumpOut("<\n");
+
+    if (V8_LIKELY(v8_flags.jit_return_dump)) {
+      JavaScriptStackFrameIterator frame_iterator(isolate);
+      JavaScriptFrame *frame = frame_iterator.frame();
+      Tagged<JSFunction> function = frame->function();
+      Tagged<Code> code = function->code(isolate);
+      if (code->is_turbofanned() || code->is_maglevved()) {
+        isolate->SetDumpNextInterpreterFrame(true);
+      }
+    }
+  }
+
   return obj;  // return TOS
 }
 
diff --git a/src/runtime/runtime.cc b/src/runtime/runtime.cc
index af669bbc926..d84af4c559e 100644
--- a/src/runtime/runtime.cc
+++ b/src/runtime/runtime.cc
@@ -207,6 +207,8 @@ bool Runtime::IsAllowListedForFuzzing(FunctionId id) {
     case Runtime::kGetUndetectable:
     case Runtime::kNeverOptimizeFunction:
     case Runtime::kOptimizeFunctionOnNextCall:
+    case Runtime::kEnableFrameDumping:
+    case Runtime::kDisableFrameDumping:
     case Runtime::kOptimizeMaglevOnNextCall:
     case Runtime::kOptimizeOsr:
     case Runtime::kPrepareFunctionForOptimization:
@@ -219,10 +221,11 @@ bool Runtime::IsAllowListedForFuzzing(FunctionId id) {
     case Runtime::kNotifyIsolateForeground:
     case Runtime::kNotifyIsolateBackground:
     case Runtime::kIsEfficiencyModeEnabled:
+    return true;
 #if V8_ENABLE_WEBASSEMBLY && !defined(OFFICIAL_BUILD)
     case Runtime::kWasmGenerateRandomModule:
+      return !v8_flags.allow_natives_for_differential_fuzzing;
 #endif  // V8_ENABLE_WEBASSEMBLY
-      return true;
     // Runtime functions only permitted for non-differential fuzzers.
     // This list may contain functions performing extra checks or returning
     // different values in the context of different flags passed to V8.
diff --git a/src/runtime/runtime.h b/src/runtime/runtime.h
index 46145a487dc..bb5b4ecd2c8 100644
--- a/src/runtime/runtime.h
+++ b/src/runtime/runtime.h
@@ -122,6 +122,7 @@ namespace internal {
   F(FunctionLogNextExecution, 1, 1)             \
   F(InstantiateAsmJs, 4, 1)                     \
   F(NotifyDeoptimized, 0, 1)                    \
+  F(NotifyDumpFrame, 0, 1)                      \
   F(ObserveNode, 1, 1)                          \
   F(ResolvePossiblyDirectEval, 6, 1)            \
   F(VerifyType, 1, 1)                           \
@@ -591,6 +592,8 @@ namespace internal {
   F(NotifyIsolateBackground, 0, 1)            \
   F(OptimizeMaglevOnNextCall, 1, 1)           \
   F(OptimizeFunctionOnNextCall, -1, 1)        \
+  F(EnableFrameDumping, -1, 1)                \
+  F(DisableFrameDumping, -1, 1)               \
   F(OptimizeOsr, -1, 1)                       \
   F(PrepareFunctionForOptimization, -1, 1)    \
   F(PretenureAllocationSite, 1, 1)            \
@@ -622,7 +625,8 @@ namespace internal {
   F(TypedArraySpeciesProtector, 0, 1)         \
   F(WaitForBackgroundOptimization, 0, 1)      \
   I(DeoptimizeNow, 0, 1)                      \
-  F(LeakHole, 0, 1)
+  F(LeakHole, 0, 1)                           \
+  F(TraceSmileyface, 3, 1)
 
 #define FOR_EACH_INTRINSIC_TYPEDARRAY(F, I)    \
   F(ArrayBufferDetach, -1, 1)                  \
